{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 What Is Argo CD? \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Why Argo CD? \u00b6 Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand. Getting Started \u00b6 Quick Start \u00b6 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Follow our getting started guide . Further user oriented documentation is provided for additional features. Developer oriented documentation is available for people interested in building third-party integrations. How it works \u00b6 Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways: kustomize applications helm charts ksonnet applications jsonnet files Plain directory of YAML/json manifests Any custom config management tool configured as a config management plugin Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available. For a quick 10 minute overview of Argo CD, check out the demo presented to the Sig Apps community meeting: Architecture \u00b6 Argo CD is implemented as a kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the Git repo). A deployed application whose live state deviates from the target state is considered OutOfSync . Argo CD reports & visualizes the differences, while providing facilities to automatically or manually sync the live state back to the desired target state. Any modifications made to the desired target state in the Git repo can be automatically applied and reflected in the specified target environments. For additional details, see architecture overview . Features \u00b6 Automated deployment of applications to specified target environments Support for multiple config management/templating tools (Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML) Ability to manage and deploy to multiple clusters SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn) Multi-tenancy and RBAC policies for authorization Rollback/Roll-anywhere to any application configuration committed in Git repository Health status analysis of application resources Automated configuration drift detection and visualization Automated or manual syncing of applications to its desired state Web UI which provides real-time view of application activity CLI for automation and CI integration Webhook integration (GitHub, BitBucket, GitLab) Access tokens for automation PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green & canary upgrades) Audit trails for application events and API calls Prometheus metrics Parameter overrides for overriding ksonnet/helm parameters in Git Development Status \u00b6 Argo CD is actively developed and is being used in production to deploy SaaS services at Intuit","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#what-is-argo-cd","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.","title":"What Is Argo CD?"},{"location":"#why-argo-cd","text":"Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.","title":"Why Argo CD?"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#quick-start","text":"kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Follow our getting started guide . Further user oriented documentation is provided for additional features. Developer oriented documentation is available for people interested in building third-party integrations.","title":"Quick Start"},{"location":"#how-it-works","text":"Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways: kustomize applications helm charts ksonnet applications jsonnet files Plain directory of YAML/json manifests Any custom config management tool configured as a config management plugin Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available. For a quick 10 minute overview of Argo CD, check out the demo presented to the Sig Apps community meeting:","title":"How it works"},{"location":"#architecture","text":"Argo CD is implemented as a kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the Git repo). A deployed application whose live state deviates from the target state is considered OutOfSync . Argo CD reports & visualizes the differences, while providing facilities to automatically or manually sync the live state back to the desired target state. Any modifications made to the desired target state in the Git repo can be automatically applied and reflected in the specified target environments. For additional details, see architecture overview .","title":"Architecture"},{"location":"#features","text":"Automated deployment of applications to specified target environments Support for multiple config management/templating tools (Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML) Ability to manage and deploy to multiple clusters SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn) Multi-tenancy and RBAC policies for authorization Rollback/Roll-anywhere to any application configuration committed in Git repository Health status analysis of application resources Automated configuration drift detection and visualization Automated or manual syncing of applications to its desired state Web UI which provides real-time view of application activity CLI for automation and CI integration Webhook integration (GitHub, BitBucket, GitLab) Access tokens for automation PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green & canary upgrades) Audit trails for application events and API calls Prometheus metrics Parameter overrides for overriding ksonnet/helm parameters in Git","title":"Features"},{"location":"#development-status","text":"Argo CD is actively developed and is being used in production to deploy SaaS services at Intuit","title":"Development Status"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 Before You Start \u00b6 You must install and run the ArgoCD using a local Kubernetes (e.g. Docker for Desktop or Minikube) first. This will help you understand the application, but also get your local environment set-up. Then, to get a good grounding in Go, try out the tutorial . Pre-requisites \u00b6 Install: docker git and git-lfs golang dep ksonnet helm kustomize kubectl kubectx minikube or Docker for Desktop Brew users can quickly install the lot: brew install go git-lfs kubectl kubectx dep ksonnet/tap/ks kubernetes-helm kustomize Check the versions: go version ; # must be v1.12.x helm version ; # must be v2.13.x kustomize version ; # must be v3.1.x Set up environment variables (e.g. is ~/.bashrc ): export GOPATH = ~/go export PATH = $PATH : $GOPATH /bin Checkout the code: go get -u github.com/argoproj/argo-cd cd ~/go/src/github.com/argoproj/argo-cd Building \u00b6 Ensure dependencies are up to date first: dep ensure make dev-tools-image make install-lint-tools go get github.com/mattn/goreman go get github.com/jstemmer/go-junit-report Common make targets: make codegen - Run code generation make lint - Lint code make test - Run unit tests make cli - Make the argocd CLI tool Check out the following documentation for instructions on running the e2e tests. Running Locally \u00b6 It is much easier to run and debug if you run ArgoCD on your local machine than in the Kubernetes cluster. You should scale the deployments to zero: kubectl -n argocd scale deployment/argocd-application-controller --replicas 0 kubectl -n argocd scale deployment/argocd-dex-server --replicas 0 kubectl -n argocd scale deployment/argocd-repo-server --replicas 0 kubectl -n argocd scale deployment/argocd-server --replicas 0 kubectl -n argocd scale deployment/argocd-redis --replicas 0 Download Yarn dependencies and Compile: ~/go/src/github.com/argoproj/argo-cd/ui yarn install yarn build Then start the services: cd ~/go/src/github.com/argoproj/argo-cd make start You can now execute argocd command against your locally running ArgoCD by appending --server localhost:8080 --plaintext --insecure , e.g.: argocd app create guestbook --path guestbook --repo https://github.com/argoproj/argocd-example-apps.git --dest-server https://kubernetes.default.svc --dest-namespace default --server localhost:8080 --plaintext --insecure You can open the UI: http://localhost:4000 As an alternative to using the above command line parameters each time you call argocd CLI, you can set the following environment variables: export ARGOCD_SERVER = 127 .0.0.1:8080 export ARGOCD_OPTS = \"--plaintext --insecure\" Running Local Containers \u00b6 You may need to run containers locally, so here's how: Create login to Docker Hub, then login. docker login Add your username as the environment variable, e.g. to your ~/.bash_profile : export IMAGE_NAMESPACE = alexcollinsintuit If you don't want to use latest as the image's tag (the default), you can set it from the environment too: export IMAGE_TAG = yourtag Build the image: DOCKER_PUSH = true make image Update the manifests (be sure to do that from a shell that has above environment variables set) make manifests Install the manifests: kubectl -n argocd apply --force -f manifests/install.yaml Scale your deployments up: kubectl -n argocd scale deployment/argocd-application-controller --replicas 1 kubectl -n argocd scale deployment/argocd-dex-server --replicas 1 kubectl -n argocd scale deployment/argocd-repo-server --replicas 1 kubectl -n argocd scale deployment/argocd-server --replicas 1 kubectl -n argocd scale deployment/argocd-redis --replicas 1 Now you can set-up the port-forwarding and open the UI or CLI.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"","title":"Contributing"},{"location":"CONTRIBUTING/#before-you-start","text":"You must install and run the ArgoCD using a local Kubernetes (e.g. Docker for Desktop or Minikube) first. This will help you understand the application, but also get your local environment set-up. Then, to get a good grounding in Go, try out the tutorial .","title":"Before You Start"},{"location":"CONTRIBUTING/#pre-requisites","text":"Install: docker git and git-lfs golang dep ksonnet helm kustomize kubectl kubectx minikube or Docker for Desktop Brew users can quickly install the lot: brew install go git-lfs kubectl kubectx dep ksonnet/tap/ks kubernetes-helm kustomize Check the versions: go version ; # must be v1.12.x helm version ; # must be v2.13.x kustomize version ; # must be v3.1.x Set up environment variables (e.g. is ~/.bashrc ): export GOPATH = ~/go export PATH = $PATH : $GOPATH /bin Checkout the code: go get -u github.com/argoproj/argo-cd cd ~/go/src/github.com/argoproj/argo-cd","title":"Pre-requisites"},{"location":"CONTRIBUTING/#building","text":"Ensure dependencies are up to date first: dep ensure make dev-tools-image make install-lint-tools go get github.com/mattn/goreman go get github.com/jstemmer/go-junit-report Common make targets: make codegen - Run code generation make lint - Lint code make test - Run unit tests make cli - Make the argocd CLI tool Check out the following documentation for instructions on running the e2e tests.","title":"Building"},{"location":"CONTRIBUTING/#running-locally","text":"It is much easier to run and debug if you run ArgoCD on your local machine than in the Kubernetes cluster. You should scale the deployments to zero: kubectl -n argocd scale deployment/argocd-application-controller --replicas 0 kubectl -n argocd scale deployment/argocd-dex-server --replicas 0 kubectl -n argocd scale deployment/argocd-repo-server --replicas 0 kubectl -n argocd scale deployment/argocd-server --replicas 0 kubectl -n argocd scale deployment/argocd-redis --replicas 0 Download Yarn dependencies and Compile: ~/go/src/github.com/argoproj/argo-cd/ui yarn install yarn build Then start the services: cd ~/go/src/github.com/argoproj/argo-cd make start You can now execute argocd command against your locally running ArgoCD by appending --server localhost:8080 --plaintext --insecure , e.g.: argocd app create guestbook --path guestbook --repo https://github.com/argoproj/argocd-example-apps.git --dest-server https://kubernetes.default.svc --dest-namespace default --server localhost:8080 --plaintext --insecure You can open the UI: http://localhost:4000 As an alternative to using the above command line parameters each time you call argocd CLI, you can set the following environment variables: export ARGOCD_SERVER = 127 .0.0.1:8080 export ARGOCD_OPTS = \"--plaintext --insecure\"","title":"Running Locally"},{"location":"CONTRIBUTING/#running-local-containers","text":"You may need to run containers locally, so here's how: Create login to Docker Hub, then login. docker login Add your username as the environment variable, e.g. to your ~/.bash_profile : export IMAGE_NAMESPACE = alexcollinsintuit If you don't want to use latest as the image's tag (the default), you can set it from the environment too: export IMAGE_TAG = yourtag Build the image: DOCKER_PUSH = true make image Update the manifests (be sure to do that from a shell that has above environment variables set) make manifests Install the manifests: kubectl -n argocd apply --force -f manifests/install.yaml Scale your deployments up: kubectl -n argocd scale deployment/argocd-application-controller --replicas 1 kubectl -n argocd scale deployment/argocd-dex-server --replicas 1 kubectl -n argocd scale deployment/argocd-repo-server --replicas 1 kubectl -n argocd scale deployment/argocd-server --replicas 1 kubectl -n argocd scale deployment/argocd-redis --replicas 1 Now you can set-up the port-forwarding and open the UI or CLI.","title":"Running Local Containers"},{"location":"SUPPORT/","text":"Support \u00b6 Make sure you've read understanding the basics the getting started guide . Looked for an answer the frequently asked questions . Ask a question in the Argo CD Slack channel \u29c9 . Read issues, report a bug, or request a feature \u29c9","title":"Support"},{"location":"SUPPORT/#support","text":"Make sure you've read understanding the basics the getting started guide . Looked for an answer the frequently asked questions . Ask a question in the Argo CD Slack channel \u29c9 . Read issues, report a bug, or request a feature \u29c9","title":"Support"},{"location":"cli_installation/","text":"Installation \u00b6 You can download the latest Argo CD version from the latest release page of this repository , which will include the argocd CLI. Linux \u00b6 You can view the latest version of Argo CD at the link above or run the following command to grab the version: VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) Replace VERSION in the command below with the version of Argo CD you would like to download: curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-linux-amd64 Make the argocd CLI executable: chmod +x /usr/local/bin/argocd You should now be able to run argocd commands. Mac \u00b6 Homebrew \u00b6 brew tap argoproj/tap brew install argoproj/tap/argocd Download With Curl \u00b6 You can view the latest version of Argo CD at the link above or run the following command to grab the version: VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) Replace VERSION in the command below with the version of Argo CD you would like to download: curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-darwin-amd64 Make the argocd CLI executable: chmod +x /usr/local/bin/argocd After finishing either of the instructions above, you should now be able to run argocd commands.","title":"Installation"},{"location":"cli_installation/#installation","text":"You can download the latest Argo CD version from the latest release page of this repository , which will include the argocd CLI.","title":"Installation"},{"location":"cli_installation/#linux","text":"You can view the latest version of Argo CD at the link above or run the following command to grab the version: VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) Replace VERSION in the command below with the version of Argo CD you would like to download: curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-linux-amd64 Make the argocd CLI executable: chmod +x /usr/local/bin/argocd You should now be able to run argocd commands.","title":"Linux"},{"location":"cli_installation/#mac","text":"","title":"Mac"},{"location":"cli_installation/#homebrew","text":"brew tap argoproj/tap brew install argoproj/tap/argocd","title":"Homebrew"},{"location":"cli_installation/#download-with-curl","text":"You can view the latest version of Argo CD at the link above or run the following command to grab the version: VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) Replace VERSION in the command below with the version of Argo CD you would like to download: curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-darwin-amd64 Make the argocd CLI executable: chmod +x /usr/local/bin/argocd After finishing either of the instructions above, you should now be able to run argocd commands.","title":"Download With Curl"},{"location":"core_concepts/","text":"Core Concepts \u00b6 Let's assume you're familiar with core Git, Docker, Kubernetes, Continuous Delivery, and GitOps concepts. Application A group of Kubernetes resources as defined by a manifest. This is a Custom Resource Definition (CRD). Application source type Which Tool is used to build the application. Target state The desired state of an application, as represented by files in a Git repository. Live state The live state of that application. What pods etc are deployed. Sync status Whether or not the live state matches the target state. Is the deployed application the same as Git says it should be? Sync The process of making an application move to its target state. E.g. by applying changes to a Kubernetes cluster. Sync operation status Whether or not a sync succeeded. Refresh Compare the latest code in Git with the live state. Figure out what is different. Health The health of the application, is it running correctly? Can it serve requests? Tool A tool to create manifests from a directory of files. E.g. Kustomize or Ksonnet. See Application Source Type . Configuration management tool See Tool . Configuration management plugin A custom tool.","title":"Core Concepts"},{"location":"core_concepts/#core-concepts","text":"Let's assume you're familiar with core Git, Docker, Kubernetes, Continuous Delivery, and GitOps concepts. Application A group of Kubernetes resources as defined by a manifest. This is a Custom Resource Definition (CRD). Application source type Which Tool is used to build the application. Target state The desired state of an application, as represented by files in a Git repository. Live state The live state of that application. What pods etc are deployed. Sync status Whether or not the live state matches the target state. Is the deployed application the same as Git says it should be? Sync The process of making an application move to its target state. E.g. by applying changes to a Kubernetes cluster. Sync operation status Whether or not a sync succeeded. Refresh Compare the latest code in Git with the live state. Figure out what is different. Health The health of the application, is it running correctly? Can it serve requests? Tool A tool to create manifests from a directory of files. E.g. Kustomize or Ksonnet. See Application Source Type . Configuration management tool See Tool . Configuration management plugin A custom tool.","title":"Core Concepts"},{"location":"faq/","text":"FAQ \u00b6 I've deleted/corrupted my repo and can't delete my app. \u00b6 Argo CD can't delete an app if it cannot generate manifests. You need to either: Reinstate/fix your repo. Delete the app using --cascade=false and then manually deleting the resources. Why is my application still OutOfSync immediately after a successful Sync? \u00b6 See Diffing documentation for reasons resources can be OutOfSync, and ways to configure Argo CD to ignore fields when differences are expected. Why is my application stuck in Progressing state? \u00b6 Argo CD provides health for several standard Kubernetes types. The Ingress and StatefulSet types have known issues which might cause health check to return Progressing state instead of Healthy . Ingress is considered healthy if status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . Some ingress controllers ( contour , traefik ) don't update status.loadBalancer.ingress field which causes Ingress to stuck in Progressing state forever. StatefulSet is considered healthy if value of status.updatedReplicas field matches to spec.replicas field. Due to Kubernetes bug kubernetes/kubernetes#68573 the status.updatedReplicas is not populated. So unless you run Kubernetes version which include the fix kubernetes/kubernetes#67570 StatefulSet might stay in Progressing state. Your StatefulSet or DaemonSet is using OnDelete instead of RollingUpdate strategy. See #1881 . As workaround Argo CD allows providing health check customization which overrides default behavior. I forgot the admin password, how do I reset it? \u00b6 By default the password is set to the name of the server pod, as per the getting started guide . To change the password, edit the argocd-secret secret and update the admin.password field with a new bcrypt hash. You can use a site like https://www.browserling.com/tools/bcrypt to generate a new hash. For example: # bcrypt(password)=$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa kubectl -n argocd patch secret argocd-secret \\ -p '{\"stringData\": { \"admin.password\": \"$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa\", \"admin.passwordMtime\": \"' $( date +%FT%T%Z ) '\" }}' Another option is to delete both the admin.password and admin.passwordMtime keys and restart argocd-server. This will set the password back to the pod name as per the getting started guide . Argo CD cannot deploy Helm Chart based applications without internet access, how can I solve it? \u00b6 Argo CD might fail to generate Helm chart manifests if the chart has dependencies located in external repositories. To solve the problem you need to make sure that requirements.yaml uses only internally available Helm repositories. Even if the chart uses only dependencies from internal repos Helm might decide to refresh stable repo. As workaround override stable repo URL in argocd-cm config map: data : # v1.2 or earlier use `helm.repositories` helm.repositories : | - url: http://<internal-helm-repo-host>:8080 name: stable # v1.3 or later use `repositories` with `type: helm` repositories : | - type: helm url: http://<internal-helm-repo-host>:8080 name: stable I've configured cluster secret but it does not show up in CLI/UI, how do I fix it? \u00b6 Check if cluster secret has argocd.argoproj.io/secret-type: cluster label. If secret has the label but the cluster is still not visible then make sure it might be a permission issue. Try to list clusters using admin user (e.g. argocd login --username admin && argocd cluster list ). Argo CD is unable to connect to my cluster, how do I troubleshoot it? \u00b6 Use the following steps to reconstruct configured cluster config and connect to your cluster manually using kubectl: kubectl exec -it <argocd-pod-name> bash # ssh into any argocd server pod argocd-util kubeconfig https://<cluster-url> /tmp/config --namespace argocd # generate your cluster config KUBECONFIG = /tmp/config kubectl get pods # test connection manually Now you can manually verify that cluster is accessible from the Argo CD pod. How Can I Terminate A Sync? \u00b6 To terminate the sync, click on the \"synchronisation\" then \"terminate\": Why Is My App Out Of Sync Even After Syncing? \u00b6 Is some cases, the tool you use may conflict with Argo CD by adding the app.kubernetes.io/instance label. E.g. using Kustomize common labels feature. Argo CD automatically sets the app.kubernetes.io/instance label and uses it to determine which resources form the app. If the tool does this too, this causes confusion. You can change this label by setting the application.instanceLabelKey value in the argocd-cm . We recommend that you use argocd.argoproj.io/instance . Note When you make this change your applications will become out of sync and will need re-syncing. See #1482 . Why Are My Resource Limits Out Of Sync? \u00b6 Kubernetes has normalized your resource limits when they are applied, and then Argo CD has then compared the version in your generated manifests to the normalized one is Kubernetes - they won't match. E.g. '1000m' normalized to '1' '0.1' normalized to '100m' '3072Mi' normalized to '3Gi' 3072 normalized to '3072' (quotes added) To fix this - replace your values with the normalized values. See #1615 How Do I Fix \"invalid cookie, longer than max length 4093\"? \u00b6 Argo CD uses a JWT as the auth token. You likely are part of many groups and have gone over the 4KB limit which is set for cookies. You can get the list of groups by opening \"developer tools -> network\" Click log in Find the call to <argocd_instance>/auth/callback?code=<random_string> Decode the token at https://jwt.io/. That will provide the list of teams that you can remove yourself from. See #2165 . Why Am I Getting rpc error: code = Unavailable desc = transport is closing When Using The CLI? \u00b6 Maybe you're behind a proxy that does not support HTTP 2? Try the --grpc-web flag.: argocd ... --grpc-web Why Am I Getting x509: certificate signed by unknown authority When Using The CLI? \u00b6 Your not running your server with correct certs. If you're not running in a production system (e.g. you're testing Argo CD out), try the --insecure flag: argocd ... --insecure Do not use --insecure in production","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#ive-deletedcorrupted-my-repo-and-cant-delete-my-app","text":"Argo CD can't delete an app if it cannot generate manifests. You need to either: Reinstate/fix your repo. Delete the app using --cascade=false and then manually deleting the resources.","title":"I've deleted/corrupted my repo and can't delete my app."},{"location":"faq/#why-is-my-application-still-outofsync-immediately-after-a-successful-sync","text":"See Diffing documentation for reasons resources can be OutOfSync, and ways to configure Argo CD to ignore fields when differences are expected.","title":"Why is my application still OutOfSync immediately after a successful Sync?"},{"location":"faq/#why-is-my-application-stuck-in-progressing-state","text":"Argo CD provides health for several standard Kubernetes types. The Ingress and StatefulSet types have known issues which might cause health check to return Progressing state instead of Healthy . Ingress is considered healthy if status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . Some ingress controllers ( contour , traefik ) don't update status.loadBalancer.ingress field which causes Ingress to stuck in Progressing state forever. StatefulSet is considered healthy if value of status.updatedReplicas field matches to spec.replicas field. Due to Kubernetes bug kubernetes/kubernetes#68573 the status.updatedReplicas is not populated. So unless you run Kubernetes version which include the fix kubernetes/kubernetes#67570 StatefulSet might stay in Progressing state. Your StatefulSet or DaemonSet is using OnDelete instead of RollingUpdate strategy. See #1881 . As workaround Argo CD allows providing health check customization which overrides default behavior.","title":"Why is my application stuck in Progressing state?"},{"location":"faq/#i-forgot-the-admin-password-how-do-i-reset-it","text":"By default the password is set to the name of the server pod, as per the getting started guide . To change the password, edit the argocd-secret secret and update the admin.password field with a new bcrypt hash. You can use a site like https://www.browserling.com/tools/bcrypt to generate a new hash. For example: # bcrypt(password)=$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa kubectl -n argocd patch secret argocd-secret \\ -p '{\"stringData\": { \"admin.password\": \"$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa\", \"admin.passwordMtime\": \"' $( date +%FT%T%Z ) '\" }}' Another option is to delete both the admin.password and admin.passwordMtime keys and restart argocd-server. This will set the password back to the pod name as per the getting started guide .","title":"I forgot the admin password, how do I reset it?"},{"location":"faq/#argo-cd-cannot-deploy-helm-chart-based-applications-without-internet-access-how-can-i-solve-it","text":"Argo CD might fail to generate Helm chart manifests if the chart has dependencies located in external repositories. To solve the problem you need to make sure that requirements.yaml uses only internally available Helm repositories. Even if the chart uses only dependencies from internal repos Helm might decide to refresh stable repo. As workaround override stable repo URL in argocd-cm config map: data : # v1.2 or earlier use `helm.repositories` helm.repositories : | - url: http://<internal-helm-repo-host>:8080 name: stable # v1.3 or later use `repositories` with `type: helm` repositories : | - type: helm url: http://<internal-helm-repo-host>:8080 name: stable","title":"Argo CD cannot deploy Helm Chart based applications without internet access, how can I solve it?"},{"location":"faq/#ive-configured-cluster-secret-but-it-does-not-show-up-in-cliui-how-do-i-fix-it","text":"Check if cluster secret has argocd.argoproj.io/secret-type: cluster label. If secret has the label but the cluster is still not visible then make sure it might be a permission issue. Try to list clusters using admin user (e.g. argocd login --username admin && argocd cluster list ).","title":"I've configured cluster secret but it does not show up in CLI/UI, how do I fix it?"},{"location":"faq/#argo-cd-is-unable-to-connect-to-my-cluster-how-do-i-troubleshoot-it","text":"Use the following steps to reconstruct configured cluster config and connect to your cluster manually using kubectl: kubectl exec -it <argocd-pod-name> bash # ssh into any argocd server pod argocd-util kubeconfig https://<cluster-url> /tmp/config --namespace argocd # generate your cluster config KUBECONFIG = /tmp/config kubectl get pods # test connection manually Now you can manually verify that cluster is accessible from the Argo CD pod.","title":"Argo CD is unable to connect to my cluster, how do I troubleshoot it?"},{"location":"faq/#how-can-i-terminate-a-sync","text":"To terminate the sync, click on the \"synchronisation\" then \"terminate\":","title":"How Can I Terminate A Sync?"},{"location":"faq/#why-is-my-app-out-of-sync-even-after-syncing","text":"Is some cases, the tool you use may conflict with Argo CD by adding the app.kubernetes.io/instance label. E.g. using Kustomize common labels feature. Argo CD automatically sets the app.kubernetes.io/instance label and uses it to determine which resources form the app. If the tool does this too, this causes confusion. You can change this label by setting the application.instanceLabelKey value in the argocd-cm . We recommend that you use argocd.argoproj.io/instance . Note When you make this change your applications will become out of sync and will need re-syncing. See #1482 .","title":"Why Is My App Out Of Sync Even After Syncing?"},{"location":"faq/#why-are-my-resource-limits-out-of-sync","text":"Kubernetes has normalized your resource limits when they are applied, and then Argo CD has then compared the version in your generated manifests to the normalized one is Kubernetes - they won't match. E.g. '1000m' normalized to '1' '0.1' normalized to '100m' '3072Mi' normalized to '3Gi' 3072 normalized to '3072' (quotes added) To fix this - replace your values with the normalized values. See #1615","title":"Why Are My Resource Limits Out Of Sync?"},{"location":"faq/#how-do-i-fix-invalid-cookie-longer-than-max-length-4093","text":"Argo CD uses a JWT as the auth token. You likely are part of many groups and have gone over the 4KB limit which is set for cookies. You can get the list of groups by opening \"developer tools -> network\" Click log in Find the call to <argocd_instance>/auth/callback?code=<random_string> Decode the token at https://jwt.io/. That will provide the list of teams that you can remove yourself from. See #2165 .","title":"How Do I Fix \"invalid cookie, longer than max length 4093\"?"},{"location":"faq/#why-am-i-getting-rpc-error-code-unavailable-desc-transport-is-closing-when-using-the-cli","text":"Maybe you're behind a proxy that does not support HTTP 2? Try the --grpc-web flag.: argocd ... --grpc-web","title":"Why Am I Getting rpc error: code = Unavailable desc = transport is closing When Using The CLI?"},{"location":"faq/#why-am-i-getting-x509-certificate-signed-by-unknown-authority-when-using-the-cli","text":"Your not running your server with correct certs. If you're not running in a production system (e.g. you're testing Argo CD out), try the --insecure flag: argocd ... --insecure Do not use --insecure in production","title":"Why Am I Getting x509: certificate signed by unknown authority When Using The CLI?"},{"location":"getting_started/","text":"Getting Started \u00b6 Tip This guide assumes you have a grounding in the tools that Argo CD is based on. Please read understanding the basics to learn about these tools. Requirements \u00b6 Installed kubectl command-line tool Have a kubeconfig file (default location is ~/.kube/config ). 1. Install Argo CD \u00b6 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml This will create a new namespace, argocd , where Argo CD services and application resources will live. On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com Note If you are not interested in UI, SSO, multi-cluster management and just want to pull changes into the cluster then you can disable authentication using --disable-auth flag and access Argo CD via CLI using --port-forward or --port-forward-namespace flags and proceed to step #6 : kubectl patch deploy argocd-server -n argocd -p '[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/command/-\", \"value\": \"--disable-auth\"}]' --type json 2. Download Argo CD CLI \u00b6 Download the latest Argo CD version from https://github.com/argoproj/argo-cd/releases/latest . More detailed installation instructions can be found via the CLI installation documentation . Also available in Mac Homebrew: brew tap argoproj/tap brew install argoproj/tap/argocd 3. Access The Argo CD API Server \u00b6 By default, the Argo CD API server is not exposed with an external IP. To access the API server, choose one of the following techniques to expose the Argo CD API server: Service Type Load Balancer \u00b6 Change the argocd-server service type to LoadBalancer : kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Ingress \u00b6 Follow the ingress documentation on how to configure Argo CD with ingress. Port Forwarding \u00b6 Kubectl port-forwarding can also be used to connect to the API server without exposing the service. kubectl port-forward svc/argocd-server -n argocd 8080 :443 The API server can then be accessed using the localhost:8080 4. Login Using The CLI \u00b6 The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the command: kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 Using the username admin and the password from above, login to Argo CD's IP or hostname: argocd login <ARGOCD_SERVER> Change the password using the command: argocd account update-password 5. Register A Cluster To Deploy Apps To (Optional) \u00b6 This step registers a cluster's credentials to Argo CD, and is only necessary when deploying to an external cluster. When deploying internally (to the same cluster that Argo CD is running in), https://kubernetes.default.svc should be used as the application's K8s API server address. First list all clusters contexts in your current kubconfig: argocd cluster add Choose a context name from the list and supply it to argocd cluster add CONTEXTNAME . For example, for docker-for-desktop context, run: argocd cluster add docker-for-desktop The above command installs a ServiceAccount ( argocd-manager ), into the kube-system namespace of that kubectl context, and binds the service account to an admin-level ClusterRole. Argo CD uses this service account token to perform its management tasks (i.e. deploy/monitoring). Note The rules of the argocd-manager-role role can be modified such that it only has create , update , patch , delete privileges to a limited set of namespaces, groups, kinds. However get , list , watch privileges are required at the cluster-scope for Argo CD to function. 6. Create An Application From A Git Repository \u00b6 An example repository containing a guestbook application is available at https://github.com/argoproj/argocd-example-apps.git to demonstrate how Argo CD works. Creating Apps Via CLI \u00b6 Note You can access Argo CD using port forwarding: add --port-forward-namespace argocd flag to every CLI command or set ARGOCD_OPTS environment variable: export ARGOCD_OPTS='--port-forward-namespace argocd' : argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default Creating Apps Via UI \u00b6 Open a browser to the Argo CD external UI, and login by visiting the IP/hostname in a browser and use the credentials set in step 4. After logging in, click the + New App button as shown below: Give your app the name guestbook , use the project default , and leave the sync policy as Manual : Connect the https://github.com/argoproj/argocd-example-apps.git repo to Argo CD by setting repository url to the github repo url, leave revision as HEAD , and set the path to guestbook : For Destination , set cluster to in-cluster and namespace to default : After filling out the information above, click Create at the top of the UI to create the guestbook application: 7. Sync (Deploy) The Application \u00b6 Once the guestbook application is created, you can now view its status: $ argocd app get guestbook Name: guestbook Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.164.88/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook Sync Policy: <none> Sync Status: OutOfSync from ( 1ff8a67 ) Health Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH apps Deployment default guestbook-ui OutOfSync Missing Service default guestbook-ui OutOfSync Missing The application status is initially in OutOfSync state since the application has yet to be deployed, and no Kubernetes resources have been created. To sync (deploy) the application, run: argocd app sync guestbook This command retrieves the manifests from the repository and performs a kubectl apply of the manifests. The guestbook app is now running and you can now view its resource components, logs, events, and assessed health status: From UI: \u00b6","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"Tip This guide assumes you have a grounding in the tools that Argo CD is based on. Please read understanding the basics to learn about these tools.","title":"Getting Started"},{"location":"getting_started/#requirements","text":"Installed kubectl command-line tool Have a kubeconfig file (default location is ~/.kube/config ).","title":"Requirements"},{"location":"getting_started/#1-install-argo-cd","text":"kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml This will create a new namespace, argocd , where Argo CD services and application resources will live. On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com Note If you are not interested in UI, SSO, multi-cluster management and just want to pull changes into the cluster then you can disable authentication using --disable-auth flag and access Argo CD via CLI using --port-forward or --port-forward-namespace flags and proceed to step #6 : kubectl patch deploy argocd-server -n argocd -p '[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/command/-\", \"value\": \"--disable-auth\"}]' --type json","title":"1. Install Argo CD"},{"location":"getting_started/#2-download-argo-cd-cli","text":"Download the latest Argo CD version from https://github.com/argoproj/argo-cd/releases/latest . More detailed installation instructions can be found via the CLI installation documentation . Also available in Mac Homebrew: brew tap argoproj/tap brew install argoproj/tap/argocd","title":"2. Download Argo CD CLI"},{"location":"getting_started/#3-access-the-argo-cd-api-server","text":"By default, the Argo CD API server is not exposed with an external IP. To access the API server, choose one of the following techniques to expose the Argo CD API server:","title":"3. Access The Argo CD API Server"},{"location":"getting_started/#service-type-load-balancer","text":"Change the argocd-server service type to LoadBalancer : kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'","title":"Service Type Load Balancer"},{"location":"getting_started/#ingress","text":"Follow the ingress documentation on how to configure Argo CD with ingress.","title":"Ingress"},{"location":"getting_started/#port-forwarding","text":"Kubectl port-forwarding can also be used to connect to the API server without exposing the service. kubectl port-forward svc/argocd-server -n argocd 8080 :443 The API server can then be accessed using the localhost:8080","title":"Port Forwarding"},{"location":"getting_started/#4-login-using-the-cli","text":"The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the command: kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 Using the username admin and the password from above, login to Argo CD's IP or hostname: argocd login <ARGOCD_SERVER> Change the password using the command: argocd account update-password","title":"4. Login Using The CLI"},{"location":"getting_started/#5-register-a-cluster-to-deploy-apps-to-optional","text":"This step registers a cluster's credentials to Argo CD, and is only necessary when deploying to an external cluster. When deploying internally (to the same cluster that Argo CD is running in), https://kubernetes.default.svc should be used as the application's K8s API server address. First list all clusters contexts in your current kubconfig: argocd cluster add Choose a context name from the list and supply it to argocd cluster add CONTEXTNAME . For example, for docker-for-desktop context, run: argocd cluster add docker-for-desktop The above command installs a ServiceAccount ( argocd-manager ), into the kube-system namespace of that kubectl context, and binds the service account to an admin-level ClusterRole. Argo CD uses this service account token to perform its management tasks (i.e. deploy/monitoring). Note The rules of the argocd-manager-role role can be modified such that it only has create , update , patch , delete privileges to a limited set of namespaces, groups, kinds. However get , list , watch privileges are required at the cluster-scope for Argo CD to function.","title":"5. Register A Cluster To Deploy Apps To (Optional)"},{"location":"getting_started/#6-create-an-application-from-a-git-repository","text":"An example repository containing a guestbook application is available at https://github.com/argoproj/argocd-example-apps.git to demonstrate how Argo CD works.","title":"6. Create An Application From A Git Repository"},{"location":"getting_started/#creating-apps-via-cli","text":"Note You can access Argo CD using port forwarding: add --port-forward-namespace argocd flag to every CLI command or set ARGOCD_OPTS environment variable: export ARGOCD_OPTS='--port-forward-namespace argocd' : argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default","title":"Creating Apps Via CLI"},{"location":"getting_started/#creating-apps-via-ui","text":"Open a browser to the Argo CD external UI, and login by visiting the IP/hostname in a browser and use the credentials set in step 4. After logging in, click the + New App button as shown below: Give your app the name guestbook , use the project default , and leave the sync policy as Manual : Connect the https://github.com/argoproj/argocd-example-apps.git repo to Argo CD by setting repository url to the github repo url, leave revision as HEAD , and set the path to guestbook : For Destination , set cluster to in-cluster and namespace to default : After filling out the information above, click Create at the top of the UI to create the guestbook application:","title":"Creating Apps Via UI"},{"location":"getting_started/#7-sync-deploy-the-application","text":"Once the guestbook application is created, you can now view its status: $ argocd app get guestbook Name: guestbook Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.164.88/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook Sync Policy: <none> Sync Status: OutOfSync from ( 1ff8a67 ) Health Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH apps Deployment default guestbook-ui OutOfSync Missing Service default guestbook-ui OutOfSync Missing The application status is initially in OutOfSync state since the application has yet to be deployed, and no Kubernetes resources have been created. To sync (deploy) the application, run: argocd app sync guestbook This command retrieves the manifests from the repository and performs a kubectl apply of the manifests. The guestbook app is now running and you can now view its resource components, logs, events, and assessed health status:","title":"7. Sync (Deploy) The Application"},{"location":"getting_started/#from-ui","text":"","title":"From UI:"},{"location":"understand_the_basics/","text":"Understand The Basics \u00b6 Before effectively using Argo CD, it is necessary to understand the underlying technology that the platform is built on. It is also necessary to understand the features being provided to you and how to use them. The section below provides some useful links to build up this understanding. Learn The Fundamentals \u00b6 Go through the online Docker and Kubernetes tutorials A Beginner-Friendly Introduction to Containers, VMs and Docker Introduction to Kubernetes Tutorials Hands on labs Depending on how you plan to template your applications: Kustomize Helm Ksonnet If you're integrating with Jenkins: Jenkins User Guide","title":"Understand The Basics"},{"location":"understand_the_basics/#understand-the-basics","text":"Before effectively using Argo CD, it is necessary to understand the underlying technology that the platform is built on. It is also necessary to understand the features being provided to you and how to use them. The section below provides some useful links to build up this understanding.","title":"Understand The Basics"},{"location":"understand_the_basics/#learn-the-fundamentals","text":"Go through the online Docker and Kubernetes tutorials A Beginner-Friendly Introduction to Containers, VMs and Docker Introduction to Kubernetes Tutorials Hands on labs Depending on how you plan to template your applications: Kustomize Helm Ksonnet If you're integrating with Jenkins: Jenkins User Guide","title":"Learn The Fundamentals"},{"location":"developer-guide/","text":"Overview \u00b6 You probably don't want to be reading this section of the docs. This part of the manual is aimed at people wanting to develop third-party applications that interact with Argo CD, e.g. An chat bot An Slack integration Note Please make sure you've completed the getting started guide .","title":"Overview"},{"location":"developer-guide/#overview","text":"You probably don't want to be reading this section of the docs. This part of the manual is aimed at people wanting to develop third-party applications that interact with Argo CD, e.g. An chat bot An Slack integration Note Please make sure you've completed the getting started guide .","title":"Overview"},{"location":"developer-guide/api-docs/","text":"API Docs \u00b6 You can find Swagger docs but setting the path /swagger-ui to your Argo CD UI's. E.g. http://localhost:8080/swagger-ui . Authorization \u00b6 You'll need to authorize your API using a bearer token. To get a token: $ curl $ARGOCD_SERVER /api/v1/session -d $'{\"username\":\"admin\",\"password\":\"password\"}' { \"token\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE1Njc4MTIzODcsImlzcyI6ImFyZ29jZCIsIm5iZiI6MTU2NzgxMjM4Nywic3ViIjoiYWRtaW4ifQ.ejyTgFxLhuY9mOBtKhcnvobg3QZXJ4_RusN_KIdVwao\" } <=v1.2 Then pass using the HTTP SetCookie header, prefixing with argocd.token : $ curl $ARGOCD_SERVER /api/v1/applications --cookie \"argocd.token= $ARGOCD_TOKEN \" { \"metadata\" : { \"selfLink\" : \"/apis/argoproj.io/v1alpha1/namespaces/argocd/applications\" , \"resourceVersion\" : \"37755\" } , \"items\" :... } v1.3 Then pass using the HTTP Authorization header, prefixing with Bearer : $ curl $ARGOCD_SERVER /api/v1/applications -H \"Authorization: Bearer $ARGOCD_TOKEN \" { \"metadata\" : { \"selfLink\" : \"/apis/argoproj.io/v1alpha1/namespaces/argocd/applications\" , \"resourceVersion\" : \"37755\" } , \"items\" :... }","title":"API Docs"},{"location":"developer-guide/api-docs/#api-docs","text":"You can find Swagger docs but setting the path /swagger-ui to your Argo CD UI's. E.g. http://localhost:8080/swagger-ui .","title":"API Docs"},{"location":"developer-guide/api-docs/#authorization","text":"You'll need to authorize your API using a bearer token. To get a token: $ curl $ARGOCD_SERVER /api/v1/session -d $'{\"username\":\"admin\",\"password\":\"password\"}' { \"token\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE1Njc4MTIzODcsImlzcyI6ImFyZ29jZCIsIm5iZiI6MTU2NzgxMjM4Nywic3ViIjoiYWRtaW4ifQ.ejyTgFxLhuY9mOBtKhcnvobg3QZXJ4_RusN_KIdVwao\" } <=v1.2 Then pass using the HTTP SetCookie header, prefixing with argocd.token : $ curl $ARGOCD_SERVER /api/v1/applications --cookie \"argocd.token= $ARGOCD_TOKEN \" { \"metadata\" : { \"selfLink\" : \"/apis/argoproj.io/v1alpha1/namespaces/argocd/applications\" , \"resourceVersion\" : \"37755\" } , \"items\" :... } v1.3 Then pass using the HTTP Authorization header, prefixing with Bearer : $ curl $ARGOCD_SERVER /api/v1/applications -H \"Authorization: Bearer $ARGOCD_TOKEN \" { \"metadata\" : { \"selfLink\" : \"/apis/argoproj.io/v1alpha1/namespaces/argocd/applications\" , \"resourceVersion\" : \"37755\" } , \"items\" :... }","title":"Authorization"},{"location":"developer-guide/ci/","text":"CI \u00b6 Troubleshooting Builds \u00b6 \"Check nothing has changed\" step fails \u00b6 If your PR fails the codegen CI step, you can either: (1) Simple - download the codgen.patch file from CircleCI and apply it: git apply codegen.patch git commit -am \"Applies codegen patch\" (2) Advanced - if you have the tools installed (see the contributing guide), run the following: make pre-commit git commit -am 'Ran pre-commit checks' Updating The Builder Image \u00b6 Login to Docker Hub: docker login Build image: make builder-image IMAGE_NAMESPACE = argoproj IMAGE_TAG = v1.0.0 Public CD \u00b6 https://cd.apps.argoproj.io/","title":"CI"},{"location":"developer-guide/ci/#ci","text":"","title":"CI"},{"location":"developer-guide/ci/#troubleshooting-builds","text":"","title":"Troubleshooting Builds"},{"location":"developer-guide/ci/#check-nothing-has-changed-step-fails","text":"If your PR fails the codegen CI step, you can either: (1) Simple - download the codgen.patch file from CircleCI and apply it: git apply codegen.patch git commit -am \"Applies codegen patch\" (2) Advanced - if you have the tools installed (see the contributing guide), run the following: make pre-commit git commit -am 'Ran pre-commit checks'","title":"\"Check nothing has changed\" step fails"},{"location":"developer-guide/ci/#updating-the-builder-image","text":"Login to Docker Hub: docker login Build image: make builder-image IMAGE_NAMESPACE = argoproj IMAGE_TAG = v1.0.0","title":"Updating The Builder Image"},{"location":"developer-guide/ci/#public-cd","text":"https://cd.apps.argoproj.io/","title":"Public CD"},{"location":"developer-guide/releasing/","text":"Releasing \u00b6 Make sure you are logged into Docker Hub: docker login Export the upstream repository and branch name, e.g.: REPO = upstream ; # or origin BRANCH = release-1.3 Set the VERSION environment variable: # release candidate VERSION = v1.3.0-rc1 # GA release VERSION = v1.3.1 Update VERSION and manifests with new version: git checkout $BRANCH echo ${ VERSION : 1 } > VERSION make dev-tools-image make manifests IMAGE_TAG = $VERSION git commit -am \"Update manifests to $VERSION \" git tag $VERSION Build, and push release to Docker Hub git clean -fd make release IMAGE_NAMESPACE = argoproj IMAGE_TAG = $VERSION DOCKER_PUSH = true git push $REPO $BRANCH git push $REPO $VERSION If GA, update stable tag: git tag stable --force && git push $REPO stable --force Update Github releases with: Getting started (copy from previous release) Changelog Binaries (e.g. dist/argocd-darwin-amd64 ). If GA, update Brew formula: git clone git@github.com:argoproj/homebrew-tap.git cd homebrew-tap ./update.sh argocd $VERSION git commit -am \"Update argocd to $VERSION \" git push Verify \u00b6 Locally: kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/ $VERSION /manifests/install.yaml Follow the Getting Started Guide . If GA: brew upgrade argocd /usr/local/bin/argocd version Sync Argo CD in https://cd.apps.argoproj.io/applications/argo-cd . Deploy the site .","title":"Releasing"},{"location":"developer-guide/releasing/#releasing","text":"Make sure you are logged into Docker Hub: docker login Export the upstream repository and branch name, e.g.: REPO = upstream ; # or origin BRANCH = release-1.3 Set the VERSION environment variable: # release candidate VERSION = v1.3.0-rc1 # GA release VERSION = v1.3.1 Update VERSION and manifests with new version: git checkout $BRANCH echo ${ VERSION : 1 } > VERSION make dev-tools-image make manifests IMAGE_TAG = $VERSION git commit -am \"Update manifests to $VERSION \" git tag $VERSION Build, and push release to Docker Hub git clean -fd make release IMAGE_NAMESPACE = argoproj IMAGE_TAG = $VERSION DOCKER_PUSH = true git push $REPO $BRANCH git push $REPO $VERSION If GA, update stable tag: git tag stable --force && git push $REPO stable --force Update Github releases with: Getting started (copy from previous release) Changelog Binaries (e.g. dist/argocd-darwin-amd64 ). If GA, update Brew formula: git clone git@github.com:argoproj/homebrew-tap.git cd homebrew-tap ./update.sh argocd $VERSION git commit -am \"Update argocd to $VERSION \" git push","title":"Releasing"},{"location":"developer-guide/releasing/#verify","text":"Locally: kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/ $VERSION /manifests/install.yaml Follow the Getting Started Guide . If GA: brew upgrade argocd /usr/local/bin/argocd version Sync Argo CD in https://cd.apps.argoproj.io/applications/argo-cd . Deploy the site .","title":"Verify"},{"location":"developer-guide/site/","text":"Site \u00b6 Developing And Testing \u00b6 The web site is build using mkdocs and mkdocs-material . To test: make serve-docs Check for broken external links: make lint-docs Deploying \u00b6 make publish-docs Analytics \u00b6 Tip Don't forget to disable your ad-blocker when testing. We collect Google Analytics .","title":"Site"},{"location":"developer-guide/site/#site","text":"","title":"Site"},{"location":"developer-guide/site/#developing-and-testing","text":"The web site is build using mkdocs and mkdocs-material . To test: make serve-docs Check for broken external links: make lint-docs","title":"Developing And Testing"},{"location":"developer-guide/site/#deploying","text":"make publish-docs","title":"Deploying"},{"location":"developer-guide/site/#analytics","text":"Tip Don't forget to disable your ad-blocker when testing. We collect Google Analytics .","title":"Analytics"},{"location":"developer-guide/test-e2e/","text":"E2E Tests \u00b6 The directory contains E2E tests and test applications. The test assume that Argo CD services are installed into argocd-e2e namespace or cluster in current context. One throw-away namespace argocd-e2e*** is created prior to tests execute. The throw-away namespace is used as a target namespace for test applications. The test/e2e/testdata directory contains various Argo CD applications. Before test execution directory is copies into /tmp/argocd-e2e*** temp directory and used in tests as a Git repository via file url: file:///tmp/argocd-e2e*** . Running Tests Locally \u00b6 Start the e2e version make start-e2e Run the tests: make test-e2e You can observe the tests by using the UI http://localhost:8080/applications . Configuration of E2E Tests execution \u00b6 The Makefile's start-e2e target starts instances of ArgoCD on your local machine, of which the most will require a network listener. If for whatever reason you already have network services on your machine listening on the same ports, the e2e tests will not be able to run. You can derive from the defaults by setting the following environment variables before you run make start-e2e : ARGOCD_E2E_APISERVER_PORT : Listener port for argocd-server (default: 8080 ) ARGOCD_E2E_REPOSERVER_PORT : Listener port for argocd-reposerver (default: 8081 ) ARGOCD_E2E_DEX_PORT : Listener port for dex (default: 5556 ) ARGOCD_E2E_REDIS_PORT : Listener port for redis (default: 6379 ) ARGOCD_E2E_YARN_CMD : Command to use for starting the UI via Yarn (default: yarn ) If you have changed the port for argocd-server , be sure to also set ARGOCD_SERVER environment variable to point to that port, e.g. export ARGOCD_SERVER=localhost:8888 before running make test-e2e so that the test will communicate to the correct server component. CI Set-up \u00b6 The tests are executed by Argo Workflow defined at .argo-ci/ci.yaml . CI job The builds an Argo CD image, deploy argo cd components into throw-away kubernetes cluster provisioned using k3s and run e2e tests against it. Test Isolation \u00b6 Some effort has been made to balance test isolation with speed. Tests are isolated as follows as each test gets: A random 5 character ID. A unique Git repository containing the testdata in /tmp/argocd-e2e/${id} . A namespace argocd-e2e-ns-${id} . An primary name for the app argocd-e2e-${id} . Troubleshooting \u00b6 Tests fails to delete argocd-e2e-ns-* namespaces. This maybe due to the metrics server, run this: kubectl api-resources If it exits with status code 1, run: kubectl delete apiservice v1beta1.metrics.k8s.io Remove /spec/finalizers from the namespace kubectl edit ns argocd-e2e-ns-*","title":"E2E Tests"},{"location":"developer-guide/test-e2e/#e2e-tests","text":"The directory contains E2E tests and test applications. The test assume that Argo CD services are installed into argocd-e2e namespace or cluster in current context. One throw-away namespace argocd-e2e*** is created prior to tests execute. The throw-away namespace is used as a target namespace for test applications. The test/e2e/testdata directory contains various Argo CD applications. Before test execution directory is copies into /tmp/argocd-e2e*** temp directory and used in tests as a Git repository via file url: file:///tmp/argocd-e2e*** .","title":"E2E Tests"},{"location":"developer-guide/test-e2e/#running-tests-locally","text":"Start the e2e version make start-e2e Run the tests: make test-e2e You can observe the tests by using the UI http://localhost:8080/applications .","title":"Running Tests Locally"},{"location":"developer-guide/test-e2e/#configuration-of-e2e-tests-execution","text":"The Makefile's start-e2e target starts instances of ArgoCD on your local machine, of which the most will require a network listener. If for whatever reason you already have network services on your machine listening on the same ports, the e2e tests will not be able to run. You can derive from the defaults by setting the following environment variables before you run make start-e2e : ARGOCD_E2E_APISERVER_PORT : Listener port for argocd-server (default: 8080 ) ARGOCD_E2E_REPOSERVER_PORT : Listener port for argocd-reposerver (default: 8081 ) ARGOCD_E2E_DEX_PORT : Listener port for dex (default: 5556 ) ARGOCD_E2E_REDIS_PORT : Listener port for redis (default: 6379 ) ARGOCD_E2E_YARN_CMD : Command to use for starting the UI via Yarn (default: yarn ) If you have changed the port for argocd-server , be sure to also set ARGOCD_SERVER environment variable to point to that port, e.g. export ARGOCD_SERVER=localhost:8888 before running make test-e2e so that the test will communicate to the correct server component.","title":"Configuration of E2E Tests execution"},{"location":"developer-guide/test-e2e/#ci-set-up","text":"The tests are executed by Argo Workflow defined at .argo-ci/ci.yaml . CI job The builds an Argo CD image, deploy argo cd components into throw-away kubernetes cluster provisioned using k3s and run e2e tests against it.","title":"CI Set-up"},{"location":"developer-guide/test-e2e/#test-isolation","text":"Some effort has been made to balance test isolation with speed. Tests are isolated as follows as each test gets: A random 5 character ID. A unique Git repository containing the testdata in /tmp/argocd-e2e/${id} . A namespace argocd-e2e-ns-${id} . An primary name for the app argocd-e2e-${id} .","title":"Test Isolation"},{"location":"developer-guide/test-e2e/#troubleshooting","text":"Tests fails to delete argocd-e2e-ns-* namespaces. This maybe due to the metrics server, run this: kubectl api-resources If it exits with status code 1, run: kubectl delete apiservice v1beta1.metrics.k8s.io Remove /spec/finalizers from the namespace kubectl edit ns argocd-e2e-ns-*","title":"Troubleshooting"},{"location":"operator-manual/","text":"Overview \u00b6 This guide is for administrator and operator wanting to install and configure Argo CD for other developers. Note Please make sure you've completed the getting started guide .","title":"Overview"},{"location":"operator-manual/#overview","text":"This guide is for administrator and operator wanting to install and configure Argo CD for other developers. Note Please make sure you've completed the getting started guide .","title":"Overview"},{"location":"operator-manual/architecture/","text":"Architectural Overview \u00b6 Components \u00b6 API Server \u00b6 The API server is a gRPC/REST server which exposes the API consumed by the Web UI, CLI, and CI/CD systems. It has the following responsibilities: application management and status reporting invoking of application operations (e.g. sync, rollback, user-defined actions) repository and cluster credential management (stored as K8s secrets) authentication and auth delegation to external identity providers RBAC enforcement listener/forwarder for Git webhook events Repository Server \u00b6 The repository server is an internal service which maintains a local cache of the Git repository holding the application manifests. It is responsible for generating and returning the Kubernetes manifests when provided the following inputs: repository URL revision (commit, tag, branch) application path template specific settings: parameters, ksonnet environments, helm values.yaml Application Controller \u00b6 The application controller is a Kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the repo). It detects OutOfSync application state and optionally takes corrective action. It is responsible for invoking any user-defined hooks for lifecycle events (PreSync, Sync, PostSync)","title":"Architectural Overview"},{"location":"operator-manual/architecture/#architectural-overview","text":"","title":"Architectural Overview"},{"location":"operator-manual/architecture/#components","text":"","title":"Components"},{"location":"operator-manual/architecture/#api-server","text":"The API server is a gRPC/REST server which exposes the API consumed by the Web UI, CLI, and CI/CD systems. It has the following responsibilities: application management and status reporting invoking of application operations (e.g. sync, rollback, user-defined actions) repository and cluster credential management (stored as K8s secrets) authentication and auth delegation to external identity providers RBAC enforcement listener/forwarder for Git webhook events","title":"API Server"},{"location":"operator-manual/architecture/#repository-server","text":"The repository server is an internal service which maintains a local cache of the Git repository holding the application manifests. It is responsible for generating and returning the Kubernetes manifests when provided the following inputs: repository URL revision (commit, tag, branch) application path template specific settings: parameters, ksonnet environments, helm values.yaml","title":"Repository Server"},{"location":"operator-manual/architecture/#application-controller","text":"The application controller is a Kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the repo). It detects OutOfSync application state and optionally takes corrective action. It is responsible for invoking any user-defined hooks for lifecycle events (PreSync, Sync, PostSync)","title":"Application Controller"},{"location":"operator-manual/cluster-bootstrapping/","text":"Cluster Bootstrapping \u00b6 This guide for operators who have already installed Argo CD, and have a new cluster and are looking to install many apps in that cluster. There's no one particular pattern to solve this problem, e.g. you could write a script to create your apps, or you could even manually create them. However, users of Argo CD tend to use the app of apps pattern . App Of Apps Pattern \u00b6 Declaratively specify one Argo CD app that consists only of other apps. Helm Example \u00b6 This example shows how to use Helm to achieve this. You can, of course, use another tool if you like. A typical layout of your Git repository for this might be: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 guestbook.yaml \u2502 \u251c\u2500\u2500 helm-dependency.yaml \u2502 \u251c\u2500\u2500 helm-guestbook.yaml \u2502 \u2514\u2500\u2500 kustomize-guestbook.yaml \u2514\u2500\u2500 values.yaml Chart.yaml is boiler-plate. templates contains one file for each child app, roughly: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook namespace : argocd finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : argocd server : {{ .Values.spec.destination.server }} project : default source : path : guestbook repoURL : https://github.com/argoproj/argocd-example-apps targetRevision : HEAD The sync policy to automated + prune, so that child apps are automatically created, synced, and deleted when the manifest is changed, but you may wish to disable this. I've also added the finalizer, which will ensure that your apps are deleted correctly. Fix the revision to a specific Git commit SHA to make sure that, even if the child apps repo changes, the app will only change when the parent app change that revision. Alternatively, you can set it to HEAD or a branch name. As you probably want to override the cluster server, this is a templated values. values.yaml contains the default values: spec : destination : server : https://kubernetes.default.svc Next, you need to create and sync your parent app, e.g. via the CLI: argocd app create apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/argoproj/argocd-example-apps.git \\ --path apps argocd app sync apps The parent app will appear as in-sync but the child apps will be out of sync: You can either sync via the UI, firstly filter by the correct label: Then select the \"out of sync\" apps and sync: Or, via the CLI: argocd app sync -l app.kubernetes.io/instance = apps View the example on Github .","title":"Cluster Bootstrapping"},{"location":"operator-manual/cluster-bootstrapping/#cluster-bootstrapping","text":"This guide for operators who have already installed Argo CD, and have a new cluster and are looking to install many apps in that cluster. There's no one particular pattern to solve this problem, e.g. you could write a script to create your apps, or you could even manually create them. However, users of Argo CD tend to use the app of apps pattern .","title":"Cluster Bootstrapping"},{"location":"operator-manual/cluster-bootstrapping/#app-of-apps-pattern","text":"Declaratively specify one Argo CD app that consists only of other apps.","title":"App Of Apps Pattern"},{"location":"operator-manual/cluster-bootstrapping/#helm-example","text":"This example shows how to use Helm to achieve this. You can, of course, use another tool if you like. A typical layout of your Git repository for this might be: \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 guestbook.yaml \u2502 \u251c\u2500\u2500 helm-dependency.yaml \u2502 \u251c\u2500\u2500 helm-guestbook.yaml \u2502 \u2514\u2500\u2500 kustomize-guestbook.yaml \u2514\u2500\u2500 values.yaml Chart.yaml is boiler-plate. templates contains one file for each child app, roughly: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook namespace : argocd finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : argocd server : {{ .Values.spec.destination.server }} project : default source : path : guestbook repoURL : https://github.com/argoproj/argocd-example-apps targetRevision : HEAD The sync policy to automated + prune, so that child apps are automatically created, synced, and deleted when the manifest is changed, but you may wish to disable this. I've also added the finalizer, which will ensure that your apps are deleted correctly. Fix the revision to a specific Git commit SHA to make sure that, even if the child apps repo changes, the app will only change when the parent app change that revision. Alternatively, you can set it to HEAD or a branch name. As you probably want to override the cluster server, this is a templated values. values.yaml contains the default values: spec : destination : server : https://kubernetes.default.svc Next, you need to create and sync your parent app, e.g. via the CLI: argocd app create apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/argoproj/argocd-example-apps.git \\ --path apps argocd app sync apps The parent app will appear as in-sync but the child apps will be out of sync: You can either sync via the UI, firstly filter by the correct label: Then select the \"out of sync\" apps and sync: Or, via the CLI: argocd app sync -l app.kubernetes.io/instance = apps View the example on Github .","title":"Helm Example"},{"location":"operator-manual/custom_tools/","text":"Custom Tooling \u00b6 Argo CD bundles preferred versions of its supported templating tools (helm, kustomize, ks, jsonnet) as part of its container images. Sometimes, it may be desired to use a specific version of a tool other than what Argo CD bundles. Some reasons to do this might be: To upgrade/downgrade to a specific version of a tool due to bugs or bug fixes. To install additional dependencies which to be used by kustomize's configmap/secret generators (e.g. curl, vault, gpg, AWS CLI) To install a config management plugin As the Argo CD repo-server is the single service responsible for generating Kubernetes manifests, it can be customized to use alternative toolchain required by your environment. Adding Tools Via Volume Mounts \u00b6 The first technique is to use an init container and a volumeMount to copy a different verison of a tool into the repo-server container. In the following example, an init container is overwriting the helm binary with a different version than what is bundled in Argo CD: spec : # 1. Define an emptyDir volume which will hold the custom binaries volumes : - name : custom-tools emptyDir : {} # 2. Use an init container to download/copy custom binaries into the emptyDir initContainers : - name : download-tools image : alpine:3.8 command : [ sh , -c ] args : - wget -qO- https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz | tar -xvzf - && mv linux-amd64/helm /custom-tools/ volumeMounts : - mountPath : /custom-tools name : custom-tools # 3. Volume mount the custom binary to the bin directory (overriding the existing version) containers : - name : argocd-repo-server volumeMounts : - mountPath : /usr/local/bin/helm name : custom-tools subPath : helm BYOI (Build Your Own Image) \u00b6 Sometimes replacing a binary isn't sufficient and you need to install other dependencies. The following example builds an entirely customized repo-server from a Dockerfile, installing extra dependencies that may be needed for generating manifests. FROM argoproj/argocd:latest # Switch to root for the ability to perform install USER root # Install tools needed for your repo-server to retrieve & decrypt secrets, render manifests # (e.g. curl, awscli, gpg, sops) RUN apt-get update && \\ apt-get install -y \\ curl \\ awscli \\ gpg && \\ apt-get clean && \\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* && \\ curl -o /usr/local/bin/sops -L https://github.com/mozilla/sops/releases/download/3.2.0/sops-3.2.0.linux && \\ chmod +x /usr/local/bin/sops # Switch back to non-root user USER argocd","title":"Custom Tooling"},{"location":"operator-manual/custom_tools/#custom-tooling","text":"Argo CD bundles preferred versions of its supported templating tools (helm, kustomize, ks, jsonnet) as part of its container images. Sometimes, it may be desired to use a specific version of a tool other than what Argo CD bundles. Some reasons to do this might be: To upgrade/downgrade to a specific version of a tool due to bugs or bug fixes. To install additional dependencies which to be used by kustomize's configmap/secret generators (e.g. curl, vault, gpg, AWS CLI) To install a config management plugin As the Argo CD repo-server is the single service responsible for generating Kubernetes manifests, it can be customized to use alternative toolchain required by your environment.","title":"Custom Tooling"},{"location":"operator-manual/custom_tools/#adding-tools-via-volume-mounts","text":"The first technique is to use an init container and a volumeMount to copy a different verison of a tool into the repo-server container. In the following example, an init container is overwriting the helm binary with a different version than what is bundled in Argo CD: spec : # 1. Define an emptyDir volume which will hold the custom binaries volumes : - name : custom-tools emptyDir : {} # 2. Use an init container to download/copy custom binaries into the emptyDir initContainers : - name : download-tools image : alpine:3.8 command : [ sh , -c ] args : - wget -qO- https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz | tar -xvzf - && mv linux-amd64/helm /custom-tools/ volumeMounts : - mountPath : /custom-tools name : custom-tools # 3. Volume mount the custom binary to the bin directory (overriding the existing version) containers : - name : argocd-repo-server volumeMounts : - mountPath : /usr/local/bin/helm name : custom-tools subPath : helm","title":"Adding Tools Via Volume Mounts"},{"location":"operator-manual/custom_tools/#byoi-build-your-own-image","text":"Sometimes replacing a binary isn't sufficient and you need to install other dependencies. The following example builds an entirely customized repo-server from a Dockerfile, installing extra dependencies that may be needed for generating manifests. FROM argoproj/argocd:latest # Switch to root for the ability to perform install USER root # Install tools needed for your repo-server to retrieve & decrypt secrets, render manifests # (e.g. curl, awscli, gpg, sops) RUN apt-get update && \\ apt-get install -y \\ curl \\ awscli \\ gpg && \\ apt-get clean && \\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* && \\ curl -o /usr/local/bin/sops -L https://github.com/mozilla/sops/releases/download/3.2.0/sops-3.2.0.linux && \\ chmod +x /usr/local/bin/sops # Switch back to non-root user USER argocd","title":"BYOI (Build Your Own Image)"},{"location":"operator-manual/declarative-setup/","text":"Declarative Setup \u00b6 Argo CD applications, projects and settings can be defined declaratively using Kubernetes manifests. Quick Reference \u00b6 Name Kind Description argocd-cm.yaml ConfigMap General Argo CD configuration argocd-secret.yaml Secret Password, Certificates, Signing Key argocd-rbac-cm.yaml ConfigMap RBAC Configuration argocd-tls-certs-cm.yaml ConfigMap Custom TLS certificates for connecting Git repositories via HTTPS (v1.2 and later) argocd-ssh-known-hosts-cm.yaml ConfigMap SSH known hosts data for connecting Git repositories via SSH (v1.2 and later) application.yaml Application Example application spec project.yaml AppProject Example project spec Applications \u00b6 The Application CRD is the Kubernetes resource object representing a deployed application instance in an environment. It is defined by two key pieces of information: source reference to the desired state in Git (repository, revision, path, environment) destination reference to the target cluster and namespace. A minimal Application spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook namespace : argocd spec : project : default source : repoURL : https://github.com/argoproj/argocd-example-apps.git targetRevision : HEAD path : guestbook destination : server : https://kubernetes.default.svc namespace : guestbook See application.yaml for additional fields Note The namespace must match the namespace of your Argo cd, typically this is argocd . Warning By default, deleting an application will not perform a cascade delete, thereby deleting its resources. You must add the finalizer if you want this behaviour - which you may well not want. metadata : finalizers : - resources-finalizer.argocd.argoproj.io App of Apps \u00b6 You can create an app that creates other apps, which in turn can create other apps. This allows you to declaratively manage a group of app that can be deployed and configured in concert. See cluster bootstrapping . Projects \u00b6 The AppProject CRD is the Kubernetes resource object representing a logical grouping of applications. It is defined by the following key pieces of information: sourceRepos reference to the repositories that applications within the project can pull manifests from. destinations reference to clusters and namespaces that applications within the project can deploy into. roles list of entities with definitions of their access to resources within the project. An example spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : my-project namespace : argocd spec : description : Example Project # Allow manifests to deploy from any Git repos sourceRepos : - '*' # Only permit applications to deploy to the guestbook namespace in the same cluster destinations : - namespace : guestbook server : https://kubernetes.default.svc # Deny all cluster-scoped resources from being created, except for Namespace clusterResourceWhitelist : - group : '' kind : Namespace # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy namespaceResourceBlacklist : - group : '' kind : ResourceQuota - group : '' kind : LimitRange - group : '' kind : NetworkPolicy roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - my-oidc-group # A role which provides sync privileges to only the guestbook-dev application, e.g. to provide # sync privileges to a CI system - name : ci-role description : Sync privileges for guestbook-dev policies : - p, proj:my-project:ci-role, applications, sync, my-project/guestbook-dev, allow # NOTE: JWT tokens can only be generated by the API server and the token is not persisted # anywhere by Argo CD. It can be prematurely revoked by removing the entry from this list. jwtTokens : - iat : 1535390316 Repositories \u00b6 Repository credentials are stored in secret. Use following steps to configure a repo: Create secret which contains repository credentials. Consider using bitnami-labs/sealed-secrets to store encrypted secret definition as a Kubernetes manifest. Register repository in the argocd-cm config map. Each repository must have url field and, depending on whether you connect using HTTPS or SSH, usernameSecret and passwordSecret (for HTTPS) or sshPrivateKeySecret (for SSH). Example for HTTPS: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: https://github.com/argoproj/my-private-repository passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username Example for SSH: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: git@github.com:argoproj/my-private-repository sshPrivateKeySecret: name: my-secret key: sshPrivateKey Tip The Kubernetes documentation has instructions for creating a secret containing a private key . Repository Credentials \u00b6 Earlier than v1.4 If you want to use the same credentials for multiple repositories, you can use repository.credentials : apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: https://github.com/argoproj/private-repo - url: https://github.com/argoproj/other-private-repo repository.credentials : | - url: https://github.com/argoproj passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username Argo CD will only use the credentials if you omit usernameSecret , passwordSecret , and sshPrivateKeySecret fields ( insecureIgnoreHostKey is ignored). A credential may be match if it's URL is the prefix of the repository's URL. The means that credentials may match, e.g in the above example both https://github.com/argoproj and https://github.com would match. Argo CD selects the first one that matches. Tip Order your credentials with the most specific at the top and the least specific at the bottom. A complete example. apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | # this has it's own credentials - url: https://github.com/argoproj/private-repo passwordSecret: name: private-repo-secret key: password usernameSecret: name: private-repo-secret key: username sshPrivateKeySecret: name: private-repo-secret key: sshPrivateKey - url: https://github.com/argoproj/other-private-repo - url: https://github.com/otherproj/another-private-repo repository.credentials : | # this will be used for the second repo - url: https://github.com/argoproj passwordSecret: name: other-private-repo-secret key: password usernameSecret: name: other-private-repo-secret key: username sshPrivateKeySecret: name: other-private-repo-secret key: sshPrivateKey # this will be used for the third repo - url: https://github.com passwordSecret: name: another-private-repo-secret key: password usernameSecret: name: another-private-repo-secret key: username sshPrivateKeySecret: name: another-private-repo-secret key: sshPrivateKey v1.4 or later If you want to use the same credentials for multiple repositories, you can use repository.credentials to configure credential templates. Credential templates can carry the same credentials information as repositories. apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: https://github.com/argoproj/private-repo - url: https://github.com/argoproj/other-private-repo repository.credentials : | - url: https://github.com/argoproj passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username In the above example, every repository accessed via HTTPS whose URL is prefixed with https://github.com/argoproj would use a username stored in the key username and a password stored in the key password of the secret my-secret for connecting to Git. In order for ArgoCD to use a credential template for any given repository, the following conditions must be met: The repository must either not be configured at all, or if configured, must not contain any credential information (i.e. contain none of sshPrivateKeySecret , usernameSecret , passwordSecret ) The URL configured for a credential template (e.g. https://github.com/argoproj ) must match as prefix for the repository URL (e.g. https://github.com/argoproj/argocd-example-apps ). Note Matching credential template URL prefixes is done on a best match effort, so the longest (best) match will take precedence. The order of definition is not important, as opposed to pre v1.4 configuration. The following keys are valid to refer to credential secrets: SSH repositories \u00b6 sshPrivateKeySecret refers to a secret where an SSH private key is stored for accessing the repositories HTTPS repositories \u00b6 usernameSecret and passwordSecret refer to secrets where username and/or password are stored for accessing the repositories tlsClientCertData and tlsClientCertKey refer to secrets where a TLS client certificate ( tlsClientCertData ) and the corresponding private key tlsClientCertKey are stored for accessing the repositories Repositories using self-signed TLS certificates (or are signed by custom CA) \u00b6 v1.2 or later You can manage the TLS certificates used to verify the authenticity of your repository servers in a ConfigMap object named argocd-tls-certs-cm . The data section should contain a map, with the repository server's hostname part (not the complete URL) as key, and the certificate(s) in PEM format as data. So, if you connect to a repository with the URL https://server.example.com/repos/my-repo , you should use server.example.com as key. The certificate data should be either the server's certificate (in case of self-signed certificate) or the certificate of the CA that was used to sign the server's certificate. You can configure multiple certificates for each server, e.g. if you are having a certificate roll-over planned. If there are no dedicated certificates configured for a repository server, the system's default trust store is used for validating the server's repository. This should be good enough for most (if not all) public Git repository services such as GitLab, GitHub and Bitbucket as well as most privately hosted sites which use certificates from well-known CAs, including Let's Encrypt certificates. An example ConfigMap object: apiVersion : v1 kind : ConfigMap metadata : name : argocd-tls-certs-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : server.example.com : | -----BEGIN CERTIFICATE----- MIIF1zCCA7+gAwIBAgIUQdTcSHY2Sxd3Tq/v1eIEZPCNbOowDQYJKoZIhvcNAQEL BQAwezELMAkGA1UEBhMCREUxFTATBgNVBAgMDExvd2VyIFNheG9ueTEQMA4GA1UE BwwHSGFub3ZlcjEVMBMGA1UECgwMVGVzdGluZyBDb3JwMRIwEAYDVQQLDAlUZXN0 c3VpdGUxGDAWBgNVBAMMD2Jhci5leGFtcGxlLmNvbTAeFw0xOTA3MDgxMzU2MTda Fw0yMDA3MDcxMzU2MTdaMHsxCzAJBgNVBAYTAkRFMRUwEwYDVQQIDAxMb3dlciBT YXhvbnkxEDAOBgNVBAcMB0hhbm92ZXIxFTATBgNVBAoMDFRlc3RpbmcgQ29ycDES MBAGA1UECwwJVGVzdHN1aXRlMRgwFgYDVQQDDA9iYXIuZXhhbXBsZS5jb20wggIi MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQCv4mHMdVUcafmaSHVpUM0zZWp5 NFXfboxA4inuOkE8kZlbGSe7wiG9WqLirdr39Ts+WSAFA6oANvbzlu3JrEQ2CHPc CNQm6diPREFwcDPFCe/eMawbwkQAPVSHPts0UoRxnpZox5pn69ghncBR+jtvx+/u P6HdwW0qqTvfJnfAF1hBJ4oIk2AXiip5kkIznsAh9W6WRy6nTVCeetmIepDOGe0G ZJIRn/OfSz7NzKylfDCat2z3EAutyeT/5oXZoWOmGg/8T7pn/pR588GoYYKRQnp+ YilqCPFX+az09EqqK/iHXnkdZ/Z2fCuU+9M/Zhrnlwlygl3RuVBI6xhm/ZsXtL2E Gxa61lNy6pyx5+hSxHEFEJshXLtioRd702VdLKxEOuYSXKeJDs1x9o6cJ75S6hko Ml1L4zCU+xEsMcvb1iQ2n7PZdacqhkFRUVVVmJ56th8aYyX7KNX6M9CD+kMpNm6J kKC1li/Iy+RI138bAvaFplajMF551kt44dSvIoJIbTr1LigudzWPqk31QaZXV/4u kD1n4p/XMc9HYU/was/CmQBFqmIZedTLTtK7clkuFN6wbwzdo1wmUNgnySQuMacO gxhHxxzRWxd24uLyk9Px+9U3BfVPaRLiOPaPoC58lyVOykjSgfpgbus7JS69fCq7 bEH4Jatp/10zkco+UQIDAQABo1MwUTAdBgNVHQ4EFgQUjXH6PHi92y4C4hQpey86 r6+x1ewwHwYDVR0jBBgwFoAUjXH6PHi92y4C4hQpey86r6+x1ewwDwYDVR0TAQH/ BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAgEAFE4SdKsX9UsLy+Z0xuHSxhTd0jfn Iih5mtzb8CDNO5oTw4z0aMeAvpsUvjJ/XjgxnkiRACXh7K9hsG2r+ageRWGevyvx CaRXFbherV1kTnZw4Y9/pgZTYVWs9jlqFOppz5sStkfjsDQ5lmPJGDii/StENAz2 XmtiPOgfG9Upb0GAJBCuKnrU9bIcT4L20gd2F4Y14ccyjlf8UiUi192IX6yM9OjT +TuXwZgqnTOq6piVgr+FTSa24qSvaXb5z/mJDLlk23npecTouLg83TNSn3R6fYQr d/Y9eXuUJ8U7/qTh2Ulz071AO9KzPOmleYPTx4Xty4xAtWi1QE5NHW9/Ajlv5OtO OnMNWIs7ssDJBsB7VFC8hcwf79jz7kC0xmQqDfw51Xhhk04kla+v+HZcFW2AO9so 6ZdVHHQnIbJa7yQJKZ+hK49IOoBR6JgdB5kymoplLLiuqZSYTcwSBZ72FYTm3iAr jzvt1hxpxVDmXvRnkhRrIRhK4QgJL0jRmirBjDY+PYYd7bdRIjN7WNZLFsgplnS8 9w6CwG32pRlm0c8kkiQ7FXA6BYCqOsDI8f1VGQv331OpR2Ck+FTv+L7DAmg6l37W +LB9LGh4OAp68ImTjqf6ioGKG0RBSznwME+r4nXtT1S/qLR6ASWUS4ViWRhbRlNK XWyb96wrUlv+E8I= -----END CERTIFICATE----- Note The argocd-tls-certs-cm ConfigMap will be mounted as a volume at the mount path /app/config/tls in the pods of argocd-server and argocd-repo-server . It will create files for each data key in the mount path directory, so above example would leave the file /app/config/tls/server.example.com , which contains the certificate data. It might take a while for changes in the ConfigMap to be reflected in your pods, depending on your Kubernetes configuration. SSH known host public keys \u00b6 If you are connecting repositories via SSH, ArgoCD will need to know the SSH known hosts public key of the repository servers. You can manage the SSH known hosts data in the ConfigMap named argocd-ssh-known-hosts-cm . This ConfigMap contains a single key/value pair, with ssh_known_hosts as the key and the actual public keys of the SSH servers as data. As opposed to TLS configuration, the public key(s) of each single repository server ArgoCD will connect via SSH must be configured, otherwise the connections to the repository will fail. There is no fallback. The data can be copied from any existing ssh_known_hosts file, or from the output of the ssh-keyscan utility. The basic format is <servername> <keydata> , one entry per line. An example ConfigMap object: apiVersion : v1 kind : ConfigMap metadata : name : argocd-ssh-known-hosts-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : ssh_known_hosts : | bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAubiN81eDcafrgMeLzaFPsw2kNvEcqTKl/VqLat/MaB33pZy0y3rJZtnqwR2qOOvbwKZYKiEO1O6VqNEBxKvJJelCq0dTXWT5pbO2gDXC6h6QDXCaHo6pOHGPUy+YBaGQRGuSusMEASYiWunYN0vCAI8QaXnWMXNMdFP3jHAJH0eDsoiGnLPBlBp4TNm6rYI74nMzgz3B9IikW4WVK+dc8KZJZWYjAuORU3jc1c/NPskD2ASinf8v3xnfXeukU0sJ5N6m5E8VLjObPEO+mN2t/FZTMZLiFqPWc/ALSqnMnnhwrNi2rbfg/rd/IpL8Le3pSBne8+seeFVBoGqzHM9yXw== github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ== gitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY= gitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9 ssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H vs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H Note The argocd-ssh-known-hosts-cm ConfigMap will be mounted as a volume at the mount path /app/config/ssh in the pods of argocd-server and argocd-repo-server . It will create a file ssh_known_hosts in that directory, which contains the SSH known hosts data used by ArgoCD for connecting to Git repositories via SSH. It might take a while for changes in the ConfigMap to be reflected in your pods, depending on your Kubernetes configuration. Clusters \u00b6 Cluster credentials are stored in secrets same as repository credentials but does not require entry in argocd-cm config map. Each secret must have label argocd.argoproj.io/secret-type: cluster . The secret data must include following fields: name - cluster name server - cluster api server url config - JSON representation of following data structure: # Basic authentication settings username : string password : string # Bearer authentication settings bearerToken : string # IAM authentication configuration awsAuthConfig : clusterName : string roleARN : string # Transport layer security configuration settings tlsClientConfig : # PEM-encoded bytes (typically read from a client certificate file). caData : string # PEM-encoded bytes (typically read from a client certificate file). certData : string # Server should be accessed without verifying the TLS certificate insecure : boolean # PEM-encoded bytes (typically read from a client certificate key file). keyData : string # ServerName is passed to the server for SNI and is used in the client to check server # ceritificates against. If ServerName is empty, the hostname used to contact the # server is used. serverName : string Cluster secret example: apiVersion : v1 kind : Secret metadata : name : mycluster-secret labels : argocd.argoproj.io/secret-type : cluster type : Opaque stringData : name : mycluster.com server : https://mycluster.com config : | { \"bearerToken\": \"<authentication token>\", \"tlsClientConfig\": { \"insecure\": false, \"caData\": \"<base64 encoded certificate>\" } } Helm Chart Repositories \u00b6 Non standard Helm Chart repositories have to be registered under the repositories key in the argocd-cm ConfigMap. Each repository must have url , type and name fields. For private Helm repos you may need to configure access credentials and HTTPS settings using usernameSecret , passwordSecret , caSecret , certSecret and keySecret fields. Example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : # v1.2 or earlier use `helm.repositories` helm.repositories : | - url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts name: istio.io # v1.3 or later use `repositories` with `type: helm` repositories : | - type: helm url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts name: istio.io - type: helm url: https://argoproj.github.io/argo-helm name: argo usernameSecret: name: my-secret key: username passwordSecret: name: my-secret key: password caSecret: name: my-secret key: ca certSecret: name: my-secret key: cert keySecret: name: my-secret key: key Resource Exclusion \u00b6 Resources can be excluded from discovery and sync so that ArgoCD is unaware of them. For example, events.k8s.io and metrics.k8s.io are always excluded. Use cases: You have temporal issues and you want to exclude problematic resources. There are many of a kind of resources that impacts ArgoCD's performance. Restrict ArgoCD's access to certain kinds of resources, e.g. secrets. See security.md#cluster-rbac . To configure this, edit the argcd-cm config map: kubectl edit configmap argocd-cm -n argocd Add resource.exclusions , e.g.: apiVersion : v1 data : resource.exclusions : | - apiGroups: - \"*\" kinds: - \"*\" clusters: - https://192.168.0.20 kind : ConfigMap The resource.exclusions node is a list of objects. Each object can have: apiGroups A list of globs to match the API group. kinds A list of kinds to match. Can be \"*\" to match all. cluster A list of globs to match the cluster. If all three match, then the resource is ignored. Notes: Quote globs in your YAML to avoid parsing errors. Invalid globs result in the whole rule being ignored. If you add a rule that matches existing resources, these will appear in the interface as OutOfSync . SSO & RBAC \u00b6 SSO configuration details: SSO RBAC configuration details: RBAC Manage Argo CD Using Argo CD \u00b6 Argo CD is able to manage itself since all settings are represented by Kubernetes manifests. The suggested way is to create Kustomize based application which uses base Argo CD manifests from https://github.com/argoproj/argo-cd and apply required changes on top. Example of kustomization.yaml : bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v1.0.1 # additional resources like ingress rules, cluster and repository secrets. resources : - clusters-secrets.yaml - repos-secrets.yaml # changes to config maps patchesStrategicMerge : - overlays/argo-cd-cm.yaml The live example of self managed Argo CD config is available at https://cd.apps.argoproj.io and with configuration stored at argoproj/argoproj-deployments . Note You will need to sign-in using your github account to get access to https://cd.apps.argoproj.io","title":"Declarative Setup"},{"location":"operator-manual/declarative-setup/#declarative-setup","text":"Argo CD applications, projects and settings can be defined declaratively using Kubernetes manifests.","title":"Declarative Setup"},{"location":"operator-manual/declarative-setup/#quick-reference","text":"Name Kind Description argocd-cm.yaml ConfigMap General Argo CD configuration argocd-secret.yaml Secret Password, Certificates, Signing Key argocd-rbac-cm.yaml ConfigMap RBAC Configuration argocd-tls-certs-cm.yaml ConfigMap Custom TLS certificates for connecting Git repositories via HTTPS (v1.2 and later) argocd-ssh-known-hosts-cm.yaml ConfigMap SSH known hosts data for connecting Git repositories via SSH (v1.2 and later) application.yaml Application Example application spec project.yaml AppProject Example project spec","title":"Quick Reference"},{"location":"operator-manual/declarative-setup/#applications","text":"The Application CRD is the Kubernetes resource object representing a deployed application instance in an environment. It is defined by two key pieces of information: source reference to the desired state in Git (repository, revision, path, environment) destination reference to the target cluster and namespace. A minimal Application spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook namespace : argocd spec : project : default source : repoURL : https://github.com/argoproj/argocd-example-apps.git targetRevision : HEAD path : guestbook destination : server : https://kubernetes.default.svc namespace : guestbook See application.yaml for additional fields Note The namespace must match the namespace of your Argo cd, typically this is argocd . Warning By default, deleting an application will not perform a cascade delete, thereby deleting its resources. You must add the finalizer if you want this behaviour - which you may well not want. metadata : finalizers : - resources-finalizer.argocd.argoproj.io","title":"Applications"},{"location":"operator-manual/declarative-setup/#app-of-apps","text":"You can create an app that creates other apps, which in turn can create other apps. This allows you to declaratively manage a group of app that can be deployed and configured in concert. See cluster bootstrapping .","title":"App of Apps"},{"location":"operator-manual/declarative-setup/#projects","text":"The AppProject CRD is the Kubernetes resource object representing a logical grouping of applications. It is defined by the following key pieces of information: sourceRepos reference to the repositories that applications within the project can pull manifests from. destinations reference to clusters and namespaces that applications within the project can deploy into. roles list of entities with definitions of their access to resources within the project. An example spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : my-project namespace : argocd spec : description : Example Project # Allow manifests to deploy from any Git repos sourceRepos : - '*' # Only permit applications to deploy to the guestbook namespace in the same cluster destinations : - namespace : guestbook server : https://kubernetes.default.svc # Deny all cluster-scoped resources from being created, except for Namespace clusterResourceWhitelist : - group : '' kind : Namespace # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy namespaceResourceBlacklist : - group : '' kind : ResourceQuota - group : '' kind : LimitRange - group : '' kind : NetworkPolicy roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - my-oidc-group # A role which provides sync privileges to only the guestbook-dev application, e.g. to provide # sync privileges to a CI system - name : ci-role description : Sync privileges for guestbook-dev policies : - p, proj:my-project:ci-role, applications, sync, my-project/guestbook-dev, allow # NOTE: JWT tokens can only be generated by the API server and the token is not persisted # anywhere by Argo CD. It can be prematurely revoked by removing the entry from this list. jwtTokens : - iat : 1535390316","title":"Projects"},{"location":"operator-manual/declarative-setup/#repositories","text":"Repository credentials are stored in secret. Use following steps to configure a repo: Create secret which contains repository credentials. Consider using bitnami-labs/sealed-secrets to store encrypted secret definition as a Kubernetes manifest. Register repository in the argocd-cm config map. Each repository must have url field and, depending on whether you connect using HTTPS or SSH, usernameSecret and passwordSecret (for HTTPS) or sshPrivateKeySecret (for SSH). Example for HTTPS: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: https://github.com/argoproj/my-private-repository passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username Example for SSH: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: git@github.com:argoproj/my-private-repository sshPrivateKeySecret: name: my-secret key: sshPrivateKey Tip The Kubernetes documentation has instructions for creating a secret containing a private key .","title":"Repositories"},{"location":"operator-manual/declarative-setup/#repository-credentials","text":"Earlier than v1.4 If you want to use the same credentials for multiple repositories, you can use repository.credentials : apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: https://github.com/argoproj/private-repo - url: https://github.com/argoproj/other-private-repo repository.credentials : | - url: https://github.com/argoproj passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username Argo CD will only use the credentials if you omit usernameSecret , passwordSecret , and sshPrivateKeySecret fields ( insecureIgnoreHostKey is ignored). A credential may be match if it's URL is the prefix of the repository's URL. The means that credentials may match, e.g in the above example both https://github.com/argoproj and https://github.com would match. Argo CD selects the first one that matches. Tip Order your credentials with the most specific at the top and the least specific at the bottom. A complete example. apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | # this has it's own credentials - url: https://github.com/argoproj/private-repo passwordSecret: name: private-repo-secret key: password usernameSecret: name: private-repo-secret key: username sshPrivateKeySecret: name: private-repo-secret key: sshPrivateKey - url: https://github.com/argoproj/other-private-repo - url: https://github.com/otherproj/another-private-repo repository.credentials : | # this will be used for the second repo - url: https://github.com/argoproj passwordSecret: name: other-private-repo-secret key: password usernameSecret: name: other-private-repo-secret key: username sshPrivateKeySecret: name: other-private-repo-secret key: sshPrivateKey # this will be used for the third repo - url: https://github.com passwordSecret: name: another-private-repo-secret key: password usernameSecret: name: another-private-repo-secret key: username sshPrivateKeySecret: name: another-private-repo-secret key: sshPrivateKey v1.4 or later If you want to use the same credentials for multiple repositories, you can use repository.credentials to configure credential templates. Credential templates can carry the same credentials information as repositories. apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : repositories : | - url: https://github.com/argoproj/private-repo - url: https://github.com/argoproj/other-private-repo repository.credentials : | - url: https://github.com/argoproj passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username In the above example, every repository accessed via HTTPS whose URL is prefixed with https://github.com/argoproj would use a username stored in the key username and a password stored in the key password of the secret my-secret for connecting to Git. In order for ArgoCD to use a credential template for any given repository, the following conditions must be met: The repository must either not be configured at all, or if configured, must not contain any credential information (i.e. contain none of sshPrivateKeySecret , usernameSecret , passwordSecret ) The URL configured for a credential template (e.g. https://github.com/argoproj ) must match as prefix for the repository URL (e.g. https://github.com/argoproj/argocd-example-apps ). Note Matching credential template URL prefixes is done on a best match effort, so the longest (best) match will take precedence. The order of definition is not important, as opposed to pre v1.4 configuration. The following keys are valid to refer to credential secrets:","title":"Repository Credentials"},{"location":"operator-manual/declarative-setup/#ssh-repositories","text":"sshPrivateKeySecret refers to a secret where an SSH private key is stored for accessing the repositories","title":"SSH repositories"},{"location":"operator-manual/declarative-setup/#https-repositories","text":"usernameSecret and passwordSecret refer to secrets where username and/or password are stored for accessing the repositories tlsClientCertData and tlsClientCertKey refer to secrets where a TLS client certificate ( tlsClientCertData ) and the corresponding private key tlsClientCertKey are stored for accessing the repositories","title":"HTTPS repositories"},{"location":"operator-manual/declarative-setup/#repositories-using-self-signed-tls-certificates-or-are-signed-by-custom-ca","text":"v1.2 or later You can manage the TLS certificates used to verify the authenticity of your repository servers in a ConfigMap object named argocd-tls-certs-cm . The data section should contain a map, with the repository server's hostname part (not the complete URL) as key, and the certificate(s) in PEM format as data. So, if you connect to a repository with the URL https://server.example.com/repos/my-repo , you should use server.example.com as key. The certificate data should be either the server's certificate (in case of self-signed certificate) or the certificate of the CA that was used to sign the server's certificate. You can configure multiple certificates for each server, e.g. if you are having a certificate roll-over planned. If there are no dedicated certificates configured for a repository server, the system's default trust store is used for validating the server's repository. This should be good enough for most (if not all) public Git repository services such as GitLab, GitHub and Bitbucket as well as most privately hosted sites which use certificates from well-known CAs, including Let's Encrypt certificates. An example ConfigMap object: apiVersion : v1 kind : ConfigMap metadata : name : argocd-tls-certs-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : server.example.com : | -----BEGIN CERTIFICATE----- MIIF1zCCA7+gAwIBAgIUQdTcSHY2Sxd3Tq/v1eIEZPCNbOowDQYJKoZIhvcNAQEL BQAwezELMAkGA1UEBhMCREUxFTATBgNVBAgMDExvd2VyIFNheG9ueTEQMA4GA1UE BwwHSGFub3ZlcjEVMBMGA1UECgwMVGVzdGluZyBDb3JwMRIwEAYDVQQLDAlUZXN0 c3VpdGUxGDAWBgNVBAMMD2Jhci5leGFtcGxlLmNvbTAeFw0xOTA3MDgxMzU2MTda Fw0yMDA3MDcxMzU2MTdaMHsxCzAJBgNVBAYTAkRFMRUwEwYDVQQIDAxMb3dlciBT YXhvbnkxEDAOBgNVBAcMB0hhbm92ZXIxFTATBgNVBAoMDFRlc3RpbmcgQ29ycDES MBAGA1UECwwJVGVzdHN1aXRlMRgwFgYDVQQDDA9iYXIuZXhhbXBsZS5jb20wggIi MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQCv4mHMdVUcafmaSHVpUM0zZWp5 NFXfboxA4inuOkE8kZlbGSe7wiG9WqLirdr39Ts+WSAFA6oANvbzlu3JrEQ2CHPc CNQm6diPREFwcDPFCe/eMawbwkQAPVSHPts0UoRxnpZox5pn69ghncBR+jtvx+/u P6HdwW0qqTvfJnfAF1hBJ4oIk2AXiip5kkIznsAh9W6WRy6nTVCeetmIepDOGe0G ZJIRn/OfSz7NzKylfDCat2z3EAutyeT/5oXZoWOmGg/8T7pn/pR588GoYYKRQnp+ YilqCPFX+az09EqqK/iHXnkdZ/Z2fCuU+9M/Zhrnlwlygl3RuVBI6xhm/ZsXtL2E Gxa61lNy6pyx5+hSxHEFEJshXLtioRd702VdLKxEOuYSXKeJDs1x9o6cJ75S6hko Ml1L4zCU+xEsMcvb1iQ2n7PZdacqhkFRUVVVmJ56th8aYyX7KNX6M9CD+kMpNm6J kKC1li/Iy+RI138bAvaFplajMF551kt44dSvIoJIbTr1LigudzWPqk31QaZXV/4u kD1n4p/XMc9HYU/was/CmQBFqmIZedTLTtK7clkuFN6wbwzdo1wmUNgnySQuMacO gxhHxxzRWxd24uLyk9Px+9U3BfVPaRLiOPaPoC58lyVOykjSgfpgbus7JS69fCq7 bEH4Jatp/10zkco+UQIDAQABo1MwUTAdBgNVHQ4EFgQUjXH6PHi92y4C4hQpey86 r6+x1ewwHwYDVR0jBBgwFoAUjXH6PHi92y4C4hQpey86r6+x1ewwDwYDVR0TAQH/ BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAgEAFE4SdKsX9UsLy+Z0xuHSxhTd0jfn Iih5mtzb8CDNO5oTw4z0aMeAvpsUvjJ/XjgxnkiRACXh7K9hsG2r+ageRWGevyvx CaRXFbherV1kTnZw4Y9/pgZTYVWs9jlqFOppz5sStkfjsDQ5lmPJGDii/StENAz2 XmtiPOgfG9Upb0GAJBCuKnrU9bIcT4L20gd2F4Y14ccyjlf8UiUi192IX6yM9OjT +TuXwZgqnTOq6piVgr+FTSa24qSvaXb5z/mJDLlk23npecTouLg83TNSn3R6fYQr d/Y9eXuUJ8U7/qTh2Ulz071AO9KzPOmleYPTx4Xty4xAtWi1QE5NHW9/Ajlv5OtO OnMNWIs7ssDJBsB7VFC8hcwf79jz7kC0xmQqDfw51Xhhk04kla+v+HZcFW2AO9so 6ZdVHHQnIbJa7yQJKZ+hK49IOoBR6JgdB5kymoplLLiuqZSYTcwSBZ72FYTm3iAr jzvt1hxpxVDmXvRnkhRrIRhK4QgJL0jRmirBjDY+PYYd7bdRIjN7WNZLFsgplnS8 9w6CwG32pRlm0c8kkiQ7FXA6BYCqOsDI8f1VGQv331OpR2Ck+FTv+L7DAmg6l37W +LB9LGh4OAp68ImTjqf6ioGKG0RBSznwME+r4nXtT1S/qLR6ASWUS4ViWRhbRlNK XWyb96wrUlv+E8I= -----END CERTIFICATE----- Note The argocd-tls-certs-cm ConfigMap will be mounted as a volume at the mount path /app/config/tls in the pods of argocd-server and argocd-repo-server . It will create files for each data key in the mount path directory, so above example would leave the file /app/config/tls/server.example.com , which contains the certificate data. It might take a while for changes in the ConfigMap to be reflected in your pods, depending on your Kubernetes configuration.","title":"Repositories using self-signed TLS certificates (or are signed by custom CA)"},{"location":"operator-manual/declarative-setup/#ssh-known-host-public-keys","text":"If you are connecting repositories via SSH, ArgoCD will need to know the SSH known hosts public key of the repository servers. You can manage the SSH known hosts data in the ConfigMap named argocd-ssh-known-hosts-cm . This ConfigMap contains a single key/value pair, with ssh_known_hosts as the key and the actual public keys of the SSH servers as data. As opposed to TLS configuration, the public key(s) of each single repository server ArgoCD will connect via SSH must be configured, otherwise the connections to the repository will fail. There is no fallback. The data can be copied from any existing ssh_known_hosts file, or from the output of the ssh-keyscan utility. The basic format is <servername> <keydata> , one entry per line. An example ConfigMap object: apiVersion : v1 kind : ConfigMap metadata : name : argocd-ssh-known-hosts-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : ssh_known_hosts : | bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAubiN81eDcafrgMeLzaFPsw2kNvEcqTKl/VqLat/MaB33pZy0y3rJZtnqwR2qOOvbwKZYKiEO1O6VqNEBxKvJJelCq0dTXWT5pbO2gDXC6h6QDXCaHo6pOHGPUy+YBaGQRGuSusMEASYiWunYN0vCAI8QaXnWMXNMdFP3jHAJH0eDsoiGnLPBlBp4TNm6rYI74nMzgz3B9IikW4WVK+dc8KZJZWYjAuORU3jc1c/NPskD2ASinf8v3xnfXeukU0sJ5N6m5E8VLjObPEO+mN2t/FZTMZLiFqPWc/ALSqnMnnhwrNi2rbfg/rd/IpL8Le3pSBne8+seeFVBoGqzHM9yXw== github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ== gitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY= gitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9 ssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H vs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H Note The argocd-ssh-known-hosts-cm ConfigMap will be mounted as a volume at the mount path /app/config/ssh in the pods of argocd-server and argocd-repo-server . It will create a file ssh_known_hosts in that directory, which contains the SSH known hosts data used by ArgoCD for connecting to Git repositories via SSH. It might take a while for changes in the ConfigMap to be reflected in your pods, depending on your Kubernetes configuration.","title":"SSH known host public keys"},{"location":"operator-manual/declarative-setup/#clusters","text":"Cluster credentials are stored in secrets same as repository credentials but does not require entry in argocd-cm config map. Each secret must have label argocd.argoproj.io/secret-type: cluster . The secret data must include following fields: name - cluster name server - cluster api server url config - JSON representation of following data structure: # Basic authentication settings username : string password : string # Bearer authentication settings bearerToken : string # IAM authentication configuration awsAuthConfig : clusterName : string roleARN : string # Transport layer security configuration settings tlsClientConfig : # PEM-encoded bytes (typically read from a client certificate file). caData : string # PEM-encoded bytes (typically read from a client certificate file). certData : string # Server should be accessed without verifying the TLS certificate insecure : boolean # PEM-encoded bytes (typically read from a client certificate key file). keyData : string # ServerName is passed to the server for SNI and is used in the client to check server # ceritificates against. If ServerName is empty, the hostname used to contact the # server is used. serverName : string Cluster secret example: apiVersion : v1 kind : Secret metadata : name : mycluster-secret labels : argocd.argoproj.io/secret-type : cluster type : Opaque stringData : name : mycluster.com server : https://mycluster.com config : | { \"bearerToken\": \"<authentication token>\", \"tlsClientConfig\": { \"insecure\": false, \"caData\": \"<base64 encoded certificate>\" } }","title":"Clusters"},{"location":"operator-manual/declarative-setup/#helm-chart-repositories","text":"Non standard Helm Chart repositories have to be registered under the repositories key in the argocd-cm ConfigMap. Each repository must have url , type and name fields. For private Helm repos you may need to configure access credentials and HTTPS settings using usernameSecret , passwordSecret , caSecret , certSecret and keySecret fields. Example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : # v1.2 or earlier use `helm.repositories` helm.repositories : | - url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts name: istio.io # v1.3 or later use `repositories` with `type: helm` repositories : | - type: helm url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts name: istio.io - type: helm url: https://argoproj.github.io/argo-helm name: argo usernameSecret: name: my-secret key: username passwordSecret: name: my-secret key: password caSecret: name: my-secret key: ca certSecret: name: my-secret key: cert keySecret: name: my-secret key: key","title":"Helm Chart Repositories"},{"location":"operator-manual/declarative-setup/#resource-exclusion","text":"Resources can be excluded from discovery and sync so that ArgoCD is unaware of them. For example, events.k8s.io and metrics.k8s.io are always excluded. Use cases: You have temporal issues and you want to exclude problematic resources. There are many of a kind of resources that impacts ArgoCD's performance. Restrict ArgoCD's access to certain kinds of resources, e.g. secrets. See security.md#cluster-rbac . To configure this, edit the argcd-cm config map: kubectl edit configmap argocd-cm -n argocd Add resource.exclusions , e.g.: apiVersion : v1 data : resource.exclusions : | - apiGroups: - \"*\" kinds: - \"*\" clusters: - https://192.168.0.20 kind : ConfigMap The resource.exclusions node is a list of objects. Each object can have: apiGroups A list of globs to match the API group. kinds A list of kinds to match. Can be \"*\" to match all. cluster A list of globs to match the cluster. If all three match, then the resource is ignored. Notes: Quote globs in your YAML to avoid parsing errors. Invalid globs result in the whole rule being ignored. If you add a rule that matches existing resources, these will appear in the interface as OutOfSync .","title":"Resource Exclusion"},{"location":"operator-manual/declarative-setup/#sso-rbac","text":"SSO configuration details: SSO RBAC configuration details: RBAC","title":"SSO &amp; RBAC"},{"location":"operator-manual/declarative-setup/#manage-argo-cd-using-argo-cd","text":"Argo CD is able to manage itself since all settings are represented by Kubernetes manifests. The suggested way is to create Kustomize based application which uses base Argo CD manifests from https://github.com/argoproj/argo-cd and apply required changes on top. Example of kustomization.yaml : bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v1.0.1 # additional resources like ingress rules, cluster and repository secrets. resources : - clusters-secrets.yaml - repos-secrets.yaml # changes to config maps patchesStrategicMerge : - overlays/argo-cd-cm.yaml The live example of self managed Argo CD config is available at https://cd.apps.argoproj.io and with configuration stored at argoproj/argoproj-deployments . Note You will need to sign-in using your github account to get access to https://cd.apps.argoproj.io","title":"Manage Argo CD Using Argo CD"},{"location":"operator-manual/disaster_recovery/","text":"Disaster Recovery \u00b6 You can use argocd-util to import and export all Argo CD data. Make sure you have ~/.kube/config pointing to your Argo CD cluster. Figure out what version of Argo CD you're running: argocd version | grep server # ... export VERSION = v1.0.1 Export to a backup: docker run -v ~/.kube:/home/argocd/.kube --rm argoproj/argocd: $VERSION argocd-util export > backup.yaml Import from a backup: docker run -v ~/.kube:/home/argocd/.kube --rm argoproj/argocd: $VERSION argocd-util import - < backup.yaml Note If you are running Argo CD on a namespace different than default remember to pass the namespace parameter (-n ). 'argocd-util export' will not fail if you run it in the wrong namespace.","title":"Disaster Recovery"},{"location":"operator-manual/disaster_recovery/#disaster-recovery","text":"You can use argocd-util to import and export all Argo CD data. Make sure you have ~/.kube/config pointing to your Argo CD cluster. Figure out what version of Argo CD you're running: argocd version | grep server # ... export VERSION = v1.0.1 Export to a backup: docker run -v ~/.kube:/home/argocd/.kube --rm argoproj/argocd: $VERSION argocd-util export > backup.yaml Import from a backup: docker run -v ~/.kube:/home/argocd/.kube --rm argoproj/argocd: $VERSION argocd-util import - < backup.yaml Note If you are running Argo CD on a namespace different than default remember to pass the namespace parameter (-n ). 'argocd-util export' will not fail if you run it in the wrong namespace.","title":"Disaster Recovery"},{"location":"operator-manual/health/","text":"Resource Health \u00b6 Overview \u00b6 Argo CD provides built-in health assessment for several standard Kubernetes types, which is then surfaced to the overall Application health status as a whole. The following checks are made for specific types of kuberentes resources: Deployment, ReplicaSet, StatefulSet DaemonSet \u00b6 Observed generation is equal to desired generation. Number of updated replicas equals the number of desired replicas. Service \u00b6 If service type is of type LoadBalancer , the status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . Ingress \u00b6 The status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . PersistentVolumeClaim \u00b6 The status.phase is Bound Custom Health Checks \u00b6 Argo CD supports custom health checks written in Lua . This is useful if you: Are affected by known issues where your Ingress or StatefulSet resources are stuck in Progressing state because of bug in your resource controller. Have a custom resource for which Argo CD does not have a built-in health check. There are two ways to configure a custom health check. The next two sections describe those ways. Way 1. Define a Custom Health Check in argocd-cm ConfigMap \u00b6 Custom health checks can be defined in resource.customizations field of argocd-cm . Following example demonstrates a health check for cert-manager.io/Certificate . data : resource.customizations : | cert-manager.io/Certificate: health.lua: | hs = {} if obj.status ~= nil then if obj.status.conditions ~= nil then for i, condition in ipairs(obj.status.conditions) do if condition.type == \"Ready\" and condition.status == \"False\" then hs.status = \"Degraded\" hs.message = condition.message return hs end if condition.type == \"Ready\" and condition.status == \"True\" then hs.status = \"Healthy\" hs.message = condition.message return hs end end end end hs.status = \"Progressing\" hs.message = \"Waiting for certificate\" return hs The obj is a global variable which contains the resource. The script must return an object with status and optional message field. NOTE: as a security measure you don't have access to most of the standard Lua libraries. Way 2. Contribute a Custom Health Check \u00b6 A health check can be bundled into Argo CD. Custom health check scripts are located in the resource_customizations directory of https://github.com/argoproj/argo-cd . This must have the following directory structure: argo-cd |-- resource_customizations | |-- your.crd.group.io # CRD group | | |-- MyKind # Resource kind | | | |-- health.lua # Health check | | | |-- health_test.yaml # Test inputs and expected results | | | +-- testdata # Directory with test resource YAML definitions Each health check must have tests defined in health_test.yaml file. The health_test.yaml is a YAML file with the following structure: tests : - healthStatus : status : ExpectedStatus message : Expected message inputPath : testdata/test-resource-definition.yaml The PR#1139 is an example of Cert Manager CRDs custom health check.","title":"Resource Health"},{"location":"operator-manual/health/#resource-health","text":"","title":"Resource Health"},{"location":"operator-manual/health/#overview","text":"Argo CD provides built-in health assessment for several standard Kubernetes types, which is then surfaced to the overall Application health status as a whole. The following checks are made for specific types of kuberentes resources:","title":"Overview"},{"location":"operator-manual/health/#deployment-replicaset-statefulset-daemonset","text":"Observed generation is equal to desired generation. Number of updated replicas equals the number of desired replicas.","title":"Deployment, ReplicaSet, StatefulSet DaemonSet"},{"location":"operator-manual/health/#service","text":"If service type is of type LoadBalancer , the status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP .","title":"Service"},{"location":"operator-manual/health/#ingress","text":"The status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP .","title":"Ingress"},{"location":"operator-manual/health/#persistentvolumeclaim","text":"The status.phase is Bound","title":"PersistentVolumeClaim"},{"location":"operator-manual/health/#custom-health-checks","text":"Argo CD supports custom health checks written in Lua . This is useful if you: Are affected by known issues where your Ingress or StatefulSet resources are stuck in Progressing state because of bug in your resource controller. Have a custom resource for which Argo CD does not have a built-in health check. There are two ways to configure a custom health check. The next two sections describe those ways.","title":"Custom Health Checks"},{"location":"operator-manual/health/#way-1-define-a-custom-health-check-in-argocd-cm-configmap","text":"Custom health checks can be defined in resource.customizations field of argocd-cm . Following example demonstrates a health check for cert-manager.io/Certificate . data : resource.customizations : | cert-manager.io/Certificate: health.lua: | hs = {} if obj.status ~= nil then if obj.status.conditions ~= nil then for i, condition in ipairs(obj.status.conditions) do if condition.type == \"Ready\" and condition.status == \"False\" then hs.status = \"Degraded\" hs.message = condition.message return hs end if condition.type == \"Ready\" and condition.status == \"True\" then hs.status = \"Healthy\" hs.message = condition.message return hs end end end end hs.status = \"Progressing\" hs.message = \"Waiting for certificate\" return hs The obj is a global variable which contains the resource. The script must return an object with status and optional message field. NOTE: as a security measure you don't have access to most of the standard Lua libraries.","title":"Way 1. Define a Custom Health Check in argocd-cm ConfigMap"},{"location":"operator-manual/health/#way-2-contribute-a-custom-health-check","text":"A health check can be bundled into Argo CD. Custom health check scripts are located in the resource_customizations directory of https://github.com/argoproj/argo-cd . This must have the following directory structure: argo-cd |-- resource_customizations | |-- your.crd.group.io # CRD group | | |-- MyKind # Resource kind | | | |-- health.lua # Health check | | | |-- health_test.yaml # Test inputs and expected results | | | +-- testdata # Directory with test resource YAML definitions Each health check must have tests defined in health_test.yaml file. The health_test.yaml is a YAML file with the following structure: tests : - healthStatus : status : ExpectedStatus message : Expected message inputPath : testdata/test-resource-definition.yaml The PR#1139 is an example of Cert Manager CRDs custom health check.","title":"Way 2. Contribute a Custom Health Check"},{"location":"operator-manual/high_availability/","text":"High Availability \u00b6 Argo CD is largely stateless, all data is persisted as Kubernetes objects, which in turn is stored in Kubernetes' etcd. Redis is only used as a throw-away cache and can be lost. When lost, it will be rebuilt without loss of service. A set HA of manifests are provided for users who wish to run Argo CD in a highly available manner. This runs more containers, and run Redis in HA mode. Manifests \u29c9 Note The HA installation will require at least three different nodes due to pod anti-affinity roles in the specs. Scaling Up \u00b6 argocd-repo-server \u00b6 settings: The argocd-repo-server is responsible for cloning Git repository, keeping it up to date and generating manifests using the appropriate tool. argocd-repo-server fork/exec config management tool to generate manifests. The fork can fail due to lack of memory and limit on the number of OS threads. The --parallelismlimit flag controls how many manifests generations are running concurrently and allows avoiding OOM kills. one instance of argocd-repo-server executes only one operation on one Git repo concurrently. Increase the number of argocd-repo-server replica count if you have a lot of applications in the same repository. argocd-repo-server clones repository into /tmp ( of path specified in TMPDIR env variable ). Pod might run out of disk space if have too many repository or repositories has a lot of files. To avoid this problem mount persistent volume. argocd-repo-server git ls-remote to resolve ambiguous revision such as HEAD , branch or tag name. This operation is happening pretty frequently and might fail. To avoid failed syncs use ARGOCD_GIT_ATTEMPTS_COUNT environment variable to retry failed requests. argocd-repo-server Every 3m (by default) Argo CD checks for changes to the app manifests. Argo CD assumes by default that manifests only change when the repo changes, so it caches generated manifests (for 24h by default). With Kustomize remote bases, or Helm patch releases, the manifests can change even though the repo has not changed. By reducing the cache time, you can get the changes without waiting for 24h. Use --repo-cache-expiration duration , and we'd suggest in low volume environments you try '1h'. Bear in mind this will negate the benefit of caching if set too low. metrics: argocd_git_request_total - Number of git requests. The metric provides two tags: repo - Git repo URL; request_type - ls-remote or fetch . argocd-application-controller \u00b6 settings: The argocd-application-controller uses argocd-repo-server to get generated manifests and Kubernetes API server to get actual cluster state. controller uses two separate queues to process application reconciliation (milliseconds) and app syncing (seconds). Number of queue processors for each queue is controlled by --status-processors (20 by default) and --operation-processors (10 by default) flags. Increase number of processors if your Argo CD instance manages too many applications. For 1000 application we use 50 for --status-processors and 25 for --operation-processors The manifest generation typically takes the most time during reconciliation. The duration of manifest generation is limited to make sure controller refresh queue does not overflow. The app reconciliation fails with Context deadline exceeded error if manifest generating taking too much time. As workaround increase value of --repo-server-timeout-seconds and consider scaling up argocd-repo-server deployment. controller uses kubectl fork/exec to push changes into the cluster and to convert resource from preferred version into user specified version (e.g. Deployment apps/v1 into extensions/v1beta1 ). Same as config management tool kubectl fork/exec might cause pod OOM kill. Use --kubectl-parallelism-limit flag to limit number of allowed concurrent kubectl fork/execs. controller uses Kubernetes watch APIs to maintain lightweight Kubernetes cluster cache. This allows to avoid querying Kubernetes during app reconciliation and significantly improve performance. For performance reasons controller monitors and caches only preferred the version of a resource. During reconciliation, the controller might have to convert cached resource from preferred version into a version of the resource stored in Git. If kubectl convert fails because conversion is not supported than controller fallback to Kubernetes API query which slows down reconciliation. In this case advice user-preferred resource version in Git. The controller polls Git every 3m by default. You can increase this duration using --app-resync seconds to reduce polling. metrics argocd_app_reconcile - reports application reconciliation duration. Can be used to build reconciliation duration heat map to get high-level reconciliation performance picture. argocd_app_k8s_request_total - number of k8s requests per application. The number of fallback Kubernetes API queries - useful to identify which application has a resource with non-preferred version and causes performance issues. argocd-server \u00b6 The argocd-server is stateless and probably least likely to cause issues. You might consider increasing number of replicas to 3 or more to ensure there is no downtime during upgrades. argocd-dex-server, argocd-redis \u00b6 The argocd-dex-server uses an in-memory database, and two or more instances would have inconsistent data. argocd-redis is pre-configured with the understanding of only three total redis servers/sentinels.","title":"High Availability"},{"location":"operator-manual/high_availability/#high-availability","text":"Argo CD is largely stateless, all data is persisted as Kubernetes objects, which in turn is stored in Kubernetes' etcd. Redis is only used as a throw-away cache and can be lost. When lost, it will be rebuilt without loss of service. A set HA of manifests are provided for users who wish to run Argo CD in a highly available manner. This runs more containers, and run Redis in HA mode. Manifests \u29c9 Note The HA installation will require at least three different nodes due to pod anti-affinity roles in the specs.","title":"High Availability"},{"location":"operator-manual/high_availability/#scaling-up","text":"","title":"Scaling Up"},{"location":"operator-manual/high_availability/#argocd-repo-server","text":"settings: The argocd-repo-server is responsible for cloning Git repository, keeping it up to date and generating manifests using the appropriate tool. argocd-repo-server fork/exec config management tool to generate manifests. The fork can fail due to lack of memory and limit on the number of OS threads. The --parallelismlimit flag controls how many manifests generations are running concurrently and allows avoiding OOM kills. one instance of argocd-repo-server executes only one operation on one Git repo concurrently. Increase the number of argocd-repo-server replica count if you have a lot of applications in the same repository. argocd-repo-server clones repository into /tmp ( of path specified in TMPDIR env variable ). Pod might run out of disk space if have too many repository or repositories has a lot of files. To avoid this problem mount persistent volume. argocd-repo-server git ls-remote to resolve ambiguous revision such as HEAD , branch or tag name. This operation is happening pretty frequently and might fail. To avoid failed syncs use ARGOCD_GIT_ATTEMPTS_COUNT environment variable to retry failed requests. argocd-repo-server Every 3m (by default) Argo CD checks for changes to the app manifests. Argo CD assumes by default that manifests only change when the repo changes, so it caches generated manifests (for 24h by default). With Kustomize remote bases, or Helm patch releases, the manifests can change even though the repo has not changed. By reducing the cache time, you can get the changes without waiting for 24h. Use --repo-cache-expiration duration , and we'd suggest in low volume environments you try '1h'. Bear in mind this will negate the benefit of caching if set too low. metrics: argocd_git_request_total - Number of git requests. The metric provides two tags: repo - Git repo URL; request_type - ls-remote or fetch .","title":"argocd-repo-server"},{"location":"operator-manual/high_availability/#argocd-application-controller","text":"settings: The argocd-application-controller uses argocd-repo-server to get generated manifests and Kubernetes API server to get actual cluster state. controller uses two separate queues to process application reconciliation (milliseconds) and app syncing (seconds). Number of queue processors for each queue is controlled by --status-processors (20 by default) and --operation-processors (10 by default) flags. Increase number of processors if your Argo CD instance manages too many applications. For 1000 application we use 50 for --status-processors and 25 for --operation-processors The manifest generation typically takes the most time during reconciliation. The duration of manifest generation is limited to make sure controller refresh queue does not overflow. The app reconciliation fails with Context deadline exceeded error if manifest generating taking too much time. As workaround increase value of --repo-server-timeout-seconds and consider scaling up argocd-repo-server deployment. controller uses kubectl fork/exec to push changes into the cluster and to convert resource from preferred version into user specified version (e.g. Deployment apps/v1 into extensions/v1beta1 ). Same as config management tool kubectl fork/exec might cause pod OOM kill. Use --kubectl-parallelism-limit flag to limit number of allowed concurrent kubectl fork/execs. controller uses Kubernetes watch APIs to maintain lightweight Kubernetes cluster cache. This allows to avoid querying Kubernetes during app reconciliation and significantly improve performance. For performance reasons controller monitors and caches only preferred the version of a resource. During reconciliation, the controller might have to convert cached resource from preferred version into a version of the resource stored in Git. If kubectl convert fails because conversion is not supported than controller fallback to Kubernetes API query which slows down reconciliation. In this case advice user-preferred resource version in Git. The controller polls Git every 3m by default. You can increase this duration using --app-resync seconds to reduce polling. metrics argocd_app_reconcile - reports application reconciliation duration. Can be used to build reconciliation duration heat map to get high-level reconciliation performance picture. argocd_app_k8s_request_total - number of k8s requests per application. The number of fallback Kubernetes API queries - useful to identify which application has a resource with non-preferred version and causes performance issues.","title":"argocd-application-controller"},{"location":"operator-manual/high_availability/#argocd-server","text":"The argocd-server is stateless and probably least likely to cause issues. You might consider increasing number of replicas to 3 or more to ensure there is no downtime during upgrades.","title":"argocd-server"},{"location":"operator-manual/high_availability/#argocd-dex-server-argocd-redis","text":"The argocd-dex-server uses an in-memory database, and two or more instances would have inconsistent data. argocd-redis is pre-configured with the understanding of only three total redis servers/sentinels.","title":"argocd-dex-server, argocd-redis"},{"location":"operator-manual/ingress/","text":"Ingress Configuration \u00b6 Argo CD runs both a gRPC server (used by the CLI), as well as a HTTP/HTTPS server (used by the UI). Both protocols are exposed by the argocd-server service object on the following ports: 443 - gRPC/HTTPS 80 - HTTP (redirects to HTTPS) There are several ways how Ingress can be configured. kubernetes/ingress-nginx \u00b6 Option 1: SSL-Passthrough \u00b6 Argo CD serves multiple protocols (gRPC/HTTPS) on the same port (443), this provides a challenge when attempting to define a single nginx ingress object and rule for the argocd-service, since the nginx.ingress.kubernetes.io/backend-protocol annotation accepts only a single value for the backend protocol (e.g. HTTP, HTTPS, GRPC, GRPCS). In order to expose the Argo CD API server with a single ingress rule and hostname, the nginx.ingress.kubernetes.io/ssl-passthrough annotation must be used to passthrough TLS connections and terminate TLS at the Argo CD API server. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-ingress annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/ssl-passthrough : \"true\" spec : rules : - host : argocd.example.com http : paths : - backend : serviceName : argocd-server servicePort : https The above rule terminates TLS at the Argo CD API server, which detects the protocol being used, and responds appropriately. Note that the nginx.ingress.kubernetes.io/ssl-passthrough annotation requires that the --enable-ssl-passthrough flag be added to the command line arguments to nginx-ingress-controller . SSL-Passthrough with cert-manager and Let's Encrypt \u00b6 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-ingress namespace : argocd annotations : cert-manager.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" nginx.ingress.kubernetes.io/ssl-passthrough : \"true\" # If you encounter a redirect loop or are getting a 307 response code # then you need to force the nginx ingress to connect to the backend using HTTPS. # # nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec : rules : - host : argocd.example.com http : paths : - backend : serviceName : argocd-server servicePort : https path : / tls : - hosts : - argocd.example.com secretName : argocd-secret # do not change, this is provided by Argo CD Option 2: Multiple Ingress Objects And Hosts \u00b6 Since ingress-nginx Ingress supports only a single protocol per Ingress object, an alternative way would be to define two Ingress objects. One for HTTP/HTTPS, and the other for gRPC: HTTP/HTTPS Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-http-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTP\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : http host : argocd.example.com tls : - hosts : - argocd.example.com secretName : argocd-secret # do not change, this is provided by Argo CD gRPC Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-grpc-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/backend-protocol : \"GRPC\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : https host : grpc.argocd.example.com tls : - hosts : - grpc.argocd.example.com secretName : argocd-secret # do not change, this is provided by Argo CD The API server should then be run with TLS disabled. Edit the argocd-server deployment to add the --insecure flag to the argocd-server command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --insecure The obvious disadvantage to this approach is that this technique requires two separate hostnames for the API server -- one for gRPC and the other for HTTP/HTTPS. However it allows TLS termination to happen at the ingress controller. Traefik (v2.0) \u00b6 Traefik can be used as an edge router and provide TLS termination within the same deployment. It currently has an advantage over NGINX in that it can terminate both TCP and HTTP connections on the same port meaning you do not require multiple ingress objects and hosts. apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : argocd-server-ingress spec : entryPoints : - websecure routes : - match : Host(`argocd.example.com`) kind : Rule services : - name : argocd-server port : 80 tls : certResolver : default options : {} AWS Application Load Balancers (ALBs) And Classic ELB (HTTP Mode) \u00b6 ALBs and Classic ELBs don't fully support HTTP2/gRPC, which is used by the argocd CLI. Thus, when using an AWS load balancer, either Classic ELB in passthrough mode is needed, or NLBs. $ argocd login <host>:<port> --grpc-web Authenticating through multiple layers of authenticating reverse proxies \u00b6 ArgoCD endpoints may be protected by one or more reverse proxies layers, in that case, you can provide additional headers through the argocd CLI --header parameter to authenticate through those layers. $ argocd login <host>:<port> --header 'x-token1:foo' --header 'x-token2:bar' # can be repeated multiple times $ argocd login <host>:<port> --header 'x-token1:foo,x-token2:bar' # headers can also be comma separated UI Base Path \u00b6 If the Argo CD UI is available under a non-root path (e.g. /argo-cd instead of / ) then the UI path should be configured in the API server. To configure the UI path add the --basehref flag into the argocd-server deployment command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --basehref - /argo-cd NOTE: The flag --basehref only changes the UI base URL. The API server will keep using the / path so you need to add a URL rewrite rule to the proxy config. Example nginx.conf with URL rewrite: worker_processes 1 ; events { worker_connections 1024 ; } http { sendfile on ; server { listen 443 ; location /argo-cd { rewrite /argo-cd/(.*) /$1 break ; proxy_pass https : // localhost : 8080 ; proxy_redirect off ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Host $server_name ; } } }","title":"Ingress Configuration"},{"location":"operator-manual/ingress/#ingress-configuration","text":"Argo CD runs both a gRPC server (used by the CLI), as well as a HTTP/HTTPS server (used by the UI). Both protocols are exposed by the argocd-server service object on the following ports: 443 - gRPC/HTTPS 80 - HTTP (redirects to HTTPS) There are several ways how Ingress can be configured.","title":"Ingress Configuration"},{"location":"operator-manual/ingress/#kubernetesingress-nginx","text":"","title":"kubernetes/ingress-nginx"},{"location":"operator-manual/ingress/#option-1-ssl-passthrough","text":"Argo CD serves multiple protocols (gRPC/HTTPS) on the same port (443), this provides a challenge when attempting to define a single nginx ingress object and rule for the argocd-service, since the nginx.ingress.kubernetes.io/backend-protocol annotation accepts only a single value for the backend protocol (e.g. HTTP, HTTPS, GRPC, GRPCS). In order to expose the Argo CD API server with a single ingress rule and hostname, the nginx.ingress.kubernetes.io/ssl-passthrough annotation must be used to passthrough TLS connections and terminate TLS at the Argo CD API server. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-ingress annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/ssl-passthrough : \"true\" spec : rules : - host : argocd.example.com http : paths : - backend : serviceName : argocd-server servicePort : https The above rule terminates TLS at the Argo CD API server, which detects the protocol being used, and responds appropriately. Note that the nginx.ingress.kubernetes.io/ssl-passthrough annotation requires that the --enable-ssl-passthrough flag be added to the command line arguments to nginx-ingress-controller .","title":"Option 1: SSL-Passthrough"},{"location":"operator-manual/ingress/#ssl-passthrough-with-cert-manager-and-lets-encrypt","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-ingress namespace : argocd annotations : cert-manager.io/cluster-issuer : letsencrypt-prod kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" nginx.ingress.kubernetes.io/ssl-passthrough : \"true\" # If you encounter a redirect loop or are getting a 307 response code # then you need to force the nginx ingress to connect to the backend using HTTPS. # # nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec : rules : - host : argocd.example.com http : paths : - backend : serviceName : argocd-server servicePort : https path : / tls : - hosts : - argocd.example.com secretName : argocd-secret # do not change, this is provided by Argo CD","title":"SSL-Passthrough with cert-manager and Let's Encrypt"},{"location":"operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts","text":"Since ingress-nginx Ingress supports only a single protocol per Ingress object, an alternative way would be to define two Ingress objects. One for HTTP/HTTPS, and the other for gRPC: HTTP/HTTPS Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-http-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTP\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : http host : argocd.example.com tls : - hosts : - argocd.example.com secretName : argocd-secret # do not change, this is provided by Argo CD gRPC Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-grpc-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/backend-protocol : \"GRPC\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : https host : grpc.argocd.example.com tls : - hosts : - grpc.argocd.example.com secretName : argocd-secret # do not change, this is provided by Argo CD The API server should then be run with TLS disabled. Edit the argocd-server deployment to add the --insecure flag to the argocd-server command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --insecure The obvious disadvantage to this approach is that this technique requires two separate hostnames for the API server -- one for gRPC and the other for HTTP/HTTPS. However it allows TLS termination to happen at the ingress controller.","title":"Option 2: Multiple Ingress Objects And Hosts"},{"location":"operator-manual/ingress/#traefik-v20","text":"Traefik can be used as an edge router and provide TLS termination within the same deployment. It currently has an advantage over NGINX in that it can terminate both TCP and HTTP connections on the same port meaning you do not require multiple ingress objects and hosts. apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : argocd-server-ingress spec : entryPoints : - websecure routes : - match : Host(`argocd.example.com`) kind : Rule services : - name : argocd-server port : 80 tls : certResolver : default options : {}","title":"Traefik (v2.0)"},{"location":"operator-manual/ingress/#aws-application-load-balancers-albs-and-classic-elb-http-mode","text":"ALBs and Classic ELBs don't fully support HTTP2/gRPC, which is used by the argocd CLI. Thus, when using an AWS load balancer, either Classic ELB in passthrough mode is needed, or NLBs. $ argocd login <host>:<port> --grpc-web","title":"AWS Application Load Balancers (ALBs) And Classic ELB (HTTP Mode)"},{"location":"operator-manual/ingress/#authenticating-through-multiple-layers-of-authenticating-reverse-proxies","text":"ArgoCD endpoints may be protected by one or more reverse proxies layers, in that case, you can provide additional headers through the argocd CLI --header parameter to authenticate through those layers. $ argocd login <host>:<port> --header 'x-token1:foo' --header 'x-token2:bar' # can be repeated multiple times $ argocd login <host>:<port> --header 'x-token1:foo,x-token2:bar' # headers can also be comma separated","title":"Authenticating through multiple layers of authenticating reverse proxies"},{"location":"operator-manual/ingress/#ui-base-path","text":"If the Argo CD UI is available under a non-root path (e.g. /argo-cd instead of / ) then the UI path should be configured in the API server. To configure the UI path add the --basehref flag into the argocd-server deployment command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --basehref - /argo-cd NOTE: The flag --basehref only changes the UI base URL. The API server will keep using the / path so you need to add a URL rewrite rule to the proxy config. Example nginx.conf with URL rewrite: worker_processes 1 ; events { worker_connections 1024 ; } http { sendfile on ; server { listen 443 ; location /argo-cd { rewrite /argo-cd/(.*) /$1 break ; proxy_pass https : // localhost : 8080 ; proxy_redirect off ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Host $server_name ; } } }","title":"UI Base Path"},{"location":"operator-manual/metrics/","text":"Metrics \u00b6 Argo CD exposes two sets of Prometheus metrics Application Metrics \u00b6 Metrics about applications. Scraped at the argocd-metrics:8082/metrics endpoint. Gauge for application health status Gauge for application sync status Counter for application sync history API Server Metrics \u00b6 Metrics about API Server API request and response activity (request totals, response codes, etc...). Scraped at the argocd-server-metrics:8083/metrics endpoint. Prometheus Operator \u00b6 If using Prometheus Operator, the following ServiceMonitor example manifests can be used. Change metadata.labels.release to the name of label selected by your Prometheus. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-server-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-repo-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-repo-server endpoints : - port : metrics Dashboards \u00b6 You can find an example Grafana dashboard here or check demo instance dashboard .","title":"Metrics"},{"location":"operator-manual/metrics/#metrics","text":"Argo CD exposes two sets of Prometheus metrics","title":"Metrics"},{"location":"operator-manual/metrics/#application-metrics","text":"Metrics about applications. Scraped at the argocd-metrics:8082/metrics endpoint. Gauge for application health status Gauge for application sync status Counter for application sync history","title":"Application Metrics"},{"location":"operator-manual/metrics/#api-server-metrics","text":"Metrics about API Server API request and response activity (request totals, response codes, etc...). Scraped at the argocd-server-metrics:8083/metrics endpoint.","title":"API Server Metrics"},{"location":"operator-manual/metrics/#prometheus-operator","text":"If using Prometheus Operator, the following ServiceMonitor example manifests can be used. Change metadata.labels.release to the name of label selected by your Prometheus. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-server-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-repo-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-repo-server endpoints : - port : metrics","title":"Prometheus Operator"},{"location":"operator-manual/metrics/#dashboards","text":"You can find an example Grafana dashboard here or check demo instance dashboard .","title":"Dashboards"},{"location":"operator-manual/notifications/","text":"Notifications \u00b6 The notifications support is not bundled into the Argo CD itself. Instead of reinventing the wheel and implementing opinionated notifications system Argo CD leverages integrations with the third-party notification system. Following integrations are recommended: To monitor Argo CD performance or health state of managed applications use Prometheus Metrics in combination with Grafana , Alertmanager . To notify the end-users of Argo CD about events like application upgrades, user errors in application definition, etc use one of the following projects: ArgoCD Notifications - Argo CD specific notification system that continuously monitors Argo CD applications and aims to integrate with various notification services such as Slack, SMTP, Telegram, Discord, etc. Argo Kube Notifier - generic Kubernetes resource controller that allows monitoring any Kubernetes resource and sends a notification when the configured rule is met. Kube Watch - a Kubernetes watcher that could publishes notification to Slack/hipchat/mattermost/flock channels. It watches the cluster for resource changes and notifies them through webhooks.","title":"Notifications"},{"location":"operator-manual/notifications/#notifications","text":"The notifications support is not bundled into the Argo CD itself. Instead of reinventing the wheel and implementing opinionated notifications system Argo CD leverages integrations with the third-party notification system. Following integrations are recommended: To monitor Argo CD performance or health state of managed applications use Prometheus Metrics in combination with Grafana , Alertmanager . To notify the end-users of Argo CD about events like application upgrades, user errors in application definition, etc use one of the following projects: ArgoCD Notifications - Argo CD specific notification system that continuously monitors Argo CD applications and aims to integrate with various notification services such as Slack, SMTP, Telegram, Discord, etc. Argo Kube Notifier - generic Kubernetes resource controller that allows monitoring any Kubernetes resource and sends a notification when the configured rule is met. Kube Watch - a Kubernetes watcher that could publishes notification to Slack/hipchat/mattermost/flock channels. It watches the cluster for resource changes and notifies them through webhooks.","title":"Notifications"},{"location":"operator-manual/rbac/","text":"RBAC Configuration \u00b6 The RBAC feature enables restriction of access to Argo CD resources. Argo CD does not have its own user management system and has only one built-in user admin . The admin user is a superuser and it has unrestricted access to the system. RBAC requires SSO configuration . Once SSO is configured, additional RBAC roles can be defined, and SSO groups can man be mapped to roles. Basic Built-in Roles \u00b6 Argo CD has two pre-defined roles but RBAC configuration allows defining roles and groups (see below). role:readonly - read-only access to all resources role:admin - unrestricted access to all resources These default built-in role definitions can be seen in builtin-policy.csv RBAC Permission Structure \u00b6 Breaking down the permissions definition differs slightly between applications and every other resource type in Argo CD. All resources except applications permissions (see next bullet): p, <role/user/group>, <resource>, <action>, <object> Applications (which belong to an AppProject): p, <role/user/group>, <resource>, <action>, <appproject>/<object> RBAC Resources and Actions \u00b6 Resources: clusters , projects , applications , repositories , certificates Actions: get , create , update , delete , sync , override , action Tying It All Together \u00b6 Additional roles and groups can be configured in argocd-rbac-cm ConfigMap. The example below configures a custom role, named org-admin . The role is assigned to any user which belongs to your-github-org:your-team group. All other users get the default policy of role:readonly , which cannot modify Argo CD settings. ArgoCD ConfigMap argocd-rbac-cm Example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.default : role:readonly policy.csv : | p, role:org-admin, applications, *, */*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, your-github-org:your-team, role:org-admin Another policy.csv example might look as follows: p, role:staging-db-admins, applications, create, staging-db-admins/*, allow p, role:staging-db-admins, applications, delete, staging-db-admins/*, allow p, role:staging-db-admins, applications, get, staging-db-admins/*, allow p, role:staging-db-admins, applications, override, staging-db-admins/*, allow p, role:staging-db-admins, applications, sync, staging-db-admins/*, allow p, role:staging-db-admins, applications, update, staging-db-admins/*, allow p, role:staging-db-admins, projects, get, staging-db-admins, allow g, db-admins, role:staging-db-admins This example defines a role called staging-db-admins with seven permissions that allow that role to perform the actions ( create / delete / get / override / sync / update applications, and get appprojects) against * (all) objects in the staging-db-admins Argo CD AppProject. Anonymous Access \u00b6 The anonymous access to Argo CD can be enabled using users.anonymous.enabled field in argocd-cm (see argocd-cm.yaml ). The anonymous users get default role permissions specified by policy.default in argocd-rbac-cm.yaml . For read-only access you'll want policy.default: role:readonly as above","title":"RBAC Configuration"},{"location":"operator-manual/rbac/#rbac-configuration","text":"The RBAC feature enables restriction of access to Argo CD resources. Argo CD does not have its own user management system and has only one built-in user admin . The admin user is a superuser and it has unrestricted access to the system. RBAC requires SSO configuration . Once SSO is configured, additional RBAC roles can be defined, and SSO groups can man be mapped to roles.","title":"RBAC Configuration"},{"location":"operator-manual/rbac/#basic-built-in-roles","text":"Argo CD has two pre-defined roles but RBAC configuration allows defining roles and groups (see below). role:readonly - read-only access to all resources role:admin - unrestricted access to all resources These default built-in role definitions can be seen in builtin-policy.csv","title":"Basic Built-in Roles"},{"location":"operator-manual/rbac/#rbac-permission-structure","text":"Breaking down the permissions definition differs slightly between applications and every other resource type in Argo CD. All resources except applications permissions (see next bullet): p, <role/user/group>, <resource>, <action>, <object> Applications (which belong to an AppProject): p, <role/user/group>, <resource>, <action>, <appproject>/<object>","title":"RBAC Permission Structure"},{"location":"operator-manual/rbac/#rbac-resources-and-actions","text":"Resources: clusters , projects , applications , repositories , certificates Actions: get , create , update , delete , sync , override , action","title":"RBAC Resources and Actions"},{"location":"operator-manual/rbac/#tying-it-all-together","text":"Additional roles and groups can be configured in argocd-rbac-cm ConfigMap. The example below configures a custom role, named org-admin . The role is assigned to any user which belongs to your-github-org:your-team group. All other users get the default policy of role:readonly , which cannot modify Argo CD settings. ArgoCD ConfigMap argocd-rbac-cm Example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.default : role:readonly policy.csv : | p, role:org-admin, applications, *, */*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, your-github-org:your-team, role:org-admin Another policy.csv example might look as follows: p, role:staging-db-admins, applications, create, staging-db-admins/*, allow p, role:staging-db-admins, applications, delete, staging-db-admins/*, allow p, role:staging-db-admins, applications, get, staging-db-admins/*, allow p, role:staging-db-admins, applications, override, staging-db-admins/*, allow p, role:staging-db-admins, applications, sync, staging-db-admins/*, allow p, role:staging-db-admins, applications, update, staging-db-admins/*, allow p, role:staging-db-admins, projects, get, staging-db-admins, allow g, db-admins, role:staging-db-admins This example defines a role called staging-db-admins with seven permissions that allow that role to perform the actions ( create / delete / get / override / sync / update applications, and get appprojects) against * (all) objects in the staging-db-admins Argo CD AppProject.","title":"Tying It All Together"},{"location":"operator-manual/rbac/#anonymous-access","text":"The anonymous access to Argo CD can be enabled using users.anonymous.enabled field in argocd-cm (see argocd-cm.yaml ). The anonymous users get default role permissions specified by policy.default in argocd-rbac-cm.yaml . For read-only access you'll want policy.default: role:readonly as above","title":"Anonymous Access"},{"location":"operator-manual/secret-management/","text":"Secret Management \u00b6 Argo CD is un-opinionated about how secrets are managed. There's many ways to do it and there's no one-size-fits-all solution. Here's some ways people are doing GitOps secrets: Bitnami Sealed Secrets Godaddy Kubernetes External Secrets Hashicorp Vault Helm Secrets Kustomize secret generator plugins aws-secret-operator KSOPS For discussion, see #1364","title":"Secret Management"},{"location":"operator-manual/secret-management/#secret-management","text":"Argo CD is un-opinionated about how secrets are managed. There's many ways to do it and there's no one-size-fits-all solution. Here's some ways people are doing GitOps secrets: Bitnami Sealed Secrets Godaddy Kubernetes External Secrets Hashicorp Vault Helm Secrets Kustomize secret generator plugins aws-secret-operator KSOPS For discussion, see #1364","title":"Secret Management"},{"location":"operator-manual/security/","text":"Security \u00b6 Argo CD has undergone rigourous internal security reviews and penetration testing to satisfy PCI compliance requirements. The following are some security topics and implementation details of Argo CD. Authentication \u00b6 Authentication to Argo CD API server is performed exclusively using JSON Web Tokens (JWTs). Username/password bearer tokens are not used for authentication. The JWT is obtained/managed in one of the following ways: For the local admin user, a username/password is exchanged for a JWT using the /api/v1/session endpoint. This token is signed & issued by the Argo CD API server itself, and has no expiration. When the admin password is updated, all existing admin JWT tokens are immediately revoked. The password is stored as a bcrypt hash in the argocd-secret Secret. For Single Sign-On users, the user completes an OAuth2 login flow to the configured OIDC identity provider (either delegated through the bundled Dex provider, or directly to a self-managed OIDC provider). This JWT is signed & issued by the IDP, and expiration and revokation is handled by the provider. Dex tokens expire after 24 hours. Automation tokens are generated for a project using the /api/v1/projects/{project}/roles/{role}/token endpoint, and are signed & issued by Argo CD. These tokens are limited in scope and privilege, and can only be used to manage application resources in the project which it belongs to. Project JWTs have a configurable expiration and can be immediately revoked by deleting the JWT reference ID from the project role. Authorization \u00b6 Authorization is performed by iterating the list of group membership in a user's JWT groups claims, and comparing each group against the roles/rules in the RBAC policy. Any matched rule permits access to the API request. TLS \u00b6 All network communication is performed over TLS including service-to-service communication between the three components (argocd-server, argocd-repo-server, argocd-application-controller). The Argo CD API server can enforce the use of TLS 1.2 using the flag: --tlsminversion 1.2 . Sensitive Information \u00b6 Secrets \u00b6 Argo CD never returns sensitive data from its API, and redacts all sensitive data in API payloads and logs. This includes: cluster credentials Git credentials OAuth2 client secrets Kubernetes Secret values External Cluster Credentials \u00b6 To manage external clusters, Argo CD stores the credentials of the external cluster as a Kubernetes Secret in the argocd namespace. This secret contains the K8s API bearer token associated with the argocd-manager ServiceAccount created during argocd cluster add , along with connection options to that API server (TLS configuration/certs, aws-iam-authenticator RoleARN, etc...). The information is used to reconstruct a REST config and kubeconfig to the cluster used by Argo CD services. To rotate the bearer token used by Argo CD, the token can be deleted (e.g. using kubectl) which causes kuberentes to generate a new secret with a new bearer token. The new token can be re-inputted to Argo CD by re-running argocd cluster add . Run the following commands against the managed cluster: # run using a kubeconfig for the externally managed cluster kubectl delete secret argocd-manager-token-XXXXXX -n kube-system argocd cluster add CONTEXTNAME To revoke Argo CD's access to a managed cluster, delete the RBAC artifacts against the managed cluster, and remove the cluster entry from Argo CD: # run using a kubeconfig for the externally managed cluster kubectl delete sa argocd-manager -n kube-system kubectl delete clusterrole argocd-manager-role kubectl delete clusterrolebinding argocd-manager-role-binding argocd cluster rm https://your-kubernetes-cluster-addr NOTE: for AWS EKS clusters, aws-iam-authenticator is used to authenticate to the external cluster, which uses IAM roles in lieu of locally stored tokens, so token rotation is not needed, and revokation is handled through IAM. Cluster RBAC \u00b6 By default, Argo CD uses a clusteradmin level role in order to: watch & operate on cluster state deploy resources to the cluster Although Argo CD requires cluster-wide read privileges to resources in the managed cluster to function properly, it does not necessarily need full write privileges to the cluster. The ClusterRole used by argocd-server and argocd-application-controller can be modified such that write privileges are limited to only the namespaces and resources that you wish Argo CD to manage. To fine-tune privileges of externally managed clusters, edit the ClusterRole of the argocd-manager-role # run using a kubeconfig for the externally managed cluster kubectl edit clusterrole argocd-manager-role To fine-tune privileges which Argo CD has against its own cluster (i.e. https://kubernetes.default.svc ), edit the following cluster roles where Argo CD is running in: # run using a kubeconfig to the cluster Argo CD is running in kubectl edit clusterrole argocd-server kubectl edit clusterrole argocd-application-controller Tip If you want to deny ArgoCD access to a kind of resource then add it as an excluded resource . Auditing \u00b6 As a GitOps deployment tool, the Git commit history provides a natural audit log of what changes were made to application configuration, when they were made, and by whom. However, this audit log only applies to what happened in Git and does not necessarily correlate one-to-one with events that happen in a cluster. For example, User A could have made multiple commits to application manifests, but User B could have just only synced those changes to the cluster sometime later. To complement the Git revision history, Argo CD emits Kubernetes Events of application activity, indicating the responsible actor when applicable. For example: $ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 1m 1m 1 guestbook.157f7c5edd33aeac Application Normal ResourceCreated argocd-server admin created application 1m 1m 1 guestbook.157f7c5f0f747acf Application Normal ResourceUpdated argocd-application-controller Updated sync status: -> OutOfSync 1m 1m 1 guestbook.157f7c5f0fbebbff Application Normal ResourceUpdated argocd-application-controller Updated health status: -> Missing 1m 1m 1 guestbook.157f7c6069e14f4d Application Normal OperationStarted argocd-server admin initiated sync to HEAD ( 8a1cb4a02d3538e54907c827352f66f20c3d7b0d ) 1m 1m 1 guestbook.157f7c60a55a81a8 Application Normal OperationCompleted argocd-application-controller Sync operation to 8a1cb4a02d3538e54907c827352f66f20c3d7b0d succeeded 1m 1m 1 guestbook.157f7c60af1ccae2 Application Normal ResourceUpdated argocd-application-controller Updated sync status: OutOfSync -> Synced 1m 1m 1 guestbook.157f7c60af5bc4f0 Application Normal ResourceUpdated argocd-application-controller Updated health status: Missing -> Progressing 1m 1m 1 guestbook.157f7c651990e848 Application Normal ResourceUpdated argocd-application-controller Updated health status: Progressing -> Healthy These events can be then be persisted for longer periods of time using other tools as Event Exporter or Event Router . WebHook Payloads \u00b6 Payloads from webhook events are considered untrusted. Argo CD only examines the payload to infer the involved applications of the webhook event (e.g. which repo was modified), then refreshes the related application for reconciliation. This refresh is the same refresh which occurs regularly at three minute intervals, just fast-tracked by the webhook event. Reporting Vulnerabilities \u00b6 Please report security vulnerabilities by e-mailing: Jesse_Suen@intuit.com Alexander_Matyushentsev@intuit.com Edward_Lee@intuit.com","title":"Security"},{"location":"operator-manual/security/#security","text":"Argo CD has undergone rigourous internal security reviews and penetration testing to satisfy PCI compliance requirements. The following are some security topics and implementation details of Argo CD.","title":"Security"},{"location":"operator-manual/security/#authentication","text":"Authentication to Argo CD API server is performed exclusively using JSON Web Tokens (JWTs). Username/password bearer tokens are not used for authentication. The JWT is obtained/managed in one of the following ways: For the local admin user, a username/password is exchanged for a JWT using the /api/v1/session endpoint. This token is signed & issued by the Argo CD API server itself, and has no expiration. When the admin password is updated, all existing admin JWT tokens are immediately revoked. The password is stored as a bcrypt hash in the argocd-secret Secret. For Single Sign-On users, the user completes an OAuth2 login flow to the configured OIDC identity provider (either delegated through the bundled Dex provider, or directly to a self-managed OIDC provider). This JWT is signed & issued by the IDP, and expiration and revokation is handled by the provider. Dex tokens expire after 24 hours. Automation tokens are generated for a project using the /api/v1/projects/{project}/roles/{role}/token endpoint, and are signed & issued by Argo CD. These tokens are limited in scope and privilege, and can only be used to manage application resources in the project which it belongs to. Project JWTs have a configurable expiration and can be immediately revoked by deleting the JWT reference ID from the project role.","title":"Authentication"},{"location":"operator-manual/security/#authorization","text":"Authorization is performed by iterating the list of group membership in a user's JWT groups claims, and comparing each group against the roles/rules in the RBAC policy. Any matched rule permits access to the API request.","title":"Authorization"},{"location":"operator-manual/security/#tls","text":"All network communication is performed over TLS including service-to-service communication between the three components (argocd-server, argocd-repo-server, argocd-application-controller). The Argo CD API server can enforce the use of TLS 1.2 using the flag: --tlsminversion 1.2 .","title":"TLS"},{"location":"operator-manual/security/#sensitive-information","text":"","title":"Sensitive Information"},{"location":"operator-manual/security/#secrets","text":"Argo CD never returns sensitive data from its API, and redacts all sensitive data in API payloads and logs. This includes: cluster credentials Git credentials OAuth2 client secrets Kubernetes Secret values","title":"Secrets"},{"location":"operator-manual/security/#external-cluster-credentials","text":"To manage external clusters, Argo CD stores the credentials of the external cluster as a Kubernetes Secret in the argocd namespace. This secret contains the K8s API bearer token associated with the argocd-manager ServiceAccount created during argocd cluster add , along with connection options to that API server (TLS configuration/certs, aws-iam-authenticator RoleARN, etc...). The information is used to reconstruct a REST config and kubeconfig to the cluster used by Argo CD services. To rotate the bearer token used by Argo CD, the token can be deleted (e.g. using kubectl) which causes kuberentes to generate a new secret with a new bearer token. The new token can be re-inputted to Argo CD by re-running argocd cluster add . Run the following commands against the managed cluster: # run using a kubeconfig for the externally managed cluster kubectl delete secret argocd-manager-token-XXXXXX -n kube-system argocd cluster add CONTEXTNAME To revoke Argo CD's access to a managed cluster, delete the RBAC artifacts against the managed cluster, and remove the cluster entry from Argo CD: # run using a kubeconfig for the externally managed cluster kubectl delete sa argocd-manager -n kube-system kubectl delete clusterrole argocd-manager-role kubectl delete clusterrolebinding argocd-manager-role-binding argocd cluster rm https://your-kubernetes-cluster-addr NOTE: for AWS EKS clusters, aws-iam-authenticator is used to authenticate to the external cluster, which uses IAM roles in lieu of locally stored tokens, so token rotation is not needed, and revokation is handled through IAM.","title":"External Cluster Credentials"},{"location":"operator-manual/security/#cluster-rbac","text":"By default, Argo CD uses a clusteradmin level role in order to: watch & operate on cluster state deploy resources to the cluster Although Argo CD requires cluster-wide read privileges to resources in the managed cluster to function properly, it does not necessarily need full write privileges to the cluster. The ClusterRole used by argocd-server and argocd-application-controller can be modified such that write privileges are limited to only the namespaces and resources that you wish Argo CD to manage. To fine-tune privileges of externally managed clusters, edit the ClusterRole of the argocd-manager-role # run using a kubeconfig for the externally managed cluster kubectl edit clusterrole argocd-manager-role To fine-tune privileges which Argo CD has against its own cluster (i.e. https://kubernetes.default.svc ), edit the following cluster roles where Argo CD is running in: # run using a kubeconfig to the cluster Argo CD is running in kubectl edit clusterrole argocd-server kubectl edit clusterrole argocd-application-controller Tip If you want to deny ArgoCD access to a kind of resource then add it as an excluded resource .","title":"Cluster RBAC"},{"location":"operator-manual/security/#auditing","text":"As a GitOps deployment tool, the Git commit history provides a natural audit log of what changes were made to application configuration, when they were made, and by whom. However, this audit log only applies to what happened in Git and does not necessarily correlate one-to-one with events that happen in a cluster. For example, User A could have made multiple commits to application manifests, but User B could have just only synced those changes to the cluster sometime later. To complement the Git revision history, Argo CD emits Kubernetes Events of application activity, indicating the responsible actor when applicable. For example: $ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 1m 1m 1 guestbook.157f7c5edd33aeac Application Normal ResourceCreated argocd-server admin created application 1m 1m 1 guestbook.157f7c5f0f747acf Application Normal ResourceUpdated argocd-application-controller Updated sync status: -> OutOfSync 1m 1m 1 guestbook.157f7c5f0fbebbff Application Normal ResourceUpdated argocd-application-controller Updated health status: -> Missing 1m 1m 1 guestbook.157f7c6069e14f4d Application Normal OperationStarted argocd-server admin initiated sync to HEAD ( 8a1cb4a02d3538e54907c827352f66f20c3d7b0d ) 1m 1m 1 guestbook.157f7c60a55a81a8 Application Normal OperationCompleted argocd-application-controller Sync operation to 8a1cb4a02d3538e54907c827352f66f20c3d7b0d succeeded 1m 1m 1 guestbook.157f7c60af1ccae2 Application Normal ResourceUpdated argocd-application-controller Updated sync status: OutOfSync -> Synced 1m 1m 1 guestbook.157f7c60af5bc4f0 Application Normal ResourceUpdated argocd-application-controller Updated health status: Missing -> Progressing 1m 1m 1 guestbook.157f7c651990e848 Application Normal ResourceUpdated argocd-application-controller Updated health status: Progressing -> Healthy These events can be then be persisted for longer periods of time using other tools as Event Exporter or Event Router .","title":"Auditing"},{"location":"operator-manual/security/#webhook-payloads","text":"Payloads from webhook events are considered untrusted. Argo CD only examines the payload to infer the involved applications of the webhook event (e.g. which repo was modified), then refreshes the related application for reconciliation. This refresh is the same refresh which occurs regularly at three minute intervals, just fast-tracked by the webhook event.","title":"WebHook Payloads"},{"location":"operator-manual/security/#reporting-vulnerabilities","text":"Please report security vulnerabilities by e-mailing: Jesse_Suen@intuit.com Alexander_Matyushentsev@intuit.com Edward_Lee@intuit.com","title":"Reporting Vulnerabilities"},{"location":"operator-manual/webhook/","text":"Git Webhook Configuration \u00b6 Overview \u00b6 Argo CD polls Git repositories every three minutes to detect changes to the manifests. To eliminate this delay from polling, the API server can be configured to receive webhook events. Argo CD supports Git webhook notifications from GitHub, GitLab, Bitbucket, Bitbucket Server and Gogs. The following explains how to configure a Git webhook for GitHub, but the same process should be applicable to other providers. 1. Create The WebHook In The Git Provider \u00b6 In your Git provider, navigate to the settings page where webhooks can be configured. The payload URL configured in the Git provider should use the /api/webhook endpoint of your Argo CD instance (e.g. [https://argocd.example.com/api/webhook]). If you wish to use a shared secret, input an arbitrary value in the secret. This value will be used when configuring the webhook in the next step. 2. Configure Argo CD With The WebHook Secret Optional) \u00b6 Configuring a webhook shared secret is optional, since Argo CD will still refresh applications related to the Git repository, even with unauthenticated webhook events. This is safe to do since the contents of webhook payloads are considered untrusted, and will only result in a refresh of the application (a process which already occurs at three-minute intervals). If Argo CD is publicly accessible, then configuring a webhook secret is recommended to prevent a DDoS attack. In the argocd-secret kubernetes secret, configure one of the following keys with the Git provider's webhook secret configured in step 1. Provider K8s Secret Key GitHub webhook.github.secret GitLab webhook.gitlab.secret BitBucket webhook.bitbucket.uuid BitBucketServer webhook.bitbucketserver.secret Gogs webhook.gogs.secret Edit the Argo CD kubernetes secret: kubectl edit secret argocd-secret -n argocd TIP: for ease of entering secrets, kubernetes supports inputting secrets in the stringData field, which saves you the trouble of base64 encoding the values and copying it to the data field. Simply copy the shared webhook secret created in step 1, to the corresponding GitHub/GitLab/BitBucket key under the stringData field: apiVersion : v1 kind : Secret metadata : name : argocd-secret namespace : argocd type : Opaque data : ... stringData : # github webhook secret webhook.github.secret : shhhh! it's a github secret # gitlab webhook secret webhook.gitlab.secret : shhhh! it's a gitlab secret # bitbucket webhook secret webhook.bitbucket.uuid : your-bitbucket-uuid # bitbucket server webhook secret webhook.bitbucketserver.secret : shhhh! it's a bitbucket server secret # gogs server webhook secret webhook.gogs.secret : shhhh! it's a gogs server secret After saving, the changes should take effect automatically.","title":"Git Webhook Configuration"},{"location":"operator-manual/webhook/#git-webhook-configuration","text":"","title":"Git Webhook Configuration"},{"location":"operator-manual/webhook/#overview","text":"Argo CD polls Git repositories every three minutes to detect changes to the manifests. To eliminate this delay from polling, the API server can be configured to receive webhook events. Argo CD supports Git webhook notifications from GitHub, GitLab, Bitbucket, Bitbucket Server and Gogs. The following explains how to configure a Git webhook for GitHub, but the same process should be applicable to other providers.","title":"Overview"},{"location":"operator-manual/webhook/#1-create-the-webhook-in-the-git-provider","text":"In your Git provider, navigate to the settings page where webhooks can be configured. The payload URL configured in the Git provider should use the /api/webhook endpoint of your Argo CD instance (e.g. [https://argocd.example.com/api/webhook]). If you wish to use a shared secret, input an arbitrary value in the secret. This value will be used when configuring the webhook in the next step.","title":"1. Create The WebHook In The Git Provider"},{"location":"operator-manual/webhook/#2-configure-argo-cd-with-the-webhook-secret-optional","text":"Configuring a webhook shared secret is optional, since Argo CD will still refresh applications related to the Git repository, even with unauthenticated webhook events. This is safe to do since the contents of webhook payloads are considered untrusted, and will only result in a refresh of the application (a process which already occurs at three-minute intervals). If Argo CD is publicly accessible, then configuring a webhook secret is recommended to prevent a DDoS attack. In the argocd-secret kubernetes secret, configure one of the following keys with the Git provider's webhook secret configured in step 1. Provider K8s Secret Key GitHub webhook.github.secret GitLab webhook.gitlab.secret BitBucket webhook.bitbucket.uuid BitBucketServer webhook.bitbucketserver.secret Gogs webhook.gogs.secret Edit the Argo CD kubernetes secret: kubectl edit secret argocd-secret -n argocd TIP: for ease of entering secrets, kubernetes supports inputting secrets in the stringData field, which saves you the trouble of base64 encoding the values and copying it to the data field. Simply copy the shared webhook secret created in step 1, to the corresponding GitHub/GitLab/BitBucket key under the stringData field: apiVersion : v1 kind : Secret metadata : name : argocd-secret namespace : argocd type : Opaque data : ... stringData : # github webhook secret webhook.github.secret : shhhh! it's a github secret # gitlab webhook secret webhook.gitlab.secret : shhhh! it's a gitlab secret # bitbucket webhook secret webhook.bitbucket.uuid : your-bitbucket-uuid # bitbucket server webhook secret webhook.bitbucketserver.secret : shhhh! it's a bitbucket server secret # gogs server webhook secret webhook.gogs.secret : shhhh! it's a gogs server secret After saving, the changes should take effect automatically.","title":"2. Configure Argo CD With The WebHook Secret Optional)"},{"location":"operator-manual/sso/","text":"SSO Overview \u00b6 Argo CD does not have any local users other than the built-in admin user. All other users are expected to login via SSO. There are two ways that SSO can be configured: Bundled Dex OIDC provider - use this option if your current provider does not support OIDC (e.g. SAML, LDAP) or if you wish to leverage any of Dex's connector features (e.g. the ability to map GitHub organizations and teams to OIDC groups claims). Existing OIDC provider - use this if you already have an OIDC provider which you are using (e.g. Okta , OneLogin , Auth0 , Microsoft ), where you manage your users, groups, and memberships. Dex \u00b6 Argo CD embeds and bundles Dex as part of its installation, for the purpose of delegating authentication to an external identity provider. Multiple types of identity providers are supported (OIDC, SAML, LDAP, GitHub, etc...). SSO configuration of Argo CD requires editing the argocd-cm ConfigMap with Dex connector settings. This document describes how to configure Argo CD SSO using GitHub (OAuth2) as an example, but the steps should be similar for other identity providers. 1. Register the application in the identity provider \u00b6 In GitHub, register a new application. The callback address should be the /api/dex/callback endpoint of your Argo CD URL (e.g. https://argocd.example.com/api/dex/callback ). After registering the app, you will receive an OAuth2 client ID and secret. These values will be inputted into the Argo CD configmap. 2. Configure Argo CD for SSO \u00b6 Edit the argocd-cm configmap: kubectl edit configmap argocd-cm -n argocd In the url key, input the base URL of Argo CD. In this example, it is https://argocd.example.com In the dex.config key, add the github connector to the connectors sub field. See Dex's GitHub connector documentation for explanation of the fields. A minimal config should populate the clientID, clientSecret generated in Step 1. You will very likely want to restrict logins to one or more GitHub organization. In the connectors.config.orgs list, add one or more GitHub organizations. Any member of the org will then be able to login to Argo CD to perform management tasks. data : url : https://argocd.example.com dex.config : | connectors: # GitHub example - type: github id: github name: GitHub config: clientID: aabbccddeeff00112233 clientSecret: $dex.github.clientSecret orgs: - name: your-github-org # GitHub enterprise example - type: github id: acme-github name: Acme GitHub config: hostName: github.acme.com clientID: abcdefghijklmnopqrst clientSecret: $dex.acme.clientSecret orgs: - name: your-github-org After saving, the changes should take affect automatically. NOTES: Any values which start with '$' will look to a key in argocd-secret of the same name (minus the $), to obtain the actual value. This allows you to store the clientSecret as a kubernetes secret. There is no need to set redirectURI in the connectors.config as shown in the dex documentation. Argo CD will automatically use the correct redirectURI for any OAuth2 connectors, to match the correct external callback URL (e.g. https://argocd.example.com/api/dex/callback ) Existing OIDC Provider \u00b6 To configure Argo CD to delegate authenticate to your existing OIDC provider, add the OAuth2 configuration to the argocd-cm ConfigMap under the oidc.config key: data : url : https://argocd.example.com oidc.config : | name: Okta issuer: https://dev-123456.oktapreview.com clientID: aaaabbbbccccddddeee clientSecret: $oidc.okta.clientSecret # Optional set of OIDC scopes to request. If omitted, defaults to: [\"openid\", \"profile\", \"email\", \"groups\"] requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"] # Optional set of OIDC claims to request on the ID token. requestedIDTokenClaims: {\"groups\": {\"essential\": true}} # Some OIDC providers require a separate clientID for different callback URLs. # For example, if configuring Argo CD with self-hosted Dex, you will need a separate client ID # for the 'localhost' (CLI) client to Dex. This field is optional. If omitted, the CLI will # use the same clientID as the Argo CD server cliClientID: vvvvwwwwxxxxyyyyzzzz Note The callback address should be the /auth/callback endpoint of your Argo CD URL (e.g. https://argocd.example.com/auth/callback). Requesting additional ID token claims \u00b6 Not all OIDC providers support a special groups scope. E.g. Okta, OneLogin and Microsoft do support a special groups scope and will return group membership with the default requestedScopes . Other OIDC providers might be able to return a claim with group membership if explicitly requested to do so. Individual claims can be requested with requestedIDTokenClaims , see OpenID Connect Claims Parameter for details. The Argo CD configuration for claims is as follows: oidc.config : | requestedIDTokenClaims: email: essential: true groups: essential: true value: org:myorg acr: essential: true values: - urn:mace:incommon:iap:silver - urn:mace:incommon:iap:bronze For a simple case this can be: oidc.config : | requestedIDTokenClaims: {\"groups\": {\"essential\": true}}","title":"SSO Overview"},{"location":"operator-manual/sso/#sso-overview","text":"Argo CD does not have any local users other than the built-in admin user. All other users are expected to login via SSO. There are two ways that SSO can be configured: Bundled Dex OIDC provider - use this option if your current provider does not support OIDC (e.g. SAML, LDAP) or if you wish to leverage any of Dex's connector features (e.g. the ability to map GitHub organizations and teams to OIDC groups claims). Existing OIDC provider - use this if you already have an OIDC provider which you are using (e.g. Okta , OneLogin , Auth0 , Microsoft ), where you manage your users, groups, and memberships.","title":"SSO Overview"},{"location":"operator-manual/sso/#dex","text":"Argo CD embeds and bundles Dex as part of its installation, for the purpose of delegating authentication to an external identity provider. Multiple types of identity providers are supported (OIDC, SAML, LDAP, GitHub, etc...). SSO configuration of Argo CD requires editing the argocd-cm ConfigMap with Dex connector settings. This document describes how to configure Argo CD SSO using GitHub (OAuth2) as an example, but the steps should be similar for other identity providers.","title":"Dex"},{"location":"operator-manual/sso/#1-register-the-application-in-the-identity-provider","text":"In GitHub, register a new application. The callback address should be the /api/dex/callback endpoint of your Argo CD URL (e.g. https://argocd.example.com/api/dex/callback ). After registering the app, you will receive an OAuth2 client ID and secret. These values will be inputted into the Argo CD configmap.","title":"1. Register the application in the identity provider"},{"location":"operator-manual/sso/#2-configure-argo-cd-for-sso","text":"Edit the argocd-cm configmap: kubectl edit configmap argocd-cm -n argocd In the url key, input the base URL of Argo CD. In this example, it is https://argocd.example.com In the dex.config key, add the github connector to the connectors sub field. See Dex's GitHub connector documentation for explanation of the fields. A minimal config should populate the clientID, clientSecret generated in Step 1. You will very likely want to restrict logins to one or more GitHub organization. In the connectors.config.orgs list, add one or more GitHub organizations. Any member of the org will then be able to login to Argo CD to perform management tasks. data : url : https://argocd.example.com dex.config : | connectors: # GitHub example - type: github id: github name: GitHub config: clientID: aabbccddeeff00112233 clientSecret: $dex.github.clientSecret orgs: - name: your-github-org # GitHub enterprise example - type: github id: acme-github name: Acme GitHub config: hostName: github.acme.com clientID: abcdefghijklmnopqrst clientSecret: $dex.acme.clientSecret orgs: - name: your-github-org After saving, the changes should take affect automatically. NOTES: Any values which start with '$' will look to a key in argocd-secret of the same name (minus the $), to obtain the actual value. This allows you to store the clientSecret as a kubernetes secret. There is no need to set redirectURI in the connectors.config as shown in the dex documentation. Argo CD will automatically use the correct redirectURI for any OAuth2 connectors, to match the correct external callback URL (e.g. https://argocd.example.com/api/dex/callback )","title":"2. Configure Argo CD for SSO"},{"location":"operator-manual/sso/#existing-oidc-provider","text":"To configure Argo CD to delegate authenticate to your existing OIDC provider, add the OAuth2 configuration to the argocd-cm ConfigMap under the oidc.config key: data : url : https://argocd.example.com oidc.config : | name: Okta issuer: https://dev-123456.oktapreview.com clientID: aaaabbbbccccddddeee clientSecret: $oidc.okta.clientSecret # Optional set of OIDC scopes to request. If omitted, defaults to: [\"openid\", \"profile\", \"email\", \"groups\"] requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"] # Optional set of OIDC claims to request on the ID token. requestedIDTokenClaims: {\"groups\": {\"essential\": true}} # Some OIDC providers require a separate clientID for different callback URLs. # For example, if configuring Argo CD with self-hosted Dex, you will need a separate client ID # for the 'localhost' (CLI) client to Dex. This field is optional. If omitted, the CLI will # use the same clientID as the Argo CD server cliClientID: vvvvwwwwxxxxyyyyzzzz Note The callback address should be the /auth/callback endpoint of your Argo CD URL (e.g. https://argocd.example.com/auth/callback).","title":"Existing OIDC Provider"},{"location":"operator-manual/sso/#requesting-additional-id-token-claims","text":"Not all OIDC providers support a special groups scope. E.g. Okta, OneLogin and Microsoft do support a special groups scope and will return group membership with the default requestedScopes . Other OIDC providers might be able to return a claim with group membership if explicitly requested to do so. Individual claims can be requested with requestedIDTokenClaims , see OpenID Connect Claims Parameter for details. The Argo CD configuration for claims is as follows: oidc.config : | requestedIDTokenClaims: email: essential: true groups: essential: true value: org:myorg acr: essential: true values: - urn:mace:incommon:iap:silver - urn:mace:incommon:iap:bronze For a simple case this can be: oidc.config : | requestedIDTokenClaims: {\"groups\": {\"essential\": true}}","title":"Requesting additional ID token claims"},{"location":"operator-manual/sso/auth0/","text":"Auth0 \u00b6 Are you using this? Please contribute! If you're using this IdP please consider contributing to this document.","title":"Auth0"},{"location":"operator-manual/sso/auth0/#auth0","text":"Are you using this? Please contribute! If you're using this IdP please consider contributing to this document.","title":"Auth0"},{"location":"operator-manual/sso/microsoft/","text":"Microsoft \u00b6 Are you using this? Please contribute! If you're using this IdP please consider contributing to this document. OIDC (without Dex) to Azure AD With Dex OIDC (without Dex) to Azure AD \u00b6 Register a new Azure AD Application Quickstart: Register an application App Registrations Inputs Redirect URI: https://argocd.example.com/auth/callback Outputs Application (client) ID: aaaaaaaa-1111-bbbb-2222-cccccccccccc Directory (tenant) ID: 33333333-dddd-4444-eeee-555555555555 Secret: some_secret Edit argocd-cm and configure the data.oidc.config section: ConfigMap -> argocd - cm data : url : https : // argocd . example . com / oidc . config : | name : Azure issuer : https : // sts . windows . net / { directory_tenant_id } / clientID : { azure_ad_application_client_id } clientSecret : $ oidc . azure . clientSecret Edit argocd-secret and configure the data.oidc.azure.clientSecret section: Secret -> argocd - secret data : oidc . azure . clientSecret : { client_secret | base64_encoded } Edit argocd-rbac-cm to configure permissions RBAC Configurations ConfigMap -> argocd - cm policy . default : role : readonly policy . csv : | p , role : org - admin , applications , * , * /*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, \"Grp Argo CD\", role:org-admin With Dex \u00b6 ConfigMap -> argocd-cm data : dex.config : | connectors: - type: microsoft id: microsoft name: Your Company GmbH config: clientID: $MICROSOFT_APPLICATION_ID clientSecret: $MICROSOFT_CLIENT_SECRET redirectURI: http://localhost:8080/api/dex/callback tenant: ffffffff-ffff-ffff-ffff-ffffffffffff groups: - DevOps","title":"Microsoft"},{"location":"operator-manual/sso/microsoft/#microsoft","text":"Are you using this? Please contribute! If you're using this IdP please consider contributing to this document. OIDC (without Dex) to Azure AD With Dex","title":"Microsoft"},{"location":"operator-manual/sso/microsoft/#oidc-without-dex-to-azure-ad","text":"Register a new Azure AD Application Quickstart: Register an application App Registrations Inputs Redirect URI: https://argocd.example.com/auth/callback Outputs Application (client) ID: aaaaaaaa-1111-bbbb-2222-cccccccccccc Directory (tenant) ID: 33333333-dddd-4444-eeee-555555555555 Secret: some_secret Edit argocd-cm and configure the data.oidc.config section: ConfigMap -> argocd - cm data : url : https : // argocd . example . com / oidc . config : | name : Azure issuer : https : // sts . windows . net / { directory_tenant_id } / clientID : { azure_ad_application_client_id } clientSecret : $ oidc . azure . clientSecret Edit argocd-secret and configure the data.oidc.azure.clientSecret section: Secret -> argocd - secret data : oidc . azure . clientSecret : { client_secret | base64_encoded } Edit argocd-rbac-cm to configure permissions RBAC Configurations ConfigMap -> argocd - cm policy . default : role : readonly policy . csv : | p , role : org - admin , applications , * , * /*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, \"Grp Argo CD\", role:org-admin","title":"OIDC (without Dex) to Azure AD"},{"location":"operator-manual/sso/microsoft/#with-dex","text":"ConfigMap -> argocd-cm data : dex.config : | connectors: - type: microsoft id: microsoft name: Your Company GmbH config: clientID: $MICROSOFT_APPLICATION_ID clientSecret: $MICROSOFT_CLIENT_SECRET redirectURI: http://localhost:8080/api/dex/callback tenant: ffffffff-ffff-ffff-ffff-ffffffffffff groups: - DevOps","title":"With Dex"},{"location":"operator-manual/sso/okta/","text":"Okta \u00b6 Are you using this? Please contribute! If you're using this IdP please consider contributing to this document. A working Single Sign-On configuration using Okta via at least two methods was achieved using: SAML (with Dex) OIDC (without Dex) SAML (with Dex) \u00b6 Create a new SAML application in Okta UI. I've disabled App Visibility because Dex doesn't support Provider-initated login flows. Click View setup instructions after creating the application in Okta. Copy the SSO URL to the argocd-cm in the data.oicd Download the CA certificate to use in the argocd-cm configuration. Edit the argocd-cm and configure the data.dex.config section: dex.config : | logger: level: debug format: json connectors: - type: saml id: okta name: Okta config: ssoURL: https://yourorganization.oktapreview.com/app/yourorganizationsandbox_appnamesaml_2/rghdr9s6hg98s9dse/sso/saml # You need `caData` _OR_ `ca`, but not both. caData: | <base64 encoded CA cert> # You need `caData` _OR_ `ca`, but not both. ca: /path/to/ca.pem redirectURI: https://ui.argocd.yourorganization.net/api/dex/callback usernameAttr: email emailAttr: email groupsAttr: group OIDC (without Dex) \u00b6 Do you want groups for RBAC later? If you want groups scope returned from Okta you need to unforunately contact support to enable API Access Management with Okta or just use SAML above! Next you may need the API Access Management feature, which the support team can enable for your OktaPreview domain for testing, to enable \"custom scopes\" and a separate endpoint to use instead of the \"public\" /oauth2/v1/authorize API Access Management endpoint. This might be a paid feature if you want OIDC unfortunately. The free alternative I found was SAML. On the Okta Admin page, navigate to the Okta API Management at Security > API . Choose your default authorization server. Click Scopes > Add Scope Add a scope called groups . Click Claims > Add Claim. Add a claim called groups Choose the matching options you need, one example is: e.g. to match groups starting with argocd- you'd return an ID Token using your scope name from step 3 (e.g. groups ) where the groups name matches the regex argocd-.* Edit the argocd-cm and configure the data.oidc.config section: oidc.config : | name: Okta issuer: https://yourorganization.oktapreview.com clientID: 0oaltaqg3oAIf2NOa0h3 clientSecret: ZXF_CfUc-rtwNfzFecGquzdeJ_MxM4sGc8pDT2Tg6t requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]. requestedIDTokenClaims: {\"groups\": {\"essential\": true}}","title":"Okta"},{"location":"operator-manual/sso/okta/#okta","text":"Are you using this? Please contribute! If you're using this IdP please consider contributing to this document. A working Single Sign-On configuration using Okta via at least two methods was achieved using: SAML (with Dex) OIDC (without Dex)","title":"Okta"},{"location":"operator-manual/sso/okta/#saml-with-dex","text":"Create a new SAML application in Okta UI. I've disabled App Visibility because Dex doesn't support Provider-initated login flows. Click View setup instructions after creating the application in Okta. Copy the SSO URL to the argocd-cm in the data.oicd Download the CA certificate to use in the argocd-cm configuration. Edit the argocd-cm and configure the data.dex.config section: dex.config : | logger: level: debug format: json connectors: - type: saml id: okta name: Okta config: ssoURL: https://yourorganization.oktapreview.com/app/yourorganizationsandbox_appnamesaml_2/rghdr9s6hg98s9dse/sso/saml # You need `caData` _OR_ `ca`, but not both. caData: | <base64 encoded CA cert> # You need `caData` _OR_ `ca`, but not both. ca: /path/to/ca.pem redirectURI: https://ui.argocd.yourorganization.net/api/dex/callback usernameAttr: email emailAttr: email groupsAttr: group","title":"SAML (with Dex)"},{"location":"operator-manual/sso/okta/#oidc-without-dex","text":"Do you want groups for RBAC later? If you want groups scope returned from Okta you need to unforunately contact support to enable API Access Management with Okta or just use SAML above! Next you may need the API Access Management feature, which the support team can enable for your OktaPreview domain for testing, to enable \"custom scopes\" and a separate endpoint to use instead of the \"public\" /oauth2/v1/authorize API Access Management endpoint. This might be a paid feature if you want OIDC unfortunately. The free alternative I found was SAML. On the Okta Admin page, navigate to the Okta API Management at Security > API . Choose your default authorization server. Click Scopes > Add Scope Add a scope called groups . Click Claims > Add Claim. Add a claim called groups Choose the matching options you need, one example is: e.g. to match groups starting with argocd- you'd return an ID Token using your scope name from step 3 (e.g. groups ) where the groups name matches the regex argocd-.* Edit the argocd-cm and configure the data.oidc.config section: oidc.config : | name: Okta issuer: https://yourorganization.oktapreview.com clientID: 0oaltaqg3oAIf2NOa0h3 clientSecret: ZXF_CfUc-rtwNfzFecGquzdeJ_MxM4sGc8pDT2Tg6t requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]. requestedIDTokenClaims: {\"groups\": {\"essential\": true}}","title":"OIDC (without Dex)"},{"location":"operator-manual/sso/onelogin/","text":"OneLogin \u00b6 Are you using this? Please contribute! If you're using this IdP please consider contributing to this document.","title":"OneLogin"},{"location":"operator-manual/sso/onelogin/#onelogin","text":"Are you using this? Please contribute! If you're using this IdP please consider contributing to this document.","title":"OneLogin"},{"location":"user-guide/","text":"Overview \u00b6 This guide is for developers who have Argo CD installed for them and are managing applications. Note Please make sure you've completed the getting started guide .","title":"Overview"},{"location":"user-guide/#overview","text":"This guide is for developers who have Argo CD installed for them and are managing applications. Note Please make sure you've completed the getting started guide .","title":"Overview"},{"location":"user-guide/app_deletion/","text":"App Deletion \u00b6 Apps can be deleted with or without a cascade option. A cascade delete , deletes both the app and its resources, rather than only the app. Deletion Using argocd \u00b6 To perform a non-cascade delete: argocd app delete APPNAME To perform a cascade delete: argocd app delete APPNAME --cascade Deletion Using kubectl \u00b6 To perform a non-cascade delete: kubetctl delete app APPNAME To perform a cascade delete set the finalizer, e.g. using kubctl patch : kubectl patch app APPNAME -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io\"]}}' --type merge kubectl delete app APPNAME About The Deletion Finalizer \u00b6 For the technical amongst you, the Argo CD application controller watches for this finalizer: metadata : finalizers : - resources-finalizer.argocd.argoproj.io Argo CD's app controller watches for this and will then delete both the app and its resources. When you invoke argocd app delete with --cascade , the finalizer is added automatically.","title":"App Deletion"},{"location":"user-guide/app_deletion/#app-deletion","text":"Apps can be deleted with or without a cascade option. A cascade delete , deletes both the app and its resources, rather than only the app.","title":"App Deletion"},{"location":"user-guide/app_deletion/#deletion-using-argocd","text":"To perform a non-cascade delete: argocd app delete APPNAME To perform a cascade delete: argocd app delete APPNAME --cascade","title":"Deletion Using argocd"},{"location":"user-guide/app_deletion/#deletion-using-kubectl","text":"To perform a non-cascade delete: kubetctl delete app APPNAME To perform a cascade delete set the finalizer, e.g. using kubctl patch : kubectl patch app APPNAME -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io\"]}}' --type merge kubectl delete app APPNAME","title":"Deletion Using kubectl"},{"location":"user-guide/app_deletion/#about-the-deletion-finalizer","text":"For the technical amongst you, the Argo CD application controller watches for this finalizer: metadata : finalizers : - resources-finalizer.argocd.argoproj.io Argo CD's app controller watches for this and will then delete both the app and its resources. When you invoke argocd app delete with --cascade , the finalizer is added automatically.","title":"About The Deletion Finalizer"},{"location":"user-guide/application_sources/","text":"Tools \u00b6 Production \u00b6 Argo CD supports several different ways in which Kubernetes manifests can be defined: Kustomize applications Helm charts Ksonnet applications A directory of YAML/JSON/Jsonnet manifests, including Jsonnet . Any custom config management tool configured as a config management plugin Development \u00b6 Argo CD also supports uploading local manifests directly. Since this is an anti-pattern of the GitOps paradigm, this should only be done for development purposes. A user with an override permission is required to upload manifests locally (typically an admin). All of the different Kubernetes deployment tools above are supported. To upload a local application: $ argocd app sync APPNAME --local /path/to/dir/","title":"Tools"},{"location":"user-guide/application_sources/#tools","text":"","title":"Tools"},{"location":"user-guide/application_sources/#production","text":"Argo CD supports several different ways in which Kubernetes manifests can be defined: Kustomize applications Helm charts Ksonnet applications A directory of YAML/JSON/Jsonnet manifests, including Jsonnet . Any custom config management tool configured as a config management plugin","title":"Production"},{"location":"user-guide/application_sources/#development","text":"Argo CD also supports uploading local manifests directly. Since this is an anti-pattern of the GitOps paradigm, this should only be done for development purposes. A user with an override permission is required to upload manifests locally (typically an admin). All of the different Kubernetes deployment tools above are supported. To upload a local application: $ argocd app sync APPNAME --local /path/to/dir/","title":"Development"},{"location":"user-guide/auto_sync/","text":"Automated Sync Policy \u00b6 Argo CD has the ability to automatically sync an application when it detects differences between the desired manifests in Git, and the live state in the cluster. A benefit of automatic sync is that CI/CD pipelines no longer need direct access to the Argo CD API server to perform the deployment. Instead, the pipeline makes a commit and push to the Git repository with the changes to the manifests in the tracking Git repo. To configure automated sync run: argocd app set <APPNAME> --sync-policy automated Alternatively, if creating the application an application manifest, specify a syncPolicy with an automated policy. spec : syncPolicy : automated : {} Automatic Pruning \u00b6 By default (and as a safety mechanism), automated sync will not delete resources when Argo CD detects the resource is no longer defined in Git. To prune the resources, a manual sync can always be performed (with pruning checked). Pruning can also be enabled to happen automatically as part of the automated sync by running: argocd app set <APPNAME> --auto-prune Or by setting the prune option to true in the automated sync policy: spec : syncPolicy : automated : prune : true selfHeal : true Automated Sync Semantics \u00b6 An automated sync will only be performed if the application is OutOfSync. Applications in a Synced or error state will not attempt automated sync. Automated sync will only attempt one synchronization per unique combination of commit SHA1 and application parameters. If the most recent successful sync in the history was already performed against the same commit-SHA and parameters, a second sync will not be attempted, unless selfHeal flag is set to true. If selfHeal flag is set to true then sync will be attempted again after self heal timeout (5 seconds by default) which is controller by --self-heal-timeout-seconds flag of argocd-application-controller deployment. Automatic sync will not reattempt a sync if the previous sync attempt against the same commit-SHA and parameters had failed. Rollback cannot be performed against an application with automated sync enabled.","title":"Automated Sync Policy"},{"location":"user-guide/auto_sync/#automated-sync-policy","text":"Argo CD has the ability to automatically sync an application when it detects differences between the desired manifests in Git, and the live state in the cluster. A benefit of automatic sync is that CI/CD pipelines no longer need direct access to the Argo CD API server to perform the deployment. Instead, the pipeline makes a commit and push to the Git repository with the changes to the manifests in the tracking Git repo. To configure automated sync run: argocd app set <APPNAME> --sync-policy automated Alternatively, if creating the application an application manifest, specify a syncPolicy with an automated policy. spec : syncPolicy : automated : {}","title":"Automated Sync Policy"},{"location":"user-guide/auto_sync/#automatic-pruning","text":"By default (and as a safety mechanism), automated sync will not delete resources when Argo CD detects the resource is no longer defined in Git. To prune the resources, a manual sync can always be performed (with pruning checked). Pruning can also be enabled to happen automatically as part of the automated sync by running: argocd app set <APPNAME> --auto-prune Or by setting the prune option to true in the automated sync policy: spec : syncPolicy : automated : prune : true selfHeal : true","title":"Automatic Pruning"},{"location":"user-guide/auto_sync/#automated-sync-semantics","text":"An automated sync will only be performed if the application is OutOfSync. Applications in a Synced or error state will not attempt automated sync. Automated sync will only attempt one synchronization per unique combination of commit SHA1 and application parameters. If the most recent successful sync in the history was already performed against the same commit-SHA and parameters, a second sync will not be attempted, unless selfHeal flag is set to true. If selfHeal flag is set to true then sync will be attempted again after self heal timeout (5 seconds by default) which is controller by --self-heal-timeout-seconds flag of argocd-application-controller deployment. Automatic sync will not reattempt a sync if the previous sync attempt against the same commit-SHA and parameters had failed. Rollback cannot be performed against an application with automated sync enabled.","title":"Automated Sync Semantics"},{"location":"user-guide/best_practices/","text":"Best Practices \u00b6 Separating Config Vs. Source Code Repositories \u00b6 Using a separate Git repository to hold your kubernetes manifests, keeping the config separate from your application source code, is highly recommended for the following reasons: It provides a clean separation of application code vs. application config. There will be times when you wish to modify just the manifests without triggering an entire CI build. For example, you likely do not want to trigger a build if you simply wish to bump the number of replicas in a Deployment spec. Cleaner audit log. For auditing purposes, a repo which only holds configuration will have a much cleaner Git history of what changes were made, without the noise coming from check-ins due to normal development activity. Your application may be comprised of services built from multiple Git repositories, but is deployed as a single unit. Oftentimes, microservices applications are comprised of services with different versioning schemes, and release cycles (e.g. ELK, Kafka + Zookeeper). It may not make sense to store the manifests in one of the source code repositories of a single component. Separation of access. The developers who are developing the application, may not necessarily be the same people who can/should push to production environments, either intentionally or unintentionally. By having separate repos, commit access can be given to the source code repo, and not the application config repo. If you are automating your CI pipeline, pushing manifest changes to the same Git repository can trigger an infinite loop of build jobs and Git commit triggers. Having a separate repo to push config changes to, prevents this from happening. Leaving Room For Imperativeness \u00b6 It may be desired to leave room for some imperativeness/automation, and not have everything defined in your Git manifests. For example, if you want the number of your deployment's replicas to be managed by Horizontal Pod Autoscaler , then you would not want to track replicas in Git. apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment spec : # do not include replicas in the manifests if you want replicas to be controlled by HPA # replicas: 1 template : spec : containers : - image : nginx:1.7.9 name : nginx ports : - containerPort : 80 ... Ensuring Manifests At Git Revisions Are Truly Immutable \u00b6 When using templating tools like helm or kustomize , it is possible for manifests to change their meaning from one day to the next. This is typically caused by changes made to an upstream helm repository or kustomize base. For example, consider the following kustomization.yaml bases : - github.com/argoproj/argo-cd//manifests/cluster-install The above kustomization has a remote base to he HEAD revision of the argo-cd repo. Since this is not stable target, the manifests for this kustomize application can suddenly change meaning, even without any changes to your own Git repository. A better version would be to use a Git tag or commit SHA. For example: bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.11.1","title":"Best Practices"},{"location":"user-guide/best_practices/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/best_practices/#separating-config-vs-source-code-repositories","text":"Using a separate Git repository to hold your kubernetes manifests, keeping the config separate from your application source code, is highly recommended for the following reasons: It provides a clean separation of application code vs. application config. There will be times when you wish to modify just the manifests without triggering an entire CI build. For example, you likely do not want to trigger a build if you simply wish to bump the number of replicas in a Deployment spec. Cleaner audit log. For auditing purposes, a repo which only holds configuration will have a much cleaner Git history of what changes were made, without the noise coming from check-ins due to normal development activity. Your application may be comprised of services built from multiple Git repositories, but is deployed as a single unit. Oftentimes, microservices applications are comprised of services with different versioning schemes, and release cycles (e.g. ELK, Kafka + Zookeeper). It may not make sense to store the manifests in one of the source code repositories of a single component. Separation of access. The developers who are developing the application, may not necessarily be the same people who can/should push to production environments, either intentionally or unintentionally. By having separate repos, commit access can be given to the source code repo, and not the application config repo. If you are automating your CI pipeline, pushing manifest changes to the same Git repository can trigger an infinite loop of build jobs and Git commit triggers. Having a separate repo to push config changes to, prevents this from happening.","title":"Separating Config Vs. Source Code Repositories"},{"location":"user-guide/best_practices/#leaving-room-for-imperativeness","text":"It may be desired to leave room for some imperativeness/automation, and not have everything defined in your Git manifests. For example, if you want the number of your deployment's replicas to be managed by Horizontal Pod Autoscaler , then you would not want to track replicas in Git. apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment spec : # do not include replicas in the manifests if you want replicas to be controlled by HPA # replicas: 1 template : spec : containers : - image : nginx:1.7.9 name : nginx ports : - containerPort : 80 ...","title":"Leaving Room For Imperativeness"},{"location":"user-guide/best_practices/#ensuring-manifests-at-git-revisions-are-truly-immutable","text":"When using templating tools like helm or kustomize , it is possible for manifests to change their meaning from one day to the next. This is typically caused by changes made to an upstream helm repository or kustomize base. For example, consider the following kustomization.yaml bases : - github.com/argoproj/argo-cd//manifests/cluster-install The above kustomization has a remote base to he HEAD revision of the argo-cd repo. Since this is not stable target, the manifests for this kustomize application can suddenly change meaning, even without any changes to your own Git repository. A better version would be to use a Git tag or commit SHA. For example: bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.11.1","title":"Ensuring Manifests At Git Revisions Are Truly Immutable"},{"location":"user-guide/build-environment/","text":"Build Environment \u00b6 v1.4 Custom tools , Helm , and Jsonnet support the following build env vars: ARGOCD_APP_NAME - name of application ARGOCD_APP_NAMESPACE - destination application namespace. ARGOCD_APP_REVISION - the resolved revision, e.g. f913b6cbf58aa5ae5ca1f8a2b149477aebcbd9d8 ARGOCD_APP_SOURCE_PATH - the path of the app within the repo ARGOCD_APP_SOURCE_REPO_URL the repo's URL ARGOCD_APP_SOURCE_TARGET_REVISION - the target revision from the spec, e.g. master .","title":"Build Environment"},{"location":"user-guide/build-environment/#build-environment","text":"v1.4 Custom tools , Helm , and Jsonnet support the following build env vars: ARGOCD_APP_NAME - name of application ARGOCD_APP_NAMESPACE - destination application namespace. ARGOCD_APP_REVISION - the resolved revision, e.g. f913b6cbf58aa5ae5ca1f8a2b149477aebcbd9d8 ARGOCD_APP_SOURCE_PATH - the path of the app within the repo ARGOCD_APP_SOURCE_REPO_URL the repo's URL ARGOCD_APP_SOURCE_TARGET_REVISION - the target revision from the spec, e.g. master .","title":"Build Environment"},{"location":"user-guide/ci_automation/","text":"Automation from CI Pipelines \u00b6 Argo CD follows the GitOps model of deployment, where desired configuration changes are first pushed to Git, and the cluster state then syncs to the desired state in git. This is a departure from imperative pipelines which do not traditionally use Git repositories to hold application config. To push new container images into to a cluster managed by Argo CD, the following workflow (or variations), might be used: Build And Publish A New Container Image \u00b6 docker build -t mycompany/guestbook:v2.0 . docker push mycompany/guestbook:v2.0 Update The Local Manifests Using Your Preferred Templating Tool, And Push The Changes To Git \u00b6 Tip The use of a different Git repository to hold your kubernetes manifests (separate from your application source code), is highly recommended. See best practices for further rationale. git clone https://github.com/mycompany/guestbook-config.git cd guestbook-config # kustomize kustomize edit set imagetag mycompany/guestbook:v2.0 # ksonnet ks param set guestbook image mycompany/guestbook:v2.0 # plain yaml kubectl patch --local -f config-deployment.yaml -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"guestbook\",\"image\":\"mycompany/guestbook:v2.0\"}]}}}}' -o yaml git add . -m \"Update guestbook to v2.0\" git push Synchronize The App (Optional) \u00b6 For convenience, the argocd CLI can be downloaded directly from the API server. This is useful so that the CLI used in the CI pipeline is always kept in-sync and uses argocd binary that is always compatible with the Argo CD API server. export ARGOCD_SERVER = argocd.mycompany.com export ARGOCD_AUTH_TOKEN = <JWT token generated from project> curl -sSL -o /usr/local/bin/argocd https:// ${ ARGOCD_SERVER } /download/argocd-linux-amd64 argocd app sync guestbook argocd app wait guestbook If automated synchronization is configured for the application, this step is unnecessary. The controller will automatically detect the new config (fast tracked using a webhook , or polled every 3 minutes), and automatically sync the new manifests.","title":"Automation from CI Pipelines"},{"location":"user-guide/ci_automation/#automation-from-ci-pipelines","text":"Argo CD follows the GitOps model of deployment, where desired configuration changes are first pushed to Git, and the cluster state then syncs to the desired state in git. This is a departure from imperative pipelines which do not traditionally use Git repositories to hold application config. To push new container images into to a cluster managed by Argo CD, the following workflow (or variations), might be used:","title":"Automation from CI Pipelines"},{"location":"user-guide/ci_automation/#build-and-publish-a-new-container-image","text":"docker build -t mycompany/guestbook:v2.0 . docker push mycompany/guestbook:v2.0","title":"Build And Publish A New Container Image"},{"location":"user-guide/ci_automation/#update-the-local-manifests-using-your-preferred-templating-tool-and-push-the-changes-to-git","text":"Tip The use of a different Git repository to hold your kubernetes manifests (separate from your application source code), is highly recommended. See best practices for further rationale. git clone https://github.com/mycompany/guestbook-config.git cd guestbook-config # kustomize kustomize edit set imagetag mycompany/guestbook:v2.0 # ksonnet ks param set guestbook image mycompany/guestbook:v2.0 # plain yaml kubectl patch --local -f config-deployment.yaml -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"guestbook\",\"image\":\"mycompany/guestbook:v2.0\"}]}}}}' -o yaml git add . -m \"Update guestbook to v2.0\" git push","title":"Update The Local Manifests Using Your Preferred Templating Tool, And Push The Changes To Git"},{"location":"user-guide/ci_automation/#synchronize-the-app-optional","text":"For convenience, the argocd CLI can be downloaded directly from the API server. This is useful so that the CLI used in the CI pipeline is always kept in-sync and uses argocd binary that is always compatible with the Argo CD API server. export ARGOCD_SERVER = argocd.mycompany.com export ARGOCD_AUTH_TOKEN = <JWT token generated from project> curl -sSL -o /usr/local/bin/argocd https:// ${ ARGOCD_SERVER } /download/argocd-linux-amd64 argocd app sync guestbook argocd app wait guestbook If automated synchronization is configured for the application, this step is unnecessary. The controller will automatically detect the new config (fast tracked using a webhook , or polled every 3 minutes), and automatically sync the new manifests.","title":"Synchronize The App (Optional)"},{"location":"user-guide/compare-options/","text":"Compare Options \u00b6 Ignoring Resources That Are Extraneous \u00b6 v1.1 You may wish to exclude resources from the app's overall sync status under certain circumstances. E.g. if they are generated by a tool. This can be done by adding this annotation: metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous Note This only affects the sync status. If the resource's health is degraded, then the app will also be degraded. Kustomize has a feature that allows you to generate config maps ( read more \u29c9 ). You can set generatorOptions to add this annotation so that your app remains in sync: configMapGenerator : - name : my-map literals : - foo=bar generatorOptions : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous kind : Kustomization Note generatorOptions adds annotations to both config maps and secrets ( read more \u29c9 ). You may wish to combine this with the Prune=false sync option .","title":"Compare Options"},{"location":"user-guide/compare-options/#compare-options","text":"","title":"Compare Options"},{"location":"user-guide/compare-options/#ignoring-resources-that-are-extraneous","text":"v1.1 You may wish to exclude resources from the app's overall sync status under certain circumstances. E.g. if they are generated by a tool. This can be done by adding this annotation: metadata : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous Note This only affects the sync status. If the resource's health is degraded, then the app will also be degraded. Kustomize has a feature that allows you to generate config maps ( read more \u29c9 ). You can set generatorOptions to add this annotation so that your app remains in sync: configMapGenerator : - name : my-map literals : - foo=bar generatorOptions : annotations : argocd.argoproj.io/compare-options : IgnoreExtraneous kind : Kustomization Note generatorOptions adds annotations to both config maps and secrets ( read more \u29c9 ). You may wish to combine this with the Prune=false sync option .","title":"Ignoring Resources That Are Extraneous"},{"location":"user-guide/config-management-plugins/","text":"Plugins \u00b6 Argo CD allows integrating more config management tools using config management plugins. Following changes are required to configure new plugin: Make sure required binaries are available in argocd-repo-server pod. The binaries can be added via volume mounts or using custom image (see custom_tools ). Register a new plugin in argocd-cm ConfigMap: data : configManagementPlugins : | - name: pluginName init: # Optional command to initialize application source directory command: [\"sample command\"] args: [\"sample args\"] generate: # Command to generate manifests YAML command: [\"sample command\"] args: [\"sample args\"] The generate command must print a valid YAML stream to stdout. Both init and generate commands are executed inside the application source directory. Create an application and specify required config management plugin name. argocd app create <appName> --config-management-plugin <pluginName> More config management plugin examples are available in argocd-example-apps . Environment \u00b6 Commands have access to The system environment variables Standard build environment Variables in the application spec: v1.2 spec : source : plugin : env : - name : FOO value : bar","title":"Plugins"},{"location":"user-guide/config-management-plugins/#plugins","text":"Argo CD allows integrating more config management tools using config management plugins. Following changes are required to configure new plugin: Make sure required binaries are available in argocd-repo-server pod. The binaries can be added via volume mounts or using custom image (see custom_tools ). Register a new plugin in argocd-cm ConfigMap: data : configManagementPlugins : | - name: pluginName init: # Optional command to initialize application source directory command: [\"sample command\"] args: [\"sample args\"] generate: # Command to generate manifests YAML command: [\"sample command\"] args: [\"sample args\"] The generate command must print a valid YAML stream to stdout. Both init and generate commands are executed inside the application source directory. Create an application and specify required config management plugin name. argocd app create <appName> --config-management-plugin <pluginName> More config management plugin examples are available in argocd-example-apps .","title":"Plugins"},{"location":"user-guide/config-management-plugins/#environment","text":"Commands have access to The system environment variables Standard build environment Variables in the application spec: v1.2 spec : source : plugin : env : - name : FOO value : bar","title":"Environment"},{"location":"user-guide/diffing/","text":"Diffing Customization \u00b6 It is possible for an application to be OutOfSync even immediately after a successful Sync operation. Some reasons for this might be: There is a bug in the manifest, where it contains extra/unknown fields from the actual K8s spec. These extra fields would get dropped when querying Kubernetes for the live state, resulting in an OutOfSync status indicating a missing field was detected. The sync was performed (with pruning disabled), and there are resources which need to be deleted. A controller or mutating webhook is altering the object after it was submitted to Kubernetes in a manner which contradicts Git. A Helm chart is using a template function such as randAlphaNum , which generates different data every time helm template is invoked. For Horizontal Pod Autoscaling (HPA) objects, the HPA controller is known to reorder spec.metrics in a specific order. See kubernetes issue #74099 . To work around this, you can order spec.replicas in Git in the same order that the controller prefers. In case it is impossible to fix the upstream issue, Argo CD allows you to optionally ignore differences of problematic resources. The diffing customization can be configured for single or multiple application resources or at a system level. Application Level Configuration \u00b6 Argo CD allows ignoring differences at a specific JSON path. The following sample application is configured to ignore differences in spec.replicas for all deployments: spec : ignoreDifferences : - group : apps kind : Deployment jsonPointers : - /spec/replicas The above customization could be narrowed to a resource with the specified name and optional namespace: spec : ignoreDifferences : - group : apps kind : Deployment name : guestbook namespace : default jsonPointers : - /spec/replicas System-Level Configuration \u00b6 The comparison of resources with well-known issues can be customized at a system level. Ignored differences can be configured for a specified group and kind in resource.customizations key of argocd-cm ConfigMap. Following is an example of a customization which ignores the caBundle field of a MutatingWebhookConfiguration webhooks: data : resource.customizations : | admissionregistration.k8s.io/MutatingWebhookConfiguration: ignoreDifferences: | jsonPointers: - /webhooks/0/clientConfig/caBundle","title":"Diffing Customization"},{"location":"user-guide/diffing/#diffing-customization","text":"It is possible for an application to be OutOfSync even immediately after a successful Sync operation. Some reasons for this might be: There is a bug in the manifest, where it contains extra/unknown fields from the actual K8s spec. These extra fields would get dropped when querying Kubernetes for the live state, resulting in an OutOfSync status indicating a missing field was detected. The sync was performed (with pruning disabled), and there are resources which need to be deleted. A controller or mutating webhook is altering the object after it was submitted to Kubernetes in a manner which contradicts Git. A Helm chart is using a template function such as randAlphaNum , which generates different data every time helm template is invoked. For Horizontal Pod Autoscaling (HPA) objects, the HPA controller is known to reorder spec.metrics in a specific order. See kubernetes issue #74099 . To work around this, you can order spec.replicas in Git in the same order that the controller prefers. In case it is impossible to fix the upstream issue, Argo CD allows you to optionally ignore differences of problematic resources. The diffing customization can be configured for single or multiple application resources or at a system level.","title":"Diffing Customization"},{"location":"user-guide/diffing/#application-level-configuration","text":"Argo CD allows ignoring differences at a specific JSON path. The following sample application is configured to ignore differences in spec.replicas for all deployments: spec : ignoreDifferences : - group : apps kind : Deployment jsonPointers : - /spec/replicas The above customization could be narrowed to a resource with the specified name and optional namespace: spec : ignoreDifferences : - group : apps kind : Deployment name : guestbook namespace : default jsonPointers : - /spec/replicas","title":"Application Level Configuration"},{"location":"user-guide/diffing/#system-level-configuration","text":"The comparison of resources with well-known issues can be customized at a system level. Ignored differences can be configured for a specified group and kind in resource.customizations key of argocd-cm ConfigMap. Following is an example of a customization which ignores the caBundle field of a MutatingWebhookConfiguration webhooks: data : resource.customizations : | admissionregistration.k8s.io/MutatingWebhookConfiguration: ignoreDifferences: | jsonPointers: - /webhooks/0/clientConfig/caBundle","title":"System-Level Configuration"},{"location":"user-guide/helm/","text":"Helm \u00b6 Values Files \u00b6 Helm has the ability to use a different, or even multiple \"values.yaml\" files to derive its parameters from. Alternate or multiple values file(s), can be specified using the --values flag. The flag can be repeated to support multiple values files: argocd app set helm-guestbook --values values-production.yaml Note Values files must be on the same directory or a subdirectory of the Helm application Helm Parameters \u00b6 Helm has the ability to set parameter values, which override any values in a values.yaml . For example, service.type is a common parameter which is exposed in a Helm chart: helm template . --set service.type = LoadBalancer Similarly Argo CD can override values in the values.yaml parameters using argo app set command, in the form of -p PARAM=VALUE . For example: argocd app set helm-guestbook -p service.type = LoadBalancer Helm Release Name \u00b6 By default the Helm release name is equal to the Application name to which it belongs. Sometimes, especially on a centralised ArgoCD, you may want to override that name, and it is possible with the release-name flag on the cli: argocd app set helm-guestbook --release-name myRelease or using the releaseName for yaml: source : helm : releaseName : myRelease Important notice on overriding the release name Please note that overriding the Helm release name might cause problems when the chart you are deploying is using the app.kubernetes.io/instance label. ArgoCD injects this label with the value of the Application name for tracking purposes. So when overriding the release name, the Application name will stop being equal to the release name. Because ArgoCD will overwrite the label with the Application name it might cause some selectors on the resources to stop working. In order to avoid this we can configure ArgoCD to use another label for tracking in the ArgoCD configmap argocd-cm.yaml - check the lines describing application.instanceLabelKey . Helm Hooks \u00b6 v1.3 or later Helm hooks are similar to Argo CD hooks . In Helm, a hook is any normal Kubernetes resource annotated with the helm.sh/hook annotation. Argo CD supports many (most?) Helm hooks by mapping the Helm annotations onto Argo CD's own hook annotations: Helm Annotation Notes helm.sh/hook: crd-install Supported as equivalent to argocd.argoproj.io/hook: PreSync . helm.sh/hook: pre-delete Not supported. In Helm stable there are 3 cases used to clean up CRDs and 3 to clean-up jobs. helm.sh/hook: pre-rollback Not supported. Never used in Helm stable. helm.sh/hook: pre-install Supported as equivalent to argocd.argoproj.io/hook: PreSync . helm.sh/hook: pre-upgrade Supported as equivalent to argocd.argoproj.io/hook: PreSync . helm.sh/hook: post-upgrade Supported as equivalent to argocd.argoproj.io/hook: PostSync . helm.sh/hook: post-install Supported as equivalent to argocd.argoproj.io/hook: PostSync . helm.sh/hook: post-delete Not supported. Never used in Helm stable. helm.sh/hook: post-rollback Not supported. Never used in Helm stable. helm.sh/hook: test-success Not supported. No equivalent in Argo CD. helm.sh/hook: test-failure Not supported. No equivalent in Argo CD. helm.sh/hook-delete-policy Supported. See also argocd.argoproj.io/hook-delete-policy ). helm.sh/hook-delete-timeout No supported. Never used in Helm stable helm.sh/hook-weight Supported as equivalent to argocd.argoproj.io/sync-wave . Unsupported hooks are ignored. In Argo CD, hooks are created by using kubectl apply , rather than kubectl create . This means that if the hook is named and already exists, it will not change unless you have annotated it with before-hook-creation . 'install' vs 'upgrade' vs 'sync' Argo CD cannot know if it is running a first-time \"install\" or an \"upgrade\" - every operation is a \"sync'. This means that, by default, apps that have pre-install and pre-upgrade will have those hooks run at the same time. Hook Tips \u00b6 Make your hook idempotent. Annotate crd-install with hook-weight: \"-2\" to make sure it runs to success before any install or upgrade hooks. Annotate pre-install and post-install with hook-weight: \"-1\" . This will make sure it runs to success before any upgrade hooks. Annotate pre-upgrade and post-upgrade with hook-delete-policy: before-hook-creation to make sure it runs on every sync. Read more about Argo hooks and Helm hooks . Random Data \u00b6 Helm templating has the ability to generate random data during chart rendering via the randAlphaNum function. Many helm charts from the charts repository make use of this feature. For example, the following is the secret for the redis helm chart : data : {{ - if .Values.password }} redis-password : {{ .Values.password | b64enc | quote }} {{ - else }} redis-password : {{ randAlphaNum 10 | b64enc | quote }} {{ - end }} The Argo CD application controller periodically compares Git state against the live state, running the helm template <CHART> command to generate the helm manifests. Because the random value is regenerated every time the comparison is made, any application which makes use of the randAlphaNum function will always be in an OutOfSync state. This can be mitigated by explicitly setting a value, in the values.yaml such that the value is stable between each comparison. For example: argocd app set redis -p password = abc123 Build Environment \u00b6 v1.4 Helm apps have access to the standard build environment via substitution as parameters. E.g. via the CLI: argocd app create APPNAME \\ --helm-set-string 'app=${ARGOCD_APP_NAME}' Or via declarative syntax: spec : source : helm : parameters : - name : app value : $ARGOCD_APP_NAME","title":"Helm"},{"location":"user-guide/helm/#helm","text":"","title":"Helm"},{"location":"user-guide/helm/#values-files","text":"Helm has the ability to use a different, or even multiple \"values.yaml\" files to derive its parameters from. Alternate or multiple values file(s), can be specified using the --values flag. The flag can be repeated to support multiple values files: argocd app set helm-guestbook --values values-production.yaml Note Values files must be on the same directory or a subdirectory of the Helm application","title":"Values Files"},{"location":"user-guide/helm/#helm-parameters","text":"Helm has the ability to set parameter values, which override any values in a values.yaml . For example, service.type is a common parameter which is exposed in a Helm chart: helm template . --set service.type = LoadBalancer Similarly Argo CD can override values in the values.yaml parameters using argo app set command, in the form of -p PARAM=VALUE . For example: argocd app set helm-guestbook -p service.type = LoadBalancer","title":"Helm Parameters"},{"location":"user-guide/helm/#helm-release-name","text":"By default the Helm release name is equal to the Application name to which it belongs. Sometimes, especially on a centralised ArgoCD, you may want to override that name, and it is possible with the release-name flag on the cli: argocd app set helm-guestbook --release-name myRelease or using the releaseName for yaml: source : helm : releaseName : myRelease Important notice on overriding the release name Please note that overriding the Helm release name might cause problems when the chart you are deploying is using the app.kubernetes.io/instance label. ArgoCD injects this label with the value of the Application name for tracking purposes. So when overriding the release name, the Application name will stop being equal to the release name. Because ArgoCD will overwrite the label with the Application name it might cause some selectors on the resources to stop working. In order to avoid this we can configure ArgoCD to use another label for tracking in the ArgoCD configmap argocd-cm.yaml - check the lines describing application.instanceLabelKey .","title":"Helm Release Name"},{"location":"user-guide/helm/#helm-hooks","text":"v1.3 or later Helm hooks are similar to Argo CD hooks . In Helm, a hook is any normal Kubernetes resource annotated with the helm.sh/hook annotation. Argo CD supports many (most?) Helm hooks by mapping the Helm annotations onto Argo CD's own hook annotations: Helm Annotation Notes helm.sh/hook: crd-install Supported as equivalent to argocd.argoproj.io/hook: PreSync . helm.sh/hook: pre-delete Not supported. In Helm stable there are 3 cases used to clean up CRDs and 3 to clean-up jobs. helm.sh/hook: pre-rollback Not supported. Never used in Helm stable. helm.sh/hook: pre-install Supported as equivalent to argocd.argoproj.io/hook: PreSync . helm.sh/hook: pre-upgrade Supported as equivalent to argocd.argoproj.io/hook: PreSync . helm.sh/hook: post-upgrade Supported as equivalent to argocd.argoproj.io/hook: PostSync . helm.sh/hook: post-install Supported as equivalent to argocd.argoproj.io/hook: PostSync . helm.sh/hook: post-delete Not supported. Never used in Helm stable. helm.sh/hook: post-rollback Not supported. Never used in Helm stable. helm.sh/hook: test-success Not supported. No equivalent in Argo CD. helm.sh/hook: test-failure Not supported. No equivalent in Argo CD. helm.sh/hook-delete-policy Supported. See also argocd.argoproj.io/hook-delete-policy ). helm.sh/hook-delete-timeout No supported. Never used in Helm stable helm.sh/hook-weight Supported as equivalent to argocd.argoproj.io/sync-wave . Unsupported hooks are ignored. In Argo CD, hooks are created by using kubectl apply , rather than kubectl create . This means that if the hook is named and already exists, it will not change unless you have annotated it with before-hook-creation . 'install' vs 'upgrade' vs 'sync' Argo CD cannot know if it is running a first-time \"install\" or an \"upgrade\" - every operation is a \"sync'. This means that, by default, apps that have pre-install and pre-upgrade will have those hooks run at the same time.","title":"Helm Hooks"},{"location":"user-guide/helm/#hook-tips","text":"Make your hook idempotent. Annotate crd-install with hook-weight: \"-2\" to make sure it runs to success before any install or upgrade hooks. Annotate pre-install and post-install with hook-weight: \"-1\" . This will make sure it runs to success before any upgrade hooks. Annotate pre-upgrade and post-upgrade with hook-delete-policy: before-hook-creation to make sure it runs on every sync. Read more about Argo hooks and Helm hooks .","title":"Hook Tips"},{"location":"user-guide/helm/#random-data","text":"Helm templating has the ability to generate random data during chart rendering via the randAlphaNum function. Many helm charts from the charts repository make use of this feature. For example, the following is the secret for the redis helm chart : data : {{ - if .Values.password }} redis-password : {{ .Values.password | b64enc | quote }} {{ - else }} redis-password : {{ randAlphaNum 10 | b64enc | quote }} {{ - end }} The Argo CD application controller periodically compares Git state against the live state, running the helm template <CHART> command to generate the helm manifests. Because the random value is regenerated every time the comparison is made, any application which makes use of the randAlphaNum function will always be in an OutOfSync state. This can be mitigated by explicitly setting a value, in the values.yaml such that the value is stable between each comparison. For example: argocd app set redis -p password = abc123","title":"Random Data"},{"location":"user-guide/helm/#build-environment","text":"v1.4 Helm apps have access to the standard build environment via substitution as parameters. E.g. via the CLI: argocd app create APPNAME \\ --helm-set-string 'app=${ARGOCD_APP_NAME}' Or via declarative syntax: spec : source : helm : parameters : - name : app value : $ARGOCD_APP_NAME","title":"Build Environment"},{"location":"user-guide/jsonnet/","text":"Jsonnet \u00b6 Any file matching *.jsonnet in a directory app is treated as a Jsonnet file. Build Environment \u00b6 v1.4 Jsonnet apps have access to the standard build environment via substitution into TLAs and external variables . E.g. via the CLI: argocd app create APPNAME \\ --jsonnet-ext-str 'app=${ARGOCD_APP_NAME}' \\ --jsonnet-tla-str 'ns=${ARGOCD_APP_NAMESPACE}' Or by declarative syntax: directory : jsonnet : extVars : - name : app value : $ARGOCD_APP_NAME tlas : - name : ns value : $ARGOCD_APP_NAMESPACE","title":"Jsonnet"},{"location":"user-guide/jsonnet/#jsonnet","text":"Any file matching *.jsonnet in a directory app is treated as a Jsonnet file.","title":"Jsonnet"},{"location":"user-guide/jsonnet/#build-environment","text":"v1.4 Jsonnet apps have access to the standard build environment via substitution into TLAs and external variables . E.g. via the CLI: argocd app create APPNAME \\ --jsonnet-ext-str 'app=${ARGOCD_APP_NAME}' \\ --jsonnet-tla-str 'ns=${ARGOCD_APP_NAMESPACE}' Or by declarative syntax: directory : jsonnet : extVars : - name : app value : $ARGOCD_APP_NAME tlas : - name : ns value : $ARGOCD_APP_NAMESPACE","title":"Build Environment"},{"location":"user-guide/ksonnet/","text":"Ksonnet \u00b6 Ksonnet is defunct and no longer supported. Environments \u00b6 Ksonnet has a first class concept of an \"environment.\" To create an application from a ksonnet app directory, an environment must be specified. For example, the following command creates the \"guestbook-default\" app, which points to the default environment: argocd app create guestbook-default --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --env default Parameters \u00b6 Ksonnet parameters all belong to a component. For example, the following are the parameters available in the guestbook app, all of which belong to the guestbook-ui component: $ ks param list COMPONENT PARAM VALUE ========= ===== ===== guestbook-ui containerPort 80 guestbook-ui image \"gcr.io/heptio-images/ks-guestbook-demo:0.1\" guestbook-ui name \"guestbook-ui\" guestbook-ui replicas 1 guestbook-ui servicePort 80 guestbook-ui type \"LoadBalancer\" When overriding ksonnet parameters in Argo CD, the component name should also be specified in the argocd app set command, in the form of -p COMPONENT=PARAM=VALUE . For example: argocd app set guestbook-default -p guestbook-ui = image = gcr.io/heptio-images/ks-guestbook-demo:0.1 Build Environment \u00b6 We do not support the standard build environment for Ksonnet.","title":"Ksonnet"},{"location":"user-guide/ksonnet/#ksonnet","text":"Ksonnet is defunct and no longer supported.","title":"Ksonnet"},{"location":"user-guide/ksonnet/#environments","text":"Ksonnet has a first class concept of an \"environment.\" To create an application from a ksonnet app directory, an environment must be specified. For example, the following command creates the \"guestbook-default\" app, which points to the default environment: argocd app create guestbook-default --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --env default","title":"Environments"},{"location":"user-guide/ksonnet/#parameters","text":"Ksonnet parameters all belong to a component. For example, the following are the parameters available in the guestbook app, all of which belong to the guestbook-ui component: $ ks param list COMPONENT PARAM VALUE ========= ===== ===== guestbook-ui containerPort 80 guestbook-ui image \"gcr.io/heptio-images/ks-guestbook-demo:0.1\" guestbook-ui name \"guestbook-ui\" guestbook-ui replicas 1 guestbook-ui servicePort 80 guestbook-ui type \"LoadBalancer\" When overriding ksonnet parameters in Argo CD, the component name should also be specified in the argocd app set command, in the form of -p COMPONENT=PARAM=VALUE . For example: argocd app set guestbook-default -p guestbook-ui = image = gcr.io/heptio-images/ks-guestbook-demo:0.1","title":"Parameters"},{"location":"user-guide/ksonnet/#build-environment","text":"We do not support the standard build environment for Ksonnet.","title":"Build Environment"},{"location":"user-guide/kustomize/","text":"Kustomize \u00b6 You have three configuration options for Kustomize: namePrefix is a prefix appended to resources for Kustomize apps nameSuffix is a suffix appended to resources for Kustomize apps images is a list of Kustomize image overrides To use Kustomize with an overlay, point your path to the overlay. Tip If you're generating resources, you should read up how to ignore those generated resources using the IgnoreExtraneous compare option . Private Remote Bases \u00b6 If you have remote bases that are either (a) HTTPS and need username/password (b) SSH and need SSH private key, then they'll inherit that from the app's repo. This will work if the remote bases uses the same credentials/private key. It will not work if they use different ones. For security reasons your app only ever knows about it's own repo (not other team's or users repos), and so you won't be able to access other private repo, even if Argo CD knows about them. Read more about private repos . kustomize build Options/Parameters \u00b6 To provide build options to kustomize build add a property to the ArgoCD CM under data: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : kustomize.buildOptions : --load_restrictor none Build Environment \u00b6 Kustomize does not support parameters and therefore cannot support the standard build environment .","title":"Kustomize"},{"location":"user-guide/kustomize/#kustomize","text":"You have three configuration options for Kustomize: namePrefix is a prefix appended to resources for Kustomize apps nameSuffix is a suffix appended to resources for Kustomize apps images is a list of Kustomize image overrides To use Kustomize with an overlay, point your path to the overlay. Tip If you're generating resources, you should read up how to ignore those generated resources using the IgnoreExtraneous compare option .","title":"Kustomize"},{"location":"user-guide/kustomize/#private-remote-bases","text":"If you have remote bases that are either (a) HTTPS and need username/password (b) SSH and need SSH private key, then they'll inherit that from the app's repo. This will work if the remote bases uses the same credentials/private key. It will not work if they use different ones. For security reasons your app only ever knows about it's own repo (not other team's or users repos), and so you won't be able to access other private repo, even if Argo CD knows about them. Read more about private repos .","title":"Private Remote Bases"},{"location":"user-guide/kustomize/#kustomize-build-optionsparameters","text":"To provide build options to kustomize build add a property to the ArgoCD CM under data: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd labels : app.kubernetes.io/name : argocd-cm app.kubernetes.io/part-of : argocd data : kustomize.buildOptions : --load_restrictor none","title":"kustomize build Options/Parameters"},{"location":"user-guide/kustomize/#build-environment","text":"Kustomize does not support parameters and therefore cannot support the standard build environment .","title":"Build Environment"},{"location":"user-guide/orphaned-resources/","text":"Orphaned Resources Monitoring \u00b6 Orphaned Kubernetes resource is a top-level namespaced resource which does not belong to any Argo CD Application. The Orphaned Resources Monitoring feature allows detecting orphaned resources, generate a warning and inspect/remove resources using Argo CD UI. The Orphaned Resources monitoring is enabled in Project settings. Once the feature is enabled each project application which target namespace has orphaned resource will get a warning condition. The orphaned resources can be located using the application details page: Before enabling feature you might consider disabling warning. In this case application users are going to see orphaned resources in the UI but application is won't get a warning condition. Exceptions \u00b6 Not every resource in the Kuberenetes cluster is controlled by the end user. Following resources are never considered as orphaned: Namespaced resources blacklisted in the project. Usually, such resources are managed by cluster administrators and not supposed to be modified by namespace user. ServiceAccount with name default ( and corresponding auto-generated ServiceAccountToken ). Service with name kubernetes in the default namespace.","title":"Orphaned Resources Monitoring"},{"location":"user-guide/orphaned-resources/#orphaned-resources-monitoring","text":"Orphaned Kubernetes resource is a top-level namespaced resource which does not belong to any Argo CD Application. The Orphaned Resources Monitoring feature allows detecting orphaned resources, generate a warning and inspect/remove resources using Argo CD UI. The Orphaned Resources monitoring is enabled in Project settings. Once the feature is enabled each project application which target namespace has orphaned resource will get a warning condition. The orphaned resources can be located using the application details page: Before enabling feature you might consider disabling warning. In this case application users are going to see orphaned resources in the UI but application is won't get a warning condition.","title":"Orphaned Resources Monitoring"},{"location":"user-guide/orphaned-resources/#exceptions","text":"Not every resource in the Kuberenetes cluster is controlled by the end user. Following resources are never considered as orphaned: Namespaced resources blacklisted in the project. Usually, such resources are managed by cluster administrators and not supposed to be modified by namespace user. ServiceAccount with name default ( and corresponding auto-generated ServiceAccountToken ). Service with name kubernetes in the default namespace.","title":"Exceptions"},{"location":"user-guide/parameters/","text":"Parameter Overrides \u00b6 Argo CD provides a mechanism to override the parameters of a ksonnet/helm app. This provides flexibility in having most of the application manifests defined in Git, while leaving room for some parts of the k8s manifests determined dynamically, or outside of Git. It also serves as an alternative way of redeploying an application by changing application parameters via Argo CD, instead of making the changes to the manifests in Git. Tip Many consider this mode of operation as an anti-pattern to GitOps, since the source of truth becomes a union of the Git repository, and the application overrides. The Argo CD parameter overrides feature is provided mainly as a convenience to developers and is intended to be used in dev/test environments, vs. production environments. To use parameter overrides, run the argocd app set -p (COMPONENT=)PARAM=VALUE command: argocd app set guestbook -p guestbook = image = example/guestbook:abcd123 argocd app sync guestbook The PARAM is expected to be a normal YAML path argocd app set guestbook -p guestbook = ingress.enabled = true argocd app set guestbook -p guestbook = ingress.hosts [ 0 ]= guestbook.myclusterurl The following are situations where parameter overrides would be useful: A team maintains a \"dev\" environment, which needs to be continually updated with the latest version of their guestbook application after every build in the tip of master. To address this use case, the application would expose a parameter named image , whose value used in the dev environment contains a placeholder value (e.g. example/guestbook:replaceme ). The placeholder value would be determined externally (outside of Git) such as a build system. Then, as part of the build pipeline, the parameter value of the image would be continually updated to the freshly built image (e.g. argocd app set guestbook -p guestbook=image=example/guestbook:abcd123 ). A sync operation would result in the application being redeployed with the new image. A repository of Helm manifests is already publicly available (e.g. https://github.com/helm/charts). Since commit access to the repository is unavailable, it is useful to be able to install charts from the public repository and customize the deployment with different parameters, without resorting to forking the repository to make the changes. For example, to install Redis from the Helm chart repository and customize the the database password, you would run: argocd app create redis --repo https://github.com/helm/charts.git --path stable/redis --dest-server https://kubernetes.default.svc --dest-namespace default -p password = abc123","title":"Parameter Overrides"},{"location":"user-guide/parameters/#parameter-overrides","text":"Argo CD provides a mechanism to override the parameters of a ksonnet/helm app. This provides flexibility in having most of the application manifests defined in Git, while leaving room for some parts of the k8s manifests determined dynamically, or outside of Git. It also serves as an alternative way of redeploying an application by changing application parameters via Argo CD, instead of making the changes to the manifests in Git. Tip Many consider this mode of operation as an anti-pattern to GitOps, since the source of truth becomes a union of the Git repository, and the application overrides. The Argo CD parameter overrides feature is provided mainly as a convenience to developers and is intended to be used in dev/test environments, vs. production environments. To use parameter overrides, run the argocd app set -p (COMPONENT=)PARAM=VALUE command: argocd app set guestbook -p guestbook = image = example/guestbook:abcd123 argocd app sync guestbook The PARAM is expected to be a normal YAML path argocd app set guestbook -p guestbook = ingress.enabled = true argocd app set guestbook -p guestbook = ingress.hosts [ 0 ]= guestbook.myclusterurl The following are situations where parameter overrides would be useful: A team maintains a \"dev\" environment, which needs to be continually updated with the latest version of their guestbook application after every build in the tip of master. To address this use case, the application would expose a parameter named image , whose value used in the dev environment contains a placeholder value (e.g. example/guestbook:replaceme ). The placeholder value would be determined externally (outside of Git) such as a build system. Then, as part of the build pipeline, the parameter value of the image would be continually updated to the freshly built image (e.g. argocd app set guestbook -p guestbook=image=example/guestbook:abcd123 ). A sync operation would result in the application being redeployed with the new image. A repository of Helm manifests is already publicly available (e.g. https://github.com/helm/charts). Since commit access to the repository is unavailable, it is useful to be able to install charts from the public repository and customize the deployment with different parameters, without resorting to forking the repository to make the changes. For example, to install Redis from the Helm chart repository and customize the the database password, you would run: argocd app create redis --repo https://github.com/helm/charts.git --path stable/redis --dest-server https://kubernetes.default.svc --dest-namespace default -p password = abc123","title":"Parameter Overrides"},{"location":"user-guide/private-repositories/","text":"Private Repositories \u00b6 Credentials \u00b6 If application manifests are located in private repository then repository credentials have to be configured. Argo CD supports both HTTP and SSH Git credentials. HTTPS Username And Password Credential \u00b6 Private repositories that require a username and password typically have a URL that start with https:// rather than git@ or ssh:// . Credentials can be configured using Argo CD CLI: argocd repo add https://github.com/argoproj/argocd-example-apps --username <username> --password <password> or UI: v1.2 or later Navigate to Settings/Repositories Click Connect Repo using HTTPS button and enter credentials Note: username in screenshot is for illustration purposes only , we have no relationship to this GitHub account should it exists Click Connect to test the connection and have the repository added earlier than v1.2 Navigate to Settings/Repositories Click Connect Repo button and enter HTTP credentials Access Token \u00b6 Instead of using username and password you might use access token. Following instructions of your Git hosting service to generate the token: Github Gitlab Bitbucket Then, connect the repository using any non-empty string as username and the access token value as a password. Note For some services, you might have to specify your account name as the username instead of any string. TLS Client Certificates for HTTPS repositories \u00b6 v1.2 and later If your repository server requires you to use TLS client certificates for authentication, you can configure ArgoCD repositories to make use of them. For this purpose, --tls-client-cert-path and --tls-client-cert-key-path switches to the argocd repo add command can be used to specify the files on your local system containing client certificate and the corresponding key, respectively: argocd repo add https://repo.example.com/repo.git --tls-client-cert-path ~/mycert.crt --tls-client-cert-key-path ~/mycert.key Of course, you can also use this in combination with the --username and --password switches, if your repository server should require this. The options --tls-client-cert-path and --tls-client-cert-key-path must always be specified together. Your TLS client certificate and corresponding key can also be configured using the UI, see instructions for adding Git repos using HTTPS. Note Your client certificate and key data must be in PEM format, other formats (such as PKCS12) are not understood. Also make sure that your certificate's key is not password protected, otherwise it cannot be used by ArgoCD. Note When pasting TLS client certificate and key in the text areas in the web UI, make sure they contain no unintended line breaks or additional characters. SSH Private Key Credential \u00b6 Private repositories that require an SSH private key have a URL that typically start with git@ or ssh:// rather than https:// . v1.2 or later You can configure your Git repository using HTTPS either using the CLI or the UI. Using the CLI: argocd repo add git@github.com:argoproj/argocd-example-apps.git --ssh-private-key-path ~/.ssh/id_rsa Using the UI: Navigate to Settings/Repositories Click Connect Repo using SSH button, enter the URL and paste the SSH private key Click Connect to test the connection and have the repository added Note When pasting SSH private key in the UI, make sure there are no unintended line breaks or additional characters in the text area Note When your SSH repository is served from a non-standard port, you have to use ssh:// -style URLs to specify your repository. The scp-style git@yourgit.com:yourrepo URLs do not support port specification, and will treat any port number as part of the repository's path. earlier than v1.2 The Argo CD UI don't support configuring SSH credentials. The SSH credentials can only be configured using the Argo CD CLI: argocd repo add git@github.com:argoproj/argocd-example-apps.git --ssh-private-key-path ~/.ssh/id_rsa Credential templates \u00b6 previous to v1.4 Credential templates are available only via declarative setup, see Repository credentials in Operator Manual. v1.4 and later You can also set up credentials to serve as templates for connecting repositories, without having to repeat credential configuration. For example, if you setup credential templates for the URL prefix https://github.com/argoproj , these credentials will be used for all repositories with this URL as prefix (e.g. https://github.com/argoproj/argocd-example-apps ) that do not have their own credentials configured. To set up a credential template using the Web UI, simply fill in all relevant credential information in the Connect repo using SSH or Connect repo using HTTPS dialogues (as described above), but select Save as credential template instead of Connect to save the credential template. Be sure to only enter the prefix URL (i.e. https://github.com/argoproj ) instead of the complete repository URL (i.e. https://github.com/argoproj/argocd-example-apps ) in the field Repository URL To manage credential templates using the CLI, use the repocreds sub-command, for example argocd repocreds add https://github.com/argoproj --username youruser --password yourpass would setup a credential template for the URL prefix https://github.com/argoproj using the specified username/password combination. Similar to the repo sub-command, you can also list and remove repository credentials using the argocd repocreds list and argocd repocreds rm commands, respectively. In order for ArgoCD to use a credential template for any given repository, the following conditions must be met: The repository must either not be configured at all, or if configured, must not contain any credential information The URL configured for a credential template (e.g. https://github.com/argoproj ) must match as prefix for the repository URL (e.g. https://github.com/argoproj/argocd-example-apps ). Note Repositories that require authentication can be added using CLI or Web UI without specifying credentials only after a matching repository credential has been set up Note Matching credential template URL prefixes is done on a best match effort, so the longest (best) match will take precedence. The order of definition is not important, as opposed to pre v1.4 configuration. The following is an example CLI session, depicting repository credential set-up: # Try to add a private repository without specifying credentials, will fail $ argocd repo add https://docker-build/repos/argocd-example-apps FATA [ 0000 ] rpc error: code = Unknown desc = authentication required # Setup a credential template for all repos under https://docker-build/repos $ argocd repocreds add https://docker-build/repos --username test --password test repository credentials for 'https://docker-build/repos' added # Repeat first step, add repo without specifying credentials # URL for template matches, will succeed $ argocd repo add https://docker-build/repos/argocd-example-apps repository 'https://docker-build/repos/argocd-example-apps' added # Add another repo under https://docker-build/repos, specifying invalid creds # Will fail, because it will not use the template (has own creds) $ argocd repo add https://docker-build/repos/example-apps-part-two --username test --password invalid FATA [ 0000 ] rpc error: code = Unknown desc = authentication required Self-signed & Untrusted TLS Certificates \u00b6 v1.2 or later If you are connecting a repository on a HTTPS server using a self-signed certificate, or a certificate signed by a custom Certificate Authority (CA) which are not known to ArgoCD, the repository will not be added due to security reasons. This is indicated by an error message such as x509: certificate signed by unknown authority . You can let ArgoCD connect the repository in an insecure way, without verifying the server's certificate at all. This can be accomplished by using the --insecure-skip-server-verification flag when adding the repository with the argocd CLI utility. However, this should be done only for non-production setups, as it imposes a serious security issue through possible man-in-the-middle attacks. You can configure ArgoCD to use a custom certificate for the verification of the server's certificate using the cert add-tls command of the argocd CLI utility. This is the recommended method and suitable for production use. In order to do so, you will need the server's certificate, or the certificate of the CA used to sign the server's certificate, in PEM format. Note For invalid server certificates, such as those without matching server name, or those that are expired, adding a CA certificate will not help. In this case, your only option will be to use the --insecure-skip-server-verification flag to connect the repository. You are strongly urged to use a valid certificate on the repository server, or to urge the server's administrator to replace the faulty certificate with a valid one. Note TLS certificates are configured on a per-server, not on a per-repository basis. If you connect multiple repositories from the same server, you only have to configure the certificates once for this server. Note It can take up to a couple of minutes until the changes performed by the argocd cert command are propagated across your cluster, depending on your Kubernetes setup. Managing TLS certificates using the CLI \u00b6 You can list all configured TLS certificates by using the argocd cert list command using the --cert-type https modifier: $ argocd cert list --cert-type https HOSTNAME TYPE SUBTYPE FINGERPRINT/SUBJECT docker-build https rsa CN = ArgoCD Test CA localhost https rsa CN = localhost Example for adding a HTTPS repository to ArgoCD without verifying the server's certificate ( Caution: This is not recommended for production use): argocd repo add --insecure-skip-server-verification https://git.example.com/test-repo Example for adding a CA certificate contained in file ~/myca-cert.pem to properly verify the repository server: argocd cert add-tls git.example.com --from ~/myca-cert.pem argocd repo add https://git.example.com/test-repo You can also add more than one PEM for a server by concatenating them into the input stream. This might be useful if the repository server is about to replace the server certificate, possibly with one signed by a different CA. This way, you can have the old (current) as well as the new (future) certificate co-existing. If you already have the old certificate configured, use the --upsert flag and add the old and the new one in a single run: cat cert1.pem cert2.pem | argocd cert add-tls git.example.com --upsert Note To replace an existing certificate for a server, use the --upsert flag to the cert add-tls CLI command. Finally, TLS certificates can be removed using the argocd cert rm command with the --cert-type https modifier: argocd cert rm --cert-type https localhost Managing TLS certificates using the ArgoCD web UI \u00b6 It is possible to add and remove TLS certificates using the ArgoCD web UI: In the navigation pane to the left, click on \"Settings\" and choose \"Certificates\" from the settings menu The following page lists all currently configured certificates and provides you with the option to add either a new TLS certificate or SSH known entries: Click on \"Add TLS certificate\", fill in relevant data and click on \"Create\". Take care to specify only the FQDN of your repository server (not the URL) and that you C&P the complete PEM of your TLS certificate into the text area field, including the ----BEGIN CERTIFICATE---- and ----END CERTIFICATE---- lines: To remove a certificate, click on the small three-dotted button next to the certificate entry, select \"Remove\" from the pop-up menu and confirm the removal in the following dialogue. Managing TLS certificates using declarative configuration \u00b6 You can also manage TLS certificates in a declarative, self-managed ArgoCD setup. All TLS certificates are stored in the ConfigMap object argocd-tls-cert-cm . Please refer to the Operator Manual for more information. Before v1.2 We do not currently have first-class support for this. See #1513 . As a work-around, you can customize your Argo CD image. See #1344 Unknown SSH Hosts \u00b6 If you are using a privately hosted Git service over SSH, then you have the following options: v1.2 or later You can let ArgoCD connect the repository in an insecure way, without verifying the server's SSH host key at all. This can be accomplished by using the --insecure-skip-server-verification flag when adding the repository with the argocd CLI utility. However, this should be done only for non-production setups, as it imposes a serious security issue through possible man-in-the-middle attacks. You can make the server's SSH public key known to ArgoCD by using the cert add-ssh command of the argocd CLI utility. This is the recommended method and suitable for production use. In order to do so, you will need the server's SSH public host key, in the known_hosts format understood by ssh . You can get the server's public SSH host key e.g. by using the ssh-keyscan utility. Note It can take up to a couple of minutes until the changes performed by the argocd cert command are propagated across your cluster, depending on your Kubernetes setup. Note When importing SSH known hosts key from a known_hosts file, the hostnames or IP addresses in the input data must not be hashed. If your known_hosts file contains hashed entries, it cannot be used as input source for adding SSH known hosts - neither in the CLI nor in the UI. If you absolutely wish to use hashed known hosts data, the only option will be using declarative setup (see below). Be aware that this will break CLI and UI certificate management, so it is generally not recommended. Managing SSH Known Hosts using the CLI \u00b6 You can list all configured SSH known host entries using the argocd cert list command with the --cert-type ssh modifier: $ argocd cert list --cert-type ssh HOSTNAME TYPE SUBTYPE FINGERPRINT/SUBJECT bitbucket.org ssh ssh-rsa SHA256:zzXQOXSRBEiUtuE8AikJYKwbHaxvSc0ojez9YXaGp1A github.com ssh ssh-rsa SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8 gitlab.com ssh ecdsa-sha2-nistp256 SHA256:HbW3g8zUjNSksFbqTiUWPWg2Bq1x8xdGUrliXFzSnUw gitlab.com ssh ssh-ed25519 SHA256:eUXGGm1YGsMAS7vkcx6JOJdOGHPem5gQp4taiCfCLB8 gitlab.com ssh ssh-rsa SHA256:ROQFvPThGrW4RuWLoL9tq9I9zJ42fK4XywyRtbOz/EQ ssh.dev.azure.com ssh ssh-rsa SHA256:ohD8VZEXGWo6Ez8GSEJQ9WpafgLFsOfLOtGGQCQo6Og vs-ssh.visualstudio.com ssh ssh-rsa SHA256:ohD8VZEXGWo6Ez8GSEJQ9WpafgLFsOfLOtGGQCQo6Og For adding SSH known host entries, the argocd cert add-ssh command can be used. You can either add from a file (using the --from <file> modifier), or by reading stdin when the --batch modifier was specified. In both cases, input must be in known_hosts format as understood by the OpenSSH client. Example for adding all available SSH public host keys for a server to ArgoCD, as collected by ssh-keyscan : ssh-keyscan server.example.com | argocd cert add-ssh --batch Example for importing an existing known_hosts file to ArgoCD: argocd cert add-ssh --batch --from /etc/ssh/ssh_known_hosts Finally, SSH known host entries can be removed using the argocd cert rm command with the --cert-type ssh modifier: argocd cert rm bitbucket.org --cert-type ssh If you have multiple SSH known host entries for a given host with different key sub-types (e.g. as for gitlab.com in the example above, there are keys of sub-types ssh-rsa , ssh-ed25519 and ecdsa-sha2-nistp256 ) and you want to only remove one of them, you can further narrow down the selection using the --cert-sub-type modifier: argocd cert rm gitlab.com --cert-type ssh --cert-sub-type ssh-ed25519 Managing SSH known hosts data using the ArgoCD web UI \u00b6 It is possible to add and remove SSH known hosts entries using the ArgoCD web UI: In the navigation pane to the left, click on \"Settings\" and choose \"Certificates\" from the settings menu The following page lists all currently configured certificates and provides you with the option to add either a new TLS certificate or SSH known entries: Click on \"Add SSH known hosts\" and paste your SSH known hosts data in the following mask. Important : Make sure there are no line breaks in the entries (key data) when you paste the data. Afterwards, click on \"Create\". To remove a certificate, click on the small three-dotted button next to the certificate entry, select \"Remove\" from the pop-up menu and confirm the removal in the following dialogue. Managing SSH known hosts data using declarative setup \u00b6 You can also manage SSH known hosts entries in a declarative, self-managed ArgoCD setup. All SSH public host keys are stored in the ConfigMap object argocd-ssh-known-hosts-cm . For more details, please refer to the Operator Manual Before v1.2 (1) You can customize the Argo CD Docker image by adding the host's SSH public key to /etc/ssh/ssh_known_hosts . Additional entries to this file can be generated using the ssh-keyscan utility (e.g. ssh-keyscan your-private-git-server.com . For more information see example which demonstrates how /etc/ssh/ssh_known_hosts can be customized. Note The /etc/ssh/ssh_known_hosts should include Git host on each Argo CD deployment as well as on a computer where argocd repo add is executed. After resolving issue #1514 only argocd-repo-server deployment has to be customized. (1) Add repository using Argo CD CLI and --insecure-ignore-host-key flag: argocd repo add git@github.com:argoproj/argocd-example-apps.git --ssh-private-key-path ~/.ssh/id_rsa --insecure-ignore-host-key Don't use in production The --insecure-ignore-host-key should not be used in production as this is subject to man-in-the-middle attacks. This does not work for Kustomize remote bases or custom plugins For Kustomize support, see #827 . Git Submodules \u00b6 v1.4 or later Submodules are supported and will be picked up automatically. If the submodule repository requires authentication then the credentials will need to match the credentials of the parent repository. Set ARGOCD_GIT_MODULES_ENABLED=false to disable submodule support Declarative Configuration \u00b6 See declarative setup","title":"Private Repositories"},{"location":"user-guide/private-repositories/#private-repositories","text":"","title":"Private Repositories"},{"location":"user-guide/private-repositories/#credentials","text":"If application manifests are located in private repository then repository credentials have to be configured. Argo CD supports both HTTP and SSH Git credentials.","title":"Credentials"},{"location":"user-guide/private-repositories/#https-username-and-password-credential","text":"Private repositories that require a username and password typically have a URL that start with https:// rather than git@ or ssh:// . Credentials can be configured using Argo CD CLI: argocd repo add https://github.com/argoproj/argocd-example-apps --username <username> --password <password> or UI: v1.2 or later Navigate to Settings/Repositories Click Connect Repo using HTTPS button and enter credentials Note: username in screenshot is for illustration purposes only , we have no relationship to this GitHub account should it exists Click Connect to test the connection and have the repository added earlier than v1.2 Navigate to Settings/Repositories Click Connect Repo button and enter HTTP credentials","title":"HTTPS Username And Password Credential"},{"location":"user-guide/private-repositories/#access-token","text":"Instead of using username and password you might use access token. Following instructions of your Git hosting service to generate the token: Github Gitlab Bitbucket Then, connect the repository using any non-empty string as username and the access token value as a password. Note For some services, you might have to specify your account name as the username instead of any string.","title":"Access Token"},{"location":"user-guide/private-repositories/#tls-client-certificates-for-https-repositories","text":"v1.2 and later If your repository server requires you to use TLS client certificates for authentication, you can configure ArgoCD repositories to make use of them. For this purpose, --tls-client-cert-path and --tls-client-cert-key-path switches to the argocd repo add command can be used to specify the files on your local system containing client certificate and the corresponding key, respectively: argocd repo add https://repo.example.com/repo.git --tls-client-cert-path ~/mycert.crt --tls-client-cert-key-path ~/mycert.key Of course, you can also use this in combination with the --username and --password switches, if your repository server should require this. The options --tls-client-cert-path and --tls-client-cert-key-path must always be specified together. Your TLS client certificate and corresponding key can also be configured using the UI, see instructions for adding Git repos using HTTPS. Note Your client certificate and key data must be in PEM format, other formats (such as PKCS12) are not understood. Also make sure that your certificate's key is not password protected, otherwise it cannot be used by ArgoCD. Note When pasting TLS client certificate and key in the text areas in the web UI, make sure they contain no unintended line breaks or additional characters.","title":"TLS Client Certificates for HTTPS repositories"},{"location":"user-guide/private-repositories/#ssh-private-key-credential","text":"Private repositories that require an SSH private key have a URL that typically start with git@ or ssh:// rather than https:// . v1.2 or later You can configure your Git repository using HTTPS either using the CLI or the UI. Using the CLI: argocd repo add git@github.com:argoproj/argocd-example-apps.git --ssh-private-key-path ~/.ssh/id_rsa Using the UI: Navigate to Settings/Repositories Click Connect Repo using SSH button, enter the URL and paste the SSH private key Click Connect to test the connection and have the repository added Note When pasting SSH private key in the UI, make sure there are no unintended line breaks or additional characters in the text area Note When your SSH repository is served from a non-standard port, you have to use ssh:// -style URLs to specify your repository. The scp-style git@yourgit.com:yourrepo URLs do not support port specification, and will treat any port number as part of the repository's path. earlier than v1.2 The Argo CD UI don't support configuring SSH credentials. The SSH credentials can only be configured using the Argo CD CLI: argocd repo add git@github.com:argoproj/argocd-example-apps.git --ssh-private-key-path ~/.ssh/id_rsa","title":"SSH Private Key Credential"},{"location":"user-guide/private-repositories/#credential-templates","text":"previous to v1.4 Credential templates are available only via declarative setup, see Repository credentials in Operator Manual. v1.4 and later You can also set up credentials to serve as templates for connecting repositories, without having to repeat credential configuration. For example, if you setup credential templates for the URL prefix https://github.com/argoproj , these credentials will be used for all repositories with this URL as prefix (e.g. https://github.com/argoproj/argocd-example-apps ) that do not have their own credentials configured. To set up a credential template using the Web UI, simply fill in all relevant credential information in the Connect repo using SSH or Connect repo using HTTPS dialogues (as described above), but select Save as credential template instead of Connect to save the credential template. Be sure to only enter the prefix URL (i.e. https://github.com/argoproj ) instead of the complete repository URL (i.e. https://github.com/argoproj/argocd-example-apps ) in the field Repository URL To manage credential templates using the CLI, use the repocreds sub-command, for example argocd repocreds add https://github.com/argoproj --username youruser --password yourpass would setup a credential template for the URL prefix https://github.com/argoproj using the specified username/password combination. Similar to the repo sub-command, you can also list and remove repository credentials using the argocd repocreds list and argocd repocreds rm commands, respectively. In order for ArgoCD to use a credential template for any given repository, the following conditions must be met: The repository must either not be configured at all, or if configured, must not contain any credential information The URL configured for a credential template (e.g. https://github.com/argoproj ) must match as prefix for the repository URL (e.g. https://github.com/argoproj/argocd-example-apps ). Note Repositories that require authentication can be added using CLI or Web UI without specifying credentials only after a matching repository credential has been set up Note Matching credential template URL prefixes is done on a best match effort, so the longest (best) match will take precedence. The order of definition is not important, as opposed to pre v1.4 configuration. The following is an example CLI session, depicting repository credential set-up: # Try to add a private repository without specifying credentials, will fail $ argocd repo add https://docker-build/repos/argocd-example-apps FATA [ 0000 ] rpc error: code = Unknown desc = authentication required # Setup a credential template for all repos under https://docker-build/repos $ argocd repocreds add https://docker-build/repos --username test --password test repository credentials for 'https://docker-build/repos' added # Repeat first step, add repo without specifying credentials # URL for template matches, will succeed $ argocd repo add https://docker-build/repos/argocd-example-apps repository 'https://docker-build/repos/argocd-example-apps' added # Add another repo under https://docker-build/repos, specifying invalid creds # Will fail, because it will not use the template (has own creds) $ argocd repo add https://docker-build/repos/example-apps-part-two --username test --password invalid FATA [ 0000 ] rpc error: code = Unknown desc = authentication required","title":"Credential templates"},{"location":"user-guide/private-repositories/#self-signed-untrusted-tls-certificates","text":"v1.2 or later If you are connecting a repository on a HTTPS server using a self-signed certificate, or a certificate signed by a custom Certificate Authority (CA) which are not known to ArgoCD, the repository will not be added due to security reasons. This is indicated by an error message such as x509: certificate signed by unknown authority . You can let ArgoCD connect the repository in an insecure way, without verifying the server's certificate at all. This can be accomplished by using the --insecure-skip-server-verification flag when adding the repository with the argocd CLI utility. However, this should be done only for non-production setups, as it imposes a serious security issue through possible man-in-the-middle attacks. You can configure ArgoCD to use a custom certificate for the verification of the server's certificate using the cert add-tls command of the argocd CLI utility. This is the recommended method and suitable for production use. In order to do so, you will need the server's certificate, or the certificate of the CA used to sign the server's certificate, in PEM format. Note For invalid server certificates, such as those without matching server name, or those that are expired, adding a CA certificate will not help. In this case, your only option will be to use the --insecure-skip-server-verification flag to connect the repository. You are strongly urged to use a valid certificate on the repository server, or to urge the server's administrator to replace the faulty certificate with a valid one. Note TLS certificates are configured on a per-server, not on a per-repository basis. If you connect multiple repositories from the same server, you only have to configure the certificates once for this server. Note It can take up to a couple of minutes until the changes performed by the argocd cert command are propagated across your cluster, depending on your Kubernetes setup.","title":"Self-signed &amp; Untrusted TLS Certificates"},{"location":"user-guide/private-repositories/#managing-tls-certificates-using-the-cli","text":"You can list all configured TLS certificates by using the argocd cert list command using the --cert-type https modifier: $ argocd cert list --cert-type https HOSTNAME TYPE SUBTYPE FINGERPRINT/SUBJECT docker-build https rsa CN = ArgoCD Test CA localhost https rsa CN = localhost Example for adding a HTTPS repository to ArgoCD without verifying the server's certificate ( Caution: This is not recommended for production use): argocd repo add --insecure-skip-server-verification https://git.example.com/test-repo Example for adding a CA certificate contained in file ~/myca-cert.pem to properly verify the repository server: argocd cert add-tls git.example.com --from ~/myca-cert.pem argocd repo add https://git.example.com/test-repo You can also add more than one PEM for a server by concatenating them into the input stream. This might be useful if the repository server is about to replace the server certificate, possibly with one signed by a different CA. This way, you can have the old (current) as well as the new (future) certificate co-existing. If you already have the old certificate configured, use the --upsert flag and add the old and the new one in a single run: cat cert1.pem cert2.pem | argocd cert add-tls git.example.com --upsert Note To replace an existing certificate for a server, use the --upsert flag to the cert add-tls CLI command. Finally, TLS certificates can be removed using the argocd cert rm command with the --cert-type https modifier: argocd cert rm --cert-type https localhost","title":"Managing TLS certificates using the CLI"},{"location":"user-guide/private-repositories/#managing-tls-certificates-using-the-argocd-web-ui","text":"It is possible to add and remove TLS certificates using the ArgoCD web UI: In the navigation pane to the left, click on \"Settings\" and choose \"Certificates\" from the settings menu The following page lists all currently configured certificates and provides you with the option to add either a new TLS certificate or SSH known entries: Click on \"Add TLS certificate\", fill in relevant data and click on \"Create\". Take care to specify only the FQDN of your repository server (not the URL) and that you C&P the complete PEM of your TLS certificate into the text area field, including the ----BEGIN CERTIFICATE---- and ----END CERTIFICATE---- lines: To remove a certificate, click on the small three-dotted button next to the certificate entry, select \"Remove\" from the pop-up menu and confirm the removal in the following dialogue.","title":"Managing TLS certificates using the ArgoCD web UI"},{"location":"user-guide/private-repositories/#managing-tls-certificates-using-declarative-configuration","text":"You can also manage TLS certificates in a declarative, self-managed ArgoCD setup. All TLS certificates are stored in the ConfigMap object argocd-tls-cert-cm . Please refer to the Operator Manual for more information. Before v1.2 We do not currently have first-class support for this. See #1513 . As a work-around, you can customize your Argo CD image. See #1344","title":"Managing TLS certificates using declarative configuration"},{"location":"user-guide/private-repositories/#unknown-ssh-hosts","text":"If you are using a privately hosted Git service over SSH, then you have the following options: v1.2 or later You can let ArgoCD connect the repository in an insecure way, without verifying the server's SSH host key at all. This can be accomplished by using the --insecure-skip-server-verification flag when adding the repository with the argocd CLI utility. However, this should be done only for non-production setups, as it imposes a serious security issue through possible man-in-the-middle attacks. You can make the server's SSH public key known to ArgoCD by using the cert add-ssh command of the argocd CLI utility. This is the recommended method and suitable for production use. In order to do so, you will need the server's SSH public host key, in the known_hosts format understood by ssh . You can get the server's public SSH host key e.g. by using the ssh-keyscan utility. Note It can take up to a couple of minutes until the changes performed by the argocd cert command are propagated across your cluster, depending on your Kubernetes setup. Note When importing SSH known hosts key from a known_hosts file, the hostnames or IP addresses in the input data must not be hashed. If your known_hosts file contains hashed entries, it cannot be used as input source for adding SSH known hosts - neither in the CLI nor in the UI. If you absolutely wish to use hashed known hosts data, the only option will be using declarative setup (see below). Be aware that this will break CLI and UI certificate management, so it is generally not recommended.","title":"Unknown SSH Hosts"},{"location":"user-guide/private-repositories/#managing-ssh-known-hosts-using-the-cli","text":"You can list all configured SSH known host entries using the argocd cert list command with the --cert-type ssh modifier: $ argocd cert list --cert-type ssh HOSTNAME TYPE SUBTYPE FINGERPRINT/SUBJECT bitbucket.org ssh ssh-rsa SHA256:zzXQOXSRBEiUtuE8AikJYKwbHaxvSc0ojez9YXaGp1A github.com ssh ssh-rsa SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8 gitlab.com ssh ecdsa-sha2-nistp256 SHA256:HbW3g8zUjNSksFbqTiUWPWg2Bq1x8xdGUrliXFzSnUw gitlab.com ssh ssh-ed25519 SHA256:eUXGGm1YGsMAS7vkcx6JOJdOGHPem5gQp4taiCfCLB8 gitlab.com ssh ssh-rsa SHA256:ROQFvPThGrW4RuWLoL9tq9I9zJ42fK4XywyRtbOz/EQ ssh.dev.azure.com ssh ssh-rsa SHA256:ohD8VZEXGWo6Ez8GSEJQ9WpafgLFsOfLOtGGQCQo6Og vs-ssh.visualstudio.com ssh ssh-rsa SHA256:ohD8VZEXGWo6Ez8GSEJQ9WpafgLFsOfLOtGGQCQo6Og For adding SSH known host entries, the argocd cert add-ssh command can be used. You can either add from a file (using the --from <file> modifier), or by reading stdin when the --batch modifier was specified. In both cases, input must be in known_hosts format as understood by the OpenSSH client. Example for adding all available SSH public host keys for a server to ArgoCD, as collected by ssh-keyscan : ssh-keyscan server.example.com | argocd cert add-ssh --batch Example for importing an existing known_hosts file to ArgoCD: argocd cert add-ssh --batch --from /etc/ssh/ssh_known_hosts Finally, SSH known host entries can be removed using the argocd cert rm command with the --cert-type ssh modifier: argocd cert rm bitbucket.org --cert-type ssh If you have multiple SSH known host entries for a given host with different key sub-types (e.g. as for gitlab.com in the example above, there are keys of sub-types ssh-rsa , ssh-ed25519 and ecdsa-sha2-nistp256 ) and you want to only remove one of them, you can further narrow down the selection using the --cert-sub-type modifier: argocd cert rm gitlab.com --cert-type ssh --cert-sub-type ssh-ed25519","title":"Managing SSH Known Hosts using the CLI"},{"location":"user-guide/private-repositories/#managing-ssh-known-hosts-data-using-the-argocd-web-ui","text":"It is possible to add and remove SSH known hosts entries using the ArgoCD web UI: In the navigation pane to the left, click on \"Settings\" and choose \"Certificates\" from the settings menu The following page lists all currently configured certificates and provides you with the option to add either a new TLS certificate or SSH known entries: Click on \"Add SSH known hosts\" and paste your SSH known hosts data in the following mask. Important : Make sure there are no line breaks in the entries (key data) when you paste the data. Afterwards, click on \"Create\". To remove a certificate, click on the small three-dotted button next to the certificate entry, select \"Remove\" from the pop-up menu and confirm the removal in the following dialogue.","title":"Managing SSH known hosts data using the ArgoCD web UI"},{"location":"user-guide/private-repositories/#managing-ssh-known-hosts-data-using-declarative-setup","text":"You can also manage SSH known hosts entries in a declarative, self-managed ArgoCD setup. All SSH public host keys are stored in the ConfigMap object argocd-ssh-known-hosts-cm . For more details, please refer to the Operator Manual Before v1.2 (1) You can customize the Argo CD Docker image by adding the host's SSH public key to /etc/ssh/ssh_known_hosts . Additional entries to this file can be generated using the ssh-keyscan utility (e.g. ssh-keyscan your-private-git-server.com . For more information see example which demonstrates how /etc/ssh/ssh_known_hosts can be customized. Note The /etc/ssh/ssh_known_hosts should include Git host on each Argo CD deployment as well as on a computer where argocd repo add is executed. After resolving issue #1514 only argocd-repo-server deployment has to be customized. (1) Add repository using Argo CD CLI and --insecure-ignore-host-key flag: argocd repo add git@github.com:argoproj/argocd-example-apps.git --ssh-private-key-path ~/.ssh/id_rsa --insecure-ignore-host-key Don't use in production The --insecure-ignore-host-key should not be used in production as this is subject to man-in-the-middle attacks. This does not work for Kustomize remote bases or custom plugins For Kustomize support, see #827 .","title":"Managing SSH known hosts data using declarative setup"},{"location":"user-guide/private-repositories/#git-submodules","text":"v1.4 or later Submodules are supported and will be picked up automatically. If the submodule repository requires authentication then the credentials will need to match the credentials of the parent repository. Set ARGOCD_GIT_MODULES_ENABLED=false to disable submodule support","title":"Git Submodules"},{"location":"user-guide/private-repositories/#declarative-configuration","text":"See declarative setup","title":"Declarative Configuration"},{"location":"user-guide/projects/","text":"Projects \u00b6 Projects provide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. Projects provide the following features: restrict what may be deployed (trusted Git source repositories) restrict where apps may be deployed to (destination clusters and namespaces) restrict what kinds of objects may or may not be deployed (e.g. RBAC, CRDs, DaemonSets, NetworkPolicy etc...) defining project roles to provide application RBAC (bound to OIDC groups and/or JWT tokens) The Default Project \u00b6 Every application belongs to a single project. If unspecified, an application belongs to the default project, which is created automatically and by default, permits deployments from any source repo, to any cluster, and all resource Kinds. The default project can be modified, but not deleted. When initially created, it's specification is configured to be the most permissive: spec : sourceRepos : - '*' destinations : - namespace : '*' server : '*' clusterResourceWhitelist : - group : '*' kind : '*' Creating Projects \u00b6 Additional projects can be created to give separate teams different levels of access to namespaces. The following command creates a new project myproject which can deploy applications to namespace mynamespace of cluster https://kubernetes.default.svc . The permitted Git source repository is set to https://github.com/argoproj/argocd-example-apps.git repository. argocd proj create myproject -d https://kubernetes.default.svc,mynamespace -s https://github.com/argoproj/argocd-example-apps.git Managing Projects \u00b6 Permitted source Git repositories are managed using commands: argocd proj add-source <PROJECT> <REPO> argocd proj remove-source <PROJECT> <REPO> Permitted destination clusters and namespaces are managed with the commands: argocd proj add-destination <PROJECT> <CLUSTER>,<NAMESPACE> argocd proj remove-destination <PROJECT> <CLUSTER>,<NAMESPACE> Permitted destination K8s resource kinds are managed with the commands. Note that namespaced-scoped resources are restricted via a blacklist, whereas cluster-scoped resources are restricted via whitelist. argocd proj allow-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj allow-namespace-resource <PROJECT> <GROUP> <KIND> argocd proj deny-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj deny-namespace-resource <PROJECT> <GROUP> <KIND> Assign Application To A Project \u00b6 The application project can be changed using app set command. In order to change the project of an app, the user must have permissions to access the new project. argocd app set guestbook-default --project myproject Configuring RBAC With Projects \u00b6 Once projects have been defined, RBAC rules can be written to restrict access to the applications in the project. The following example configures RBAC for two GitHub teams: team1 and team2 , both in the GitHub org, some-github-org . There are two projects, project-a and project-b . team1 can only manage applications in project-a , while team2 can only manage applications in project-b . Both team1 and team2 have the ability to manage repositories. ConfigMap argocd-rbac-cm example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.default : \"\" policy.csv : | p, some-github-org:team1, applications, *, project-a/*, allow p, some-github-org:team2, applications, *, project-b/*, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, some-github-org:team1, org-admin g, some-github-org:team2, org-admin Project Roles \u00b6 Projects include a feature called roles that enable automated access to a project's applications. These can be used to give a CI pipeline a restricted set of permissions. For example, a CI system may only be able to sync a single app (but not change its source or destination). Projects can have multiple roles, and those roles can have different access granted to them. These permissions are called policies, and they are stored within the role as a list of policy strings. A role's policy can only grant access to that role and are limited to applications within the role's project. However, the policies have an option for granting wildcard access to any application within a project. In order to create roles in a project and add policies to a role, a user will need permission to update a project. The following commands can be used to manage a role. argocd proj role list argocd proj role get argocd proj role create argocd proj role delete argocd proj role add-policy argocd proj role remove-policy Project roles in itself are not useful without generating a token to associate to that role. Argo CD supports JWT tokens as the means to authenticate to a role. Since the JWT token is associated with a role's policies, any changes to the role's policies will immediately take effect for that JWT token. The following commands are used to manage the JWT tokens. argocd proj role create-token PROJECT ROLE-NAME argocd proj role delete-token PROJECT ROLE-NAME ISSUED-AT Since the JWT tokens aren't stored in Argo CD, they can only be retrieved when they are created. A user can leverage them in the cli by either passing them in using the --auth-token flag or setting the ARGOCD_AUTH_TOKEN environment variable. The JWT tokens can be used until they expire or are revoked. The JWT tokens can created with or without an expiration, but the default on the cli is creates them without an expirations date. Even if a token has not expired, it cannot be used if the token has been revoked. Below is an example of leveraging a JWT token to access a guestbook application. It makes the assumption that the user already has a project named myproject and an application called guestbook-default. PROJ = myproject APP = guestbook-default ROLE = get-role argocd proj role create $PROJ $ROLE argocd proj role create-token $PROJ $ROLE -e 10m JWT = <value from command above> argocd proj role list $PROJ argocd proj role get $PROJ $ROLE # This command will fail because the JWT Token associated with the project role does not have a policy to allow access to the application argocd app get $APP --auth-token $JWT # Adding a policy to grant access to the application for the new role argocd proj role add-policy $PROJ $ROLE --action get --permission allow --object $APP argocd app get $PROJ - $ROLE --auth-token $JWT # Removing the policy we added and adding one with a wildcard. argocd proj role remove-policy $PROJ $TOKEN -a get -o $PROJ - $TOKEN argocd proj role remove-policy $PROJ $TOKEN -a get -o '*' # The wildcard allows us to access the application due to the wildcard. argocd app get $PROJ - $TOKEN --auth-token $JWT argocd proj role get $PROJ argocd proj role get $PROJ $ROLE # Revoking the JWT token argocd proj role delete-token $PROJ $ROLE <id field from the last command> # This will fail since the JWT Token was deleted for the project role. argocd app get $APP --auth-token $JWT","title":"Projects"},{"location":"user-guide/projects/#projects","text":"Projects provide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. Projects provide the following features: restrict what may be deployed (trusted Git source repositories) restrict where apps may be deployed to (destination clusters and namespaces) restrict what kinds of objects may or may not be deployed (e.g. RBAC, CRDs, DaemonSets, NetworkPolicy etc...) defining project roles to provide application RBAC (bound to OIDC groups and/or JWT tokens)","title":"Projects"},{"location":"user-guide/projects/#the-default-project","text":"Every application belongs to a single project. If unspecified, an application belongs to the default project, which is created automatically and by default, permits deployments from any source repo, to any cluster, and all resource Kinds. The default project can be modified, but not deleted. When initially created, it's specification is configured to be the most permissive: spec : sourceRepos : - '*' destinations : - namespace : '*' server : '*' clusterResourceWhitelist : - group : '*' kind : '*'","title":"The Default Project"},{"location":"user-guide/projects/#creating-projects","text":"Additional projects can be created to give separate teams different levels of access to namespaces. The following command creates a new project myproject which can deploy applications to namespace mynamespace of cluster https://kubernetes.default.svc . The permitted Git source repository is set to https://github.com/argoproj/argocd-example-apps.git repository. argocd proj create myproject -d https://kubernetes.default.svc,mynamespace -s https://github.com/argoproj/argocd-example-apps.git","title":"Creating Projects"},{"location":"user-guide/projects/#managing-projects","text":"Permitted source Git repositories are managed using commands: argocd proj add-source <PROJECT> <REPO> argocd proj remove-source <PROJECT> <REPO> Permitted destination clusters and namespaces are managed with the commands: argocd proj add-destination <PROJECT> <CLUSTER>,<NAMESPACE> argocd proj remove-destination <PROJECT> <CLUSTER>,<NAMESPACE> Permitted destination K8s resource kinds are managed with the commands. Note that namespaced-scoped resources are restricted via a blacklist, whereas cluster-scoped resources are restricted via whitelist. argocd proj allow-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj allow-namespace-resource <PROJECT> <GROUP> <KIND> argocd proj deny-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj deny-namespace-resource <PROJECT> <GROUP> <KIND>","title":"Managing Projects"},{"location":"user-guide/projects/#assign-application-to-a-project","text":"The application project can be changed using app set command. In order to change the project of an app, the user must have permissions to access the new project. argocd app set guestbook-default --project myproject","title":"Assign Application To A Project"},{"location":"user-guide/projects/#configuring-rbac-with-projects","text":"Once projects have been defined, RBAC rules can be written to restrict access to the applications in the project. The following example configures RBAC for two GitHub teams: team1 and team2 , both in the GitHub org, some-github-org . There are two projects, project-a and project-b . team1 can only manage applications in project-a , while team2 can only manage applications in project-b . Both team1 and team2 have the ability to manage repositories. ConfigMap argocd-rbac-cm example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.default : \"\" policy.csv : | p, some-github-org:team1, applications, *, project-a/*, allow p, some-github-org:team2, applications, *, project-b/*, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, some-github-org:team1, org-admin g, some-github-org:team2, org-admin","title":"Configuring RBAC With Projects"},{"location":"user-guide/projects/#project-roles","text":"Projects include a feature called roles that enable automated access to a project's applications. These can be used to give a CI pipeline a restricted set of permissions. For example, a CI system may only be able to sync a single app (but not change its source or destination). Projects can have multiple roles, and those roles can have different access granted to them. These permissions are called policies, and they are stored within the role as a list of policy strings. A role's policy can only grant access to that role and are limited to applications within the role's project. However, the policies have an option for granting wildcard access to any application within a project. In order to create roles in a project and add policies to a role, a user will need permission to update a project. The following commands can be used to manage a role. argocd proj role list argocd proj role get argocd proj role create argocd proj role delete argocd proj role add-policy argocd proj role remove-policy Project roles in itself are not useful without generating a token to associate to that role. Argo CD supports JWT tokens as the means to authenticate to a role. Since the JWT token is associated with a role's policies, any changes to the role's policies will immediately take effect for that JWT token. The following commands are used to manage the JWT tokens. argocd proj role create-token PROJECT ROLE-NAME argocd proj role delete-token PROJECT ROLE-NAME ISSUED-AT Since the JWT tokens aren't stored in Argo CD, they can only be retrieved when they are created. A user can leverage them in the cli by either passing them in using the --auth-token flag or setting the ARGOCD_AUTH_TOKEN environment variable. The JWT tokens can be used until they expire or are revoked. The JWT tokens can created with or without an expiration, but the default on the cli is creates them without an expirations date. Even if a token has not expired, it cannot be used if the token has been revoked. Below is an example of leveraging a JWT token to access a guestbook application. It makes the assumption that the user already has a project named myproject and an application called guestbook-default. PROJ = myproject APP = guestbook-default ROLE = get-role argocd proj role create $PROJ $ROLE argocd proj role create-token $PROJ $ROLE -e 10m JWT = <value from command above> argocd proj role list $PROJ argocd proj role get $PROJ $ROLE # This command will fail because the JWT Token associated with the project role does not have a policy to allow access to the application argocd app get $APP --auth-token $JWT # Adding a policy to grant access to the application for the new role argocd proj role add-policy $PROJ $ROLE --action get --permission allow --object $APP argocd app get $PROJ - $ROLE --auth-token $JWT # Removing the policy we added and adding one with a wildcard. argocd proj role remove-policy $PROJ $TOKEN -a get -o $PROJ - $TOKEN argocd proj role remove-policy $PROJ $TOKEN -a get -o '*' # The wildcard allows us to access the application due to the wildcard. argocd app get $PROJ - $TOKEN --auth-token $JWT argocd proj role get $PROJ argocd proj role get $PROJ $ROLE # Revoking the JWT token argocd proj role delete-token $PROJ $ROLE <id field from the last command> # This will fail since the JWT Token was deleted for the project role. argocd app get $APP --auth-token $JWT","title":"Project Roles"},{"location":"user-guide/resource_hooks/","text":"Resource Hooks \u00b6 Overview \u00b6 Synchronization can be configured using resource hooks. Hooks are ways to run scripts before, during, and after a Sync operation. Hooks can also be run if a Sync operation fails at any point. Some use cases for hooks are: Using a PreSync hook to perform a database schema migration before deploying a new version of the app. Using a Sync hook to orchestrate a complex deployment requiring more sophistication than the Kubernetes rolling update strategy. Using a PostSync hook to run integration and health checks after a deployment. Using a SyncFail hook to run clean-up or finalizer logic if a Sync operation fails. SyncFail hooks are only available starting in v1.2 Usage \u00b6 Hooks are simply Kubernetes manifests annotated with argocd.argoproj.io/hook , e.g.: apiVersion : batch/v1 kind : Job metadata : generateName : schema-migrate- annotations : argocd.argoproj.io/hook : PreSync During a Sync operation, Argo CD will apply the resource during the appropriate phase of the deployment. Hooks can be any type of Kubernetes resource kind, but tend to be Pod, Job or Argo Workflows . Multiple hooks can be specified as a comma separated list. The following hooks are defined: Hook Description PreSync Executes prior to the apply of the manifests. Sync Executes after all PreSync hooks completed and were successful, at the same time as the apply of the manifests. Skip Indicates to Argo CD to skip the apply of the manifest. PostSync Executes after all Sync hooks completed and were successful, a successful apply, and all resources in a Healthy state. SyncFail Executes when the sync operation fails. Available starting in v1.2 Generate Name \u00b6 Named hooks (i.e. ones with /metadata/name ) will only be created once. If you want a hook to be re-created each time either use BeforeHookCreation policy (see below) or /metadata/generateName . Selective Sync \u00b6 Hooks are not run during selective sync . Hook Deletion Policies \u00b6 Hooks can be deleted in an automatic fashion using the annotation: argocd.argoproj.io/hook-delete-policy . apiVersion : batch/v1 kind : Job metadata : generateName : integration-test- annotations : argocd.argoproj.io/hook : PostSync argocd.argoproj.io/hook-delete-policy : HookSucceeded The following policies define when the hook will be deleted. Policy Description HookSucceeded The hook resource is deleted after the hook succeeded (e.g. Job/Workflow completed successfully). HookFailed The hook resource is deleted after the hook failed. BeforeHookCreation Any existing hook resource is deleted before the new one is created (since v1.3). As an alternative to hook deletion policies, both Jobs and Argo Workflows support the ttlSecondsAfterFinished field in the spec, which let their respective controllers delete the Job/Workflow after it completes. spec : ttlSecondsAfterFinished : 600 Using A Hook To Send A Slack Message \u00b6 The following example uses the Slack API to send a a Slack message when sync completes or fails: apiVersion : batch/v1 kind : Job metadata : generateName : app-slack-notification- annotations : argocd.argoproj.io/hook : PostSync argocd.argoproj.io/hook-delete-policy : HookSucceeded spec : template : spec : containers : - name : slack-notification image : appropriate/curl command : - \"curl\" - \"-X\" - \"POST\" - \"--data-urlencode\" - \"payload={\\\"channel\\\": \\\"#somechannel\\\", \\\"username\\\": \\\"hello\\\", \\\"text\\\": \\\"App Sync succeeded\\\", \\\"icon_emoji\\\": \\\":ghost:\\\"}\" - \"https://hooks.slack.com/services/...\" restartPolicy : Never backoffLimit : 2 yaml apiVersion: batch/v1 kind: Job metadata: generateName: app-slack-notification-fail- annotations: argocd.argoproj.io/hook: SyncFail argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - name: slack-notification image: appropriate/curl command: - \"curl\" - \"-X\" - \"POST\" - \"--data-urlencode\" - \"payload={\\\"channel\\\": \\\"#somechannel\\\", \\\"username\\\": \\\"hello\\\", \\\"text\\\": \\\"App Sync failed\\\", \\\"icon_emoji\\\": \\\":ghost:\\\"}\" - \"https://hooks.slack.com/services/...\" restartPolicy: Never backoffLimit: 2","title":"Resource Hooks"},{"location":"user-guide/resource_hooks/#resource-hooks","text":"","title":"Resource Hooks"},{"location":"user-guide/resource_hooks/#overview","text":"Synchronization can be configured using resource hooks. Hooks are ways to run scripts before, during, and after a Sync operation. Hooks can also be run if a Sync operation fails at any point. Some use cases for hooks are: Using a PreSync hook to perform a database schema migration before deploying a new version of the app. Using a Sync hook to orchestrate a complex deployment requiring more sophistication than the Kubernetes rolling update strategy. Using a PostSync hook to run integration and health checks after a deployment. Using a SyncFail hook to run clean-up or finalizer logic if a Sync operation fails. SyncFail hooks are only available starting in v1.2","title":"Overview"},{"location":"user-guide/resource_hooks/#usage","text":"Hooks are simply Kubernetes manifests annotated with argocd.argoproj.io/hook , e.g.: apiVersion : batch/v1 kind : Job metadata : generateName : schema-migrate- annotations : argocd.argoproj.io/hook : PreSync During a Sync operation, Argo CD will apply the resource during the appropriate phase of the deployment. Hooks can be any type of Kubernetes resource kind, but tend to be Pod, Job or Argo Workflows . Multiple hooks can be specified as a comma separated list. The following hooks are defined: Hook Description PreSync Executes prior to the apply of the manifests. Sync Executes after all PreSync hooks completed and were successful, at the same time as the apply of the manifests. Skip Indicates to Argo CD to skip the apply of the manifest. PostSync Executes after all Sync hooks completed and were successful, a successful apply, and all resources in a Healthy state. SyncFail Executes when the sync operation fails. Available starting in v1.2","title":"Usage"},{"location":"user-guide/resource_hooks/#generate-name","text":"Named hooks (i.e. ones with /metadata/name ) will only be created once. If you want a hook to be re-created each time either use BeforeHookCreation policy (see below) or /metadata/generateName .","title":"Generate Name"},{"location":"user-guide/resource_hooks/#selective-sync","text":"Hooks are not run during selective sync .","title":"Selective Sync"},{"location":"user-guide/resource_hooks/#hook-deletion-policies","text":"Hooks can be deleted in an automatic fashion using the annotation: argocd.argoproj.io/hook-delete-policy . apiVersion : batch/v1 kind : Job metadata : generateName : integration-test- annotations : argocd.argoproj.io/hook : PostSync argocd.argoproj.io/hook-delete-policy : HookSucceeded The following policies define when the hook will be deleted. Policy Description HookSucceeded The hook resource is deleted after the hook succeeded (e.g. Job/Workflow completed successfully). HookFailed The hook resource is deleted after the hook failed. BeforeHookCreation Any existing hook resource is deleted before the new one is created (since v1.3). As an alternative to hook deletion policies, both Jobs and Argo Workflows support the ttlSecondsAfterFinished field in the spec, which let their respective controllers delete the Job/Workflow after it completes. spec : ttlSecondsAfterFinished : 600","title":"Hook Deletion Policies"},{"location":"user-guide/resource_hooks/#using-a-hook-to-send-a-slack-message","text":"The following example uses the Slack API to send a a Slack message when sync completes or fails: apiVersion : batch/v1 kind : Job metadata : generateName : app-slack-notification- annotations : argocd.argoproj.io/hook : PostSync argocd.argoproj.io/hook-delete-policy : HookSucceeded spec : template : spec : containers : - name : slack-notification image : appropriate/curl command : - \"curl\" - \"-X\" - \"POST\" - \"--data-urlencode\" - \"payload={\\\"channel\\\": \\\"#somechannel\\\", \\\"username\\\": \\\"hello\\\", \\\"text\\\": \\\"App Sync succeeded\\\", \\\"icon_emoji\\\": \\\":ghost:\\\"}\" - \"https://hooks.slack.com/services/...\" restartPolicy : Never backoffLimit : 2 yaml apiVersion: batch/v1 kind: Job metadata: generateName: app-slack-notification-fail- annotations: argocd.argoproj.io/hook: SyncFail argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - name: slack-notification image: appropriate/curl command: - \"curl\" - \"-X\" - \"POST\" - \"--data-urlencode\" - \"payload={\\\"channel\\\": \\\"#somechannel\\\", \\\"username\\\": \\\"hello\\\", \\\"text\\\": \\\"App Sync failed\\\", \\\"icon_emoji\\\": \\\":ghost:\\\"}\" - \"https://hooks.slack.com/services/...\" restartPolicy: Never backoffLimit: 2","title":"Using A Hook To Send A Slack Message"},{"location":"user-guide/selective_sync/","text":"Selective Sync \u00b6 A selective sync is one where only some resources are sync'd. You can choose which resources from the UI: When doing so, bear in mind: Your sync is not recorded in the history, and so rollback is not possible. Hooks are not run.","title":"Selective Sync"},{"location":"user-guide/selective_sync/#selective-sync","text":"A selective sync is one where only some resources are sync'd. You can choose which resources from the UI: When doing so, bear in mind: Your sync is not recorded in the history, and so rollback is not possible. Hooks are not run.","title":"Selective Sync"},{"location":"user-guide/status-badge/","text":"Status Badge \u00b6 v1.2 Argo CD can display a badge with health and sync status for any application. The feature is disabled by default because badge image is available to any user without authentication. The feature can be enabled using statusbadge.enabled key of argocd-cm ConfigMap (see argocd-cm.yaml ). To show this badge, use the following URL format ${argoCdBaseUrl}/api/badge?name=${appName} , e.g. http://localhost:8080/api/badge?name=guestbook. The URLs for status image are available on application details page: Navigate to application details page and click on 'Details' button. Scroll down to 'Status Badge' section. Select required template such as URL, Markdown etc. for the status image URL in markdown, html, etc are available . Copy the text and paste it into your README or website.","title":"Status Badge"},{"location":"user-guide/status-badge/#status-badge","text":"v1.2 Argo CD can display a badge with health and sync status for any application. The feature is disabled by default because badge image is available to any user without authentication. The feature can be enabled using statusbadge.enabled key of argocd-cm ConfigMap (see argocd-cm.yaml ). To show this badge, use the following URL format ${argoCdBaseUrl}/api/badge?name=${appName} , e.g. http://localhost:8080/api/badge?name=guestbook. The URLs for status image are available on application details page: Navigate to application details page and click on 'Details' button. Scroll down to 'Status Badge' section. Select required template such as URL, Markdown etc. for the status image URL in markdown, html, etc are available . Copy the text and paste it into your README or website.","title":"Status Badge"},{"location":"user-guide/sync-options/","text":"Sync Options \u00b6 No Prune Resources \u00b6 v1.1 You may wish to prevent an object from being pruned: metadata : annotations : argocd.argoproj.io/sync-options : Prune=false In the UI, the pod will simply appear as out-of-sync: The sync-status panel shows that pruning was skipped, and why: The app will be out of sync if Argo CD expects a resource to be pruned. You may wish to use this along with compare options . Disable Kubectl Validation \u00b6 v1.2 For a certain class of objects, it is necessary to kubectl apply them using the --validate=false flag. Examples of this are kubernetes types which uses RawExtension , such as ServiceCatalog . You can do using this annotations: metadata : annotations : argocd.argoproj.io/sync-options : Validate=false If you want to exclude a whole class of objects globally, consider setting resource.customizations in system level configuation .","title":"Sync Options"},{"location":"user-guide/sync-options/#sync-options","text":"","title":"Sync Options"},{"location":"user-guide/sync-options/#no-prune-resources","text":"v1.1 You may wish to prevent an object from being pruned: metadata : annotations : argocd.argoproj.io/sync-options : Prune=false In the UI, the pod will simply appear as out-of-sync: The sync-status panel shows that pruning was skipped, and why: The app will be out of sync if Argo CD expects a resource to be pruned. You may wish to use this along with compare options .","title":"No Prune Resources"},{"location":"user-guide/sync-options/#disable-kubectl-validation","text":"v1.2 For a certain class of objects, it is necessary to kubectl apply them using the --validate=false flag. Examples of this are kubernetes types which uses RawExtension , such as ServiceCatalog . You can do using this annotations: metadata : annotations : argocd.argoproj.io/sync-options : Validate=false If you want to exclude a whole class of objects globally, consider setting resource.customizations in system level configuation .","title":"Disable Kubectl Validation"},{"location":"user-guide/sync-waves/","text":"Sync Phases and Waves \u00b6 v1.1 Argo CD executes a sync operation in a number of steps. At a high-level, there are three phases pre-sync , sync and post-sync . Within each phase you can have one or more waves, than allows you to ensure certain resources are healthy before subsequent resources are synced. How Do I Configure Phases? \u00b6 Pre-sync and post-sync can only contain hooks. Apply the hook annotation: metadata : annotations : argocd.argoproj.io/hook : PreSync Read more about hooks . How Do I Configure Waves? \u00b6 Specify the wave using the following annotation: metadata : annotations : argocd.argoproj.io/sync-wave : \"5\" Hooks and resources are assigned to wave zero by default. The wave can be negative, so you can create a wave that runs before all other resources. How Does It Work? \u00b6 When Argo CD starts a sync, it orders the resources in the following precedence: The phase The wave they are in (lower values first) By kind (e.g. namespaces first) By name It then determines which the number of the next wave to apply. This is the first number where any resource is out-of-sync or unhealthy. It applies resources in that wave. It repeats this process until all phases and waves are in in-sync and healthy. Because an application can have resources that are unhealthy in the first wave, it may be that the app can never get to healthy.","title":"Sync Phases and Waves"},{"location":"user-guide/sync-waves/#sync-phases-and-waves","text":"v1.1 Argo CD executes a sync operation in a number of steps. At a high-level, there are three phases pre-sync , sync and post-sync . Within each phase you can have one or more waves, than allows you to ensure certain resources are healthy before subsequent resources are synced.","title":"Sync Phases and Waves"},{"location":"user-guide/sync-waves/#how-do-i-configure-phases","text":"Pre-sync and post-sync can only contain hooks. Apply the hook annotation: metadata : annotations : argocd.argoproj.io/hook : PreSync Read more about hooks .","title":"How Do I Configure Phases?"},{"location":"user-guide/sync-waves/#how-do-i-configure-waves","text":"Specify the wave using the following annotation: metadata : annotations : argocd.argoproj.io/sync-wave : \"5\" Hooks and resources are assigned to wave zero by default. The wave can be negative, so you can create a wave that runs before all other resources.","title":"How Do I Configure Waves?"},{"location":"user-guide/sync-waves/#how-does-it-work","text":"When Argo CD starts a sync, it orders the resources in the following precedence: The phase The wave they are in (lower values first) By kind (e.g. namespaces first) By name It then determines which the number of the next wave to apply. This is the first number where any resource is out-of-sync or unhealthy. It applies resources in that wave. It repeats this process until all phases and waves are in in-sync and healthy. Because an application can have resources that are unhealthy in the first wave, it may be that the app can never get to healthy.","title":"How Does It Work?"},{"location":"user-guide/sync_windows/","text":"Sync Windows \u00b6 Sync windows are configurable windows of time where syncs will either be blocked or allowed. These are defined by a kind, which can be either allow or deny , a schedule in cron format and a duration along with one or more of either applications , namespaces and clusters . Wildcards are supported. These windows affect the running of both manual and automated syncs but allow an override for manual syncs which is useful if you are only interested in preventing automated syncs or if you need to temporarily override a window to perform a sync. The windows work in the following way. If there are no windows matching an application then all syncs are allowed. If there are any allow windows matching an application then syncs will only be allowed when there ia an active allow windows. If there are any deny windows matching an application then all syncs will be denied when the deny windows are active. If there is an active matching allow and an active matching deny then syncs will be denied as deny windows override allow windows. The UI and the CLI will both display the state of the sync windows. The UI has a panel which will display different colours depending on the state. The colours are as follows. Red: sync denied , Orange: manual allowed and Green: sync allowed . To display the sync state using the CLI: argocd app get APP Which will return the sync state and any matching windows. Name : guestbook Project : default Server : in - cluster Namespace : default URL : http : // localhost : 8080 / applications / guestbook Repo : https : // github . com / argoproj / argocd - example - apps . git Target : Path : guestbook SyncWindow : Sync Denied Assigned Windows : deny : 0 2 * * * : 1 h , allow : 0 2 3 3 3 : 1 h Sync Policy : Automated Sync Status : Synced to ( 5 c2d89b ) Health Status : Healthy Windows can be created using the CLI: argocd proj windows add PROJECT \\ --kind allow \\ --schedule \"0 22 * * *\" \\ --duration 1h \\ --applications \"*\" Alternatively, they can be created directly in the AppProject manifest: apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : default spec : syncWindows : - kind : allow schedule : '10 1 * * *' duration : 1h applications : - '*-prod' manualSync : true - kind : deny schedule : '0 22 * * *' duration : 1h namespaces : - default - kind : allow schedule : '0 23 * * *' duration : 1h clusters : - in-cluster - cluster1 In order to perform a sync when syncs are being prevented by a window, you can configure the window to allow manual syncs using the CLI, UI or directly in the AppProject manifest: argocd proj windows enable-manual-sync PROJECT ID To disable argocd proj windows disable-manual-sync PROJECT ID Windows can be listed using the CLI or viewed in the UI: argocd proj windows list PROJECT ID STATUS KIND SCHEDULE DURATION APPLICATIONS NAMESPACES CLUSTERS MANUALSYNC 0 Active allow * * * * * 1h - - prod1 Disabled 1 Inactive deny * * * * 1 3h - default - Disabled 2 Inactive allow 1 2 * * * 1h prod-* - - Enabled 3 Active deny * * * * * 1h - default - Disabled All fields of a window can be updated using either the CLI or UI. The applications , namespaces and clusters fields require the update to contain all of the required values. For example if updating the namespaces field and it already contains default and kube-system then the new value would have to include those in the list. argocd proj windows update PROJECT ID --namespaces default,kube-system,prod1","title":"Sync Windows"},{"location":"user-guide/sync_windows/#sync-windows","text":"Sync windows are configurable windows of time where syncs will either be blocked or allowed. These are defined by a kind, which can be either allow or deny , a schedule in cron format and a duration along with one or more of either applications , namespaces and clusters . Wildcards are supported. These windows affect the running of both manual and automated syncs but allow an override for manual syncs which is useful if you are only interested in preventing automated syncs or if you need to temporarily override a window to perform a sync. The windows work in the following way. If there are no windows matching an application then all syncs are allowed. If there are any allow windows matching an application then syncs will only be allowed when there ia an active allow windows. If there are any deny windows matching an application then all syncs will be denied when the deny windows are active. If there is an active matching allow and an active matching deny then syncs will be denied as deny windows override allow windows. The UI and the CLI will both display the state of the sync windows. The UI has a panel which will display different colours depending on the state. The colours are as follows. Red: sync denied , Orange: manual allowed and Green: sync allowed . To display the sync state using the CLI: argocd app get APP Which will return the sync state and any matching windows. Name : guestbook Project : default Server : in - cluster Namespace : default URL : http : // localhost : 8080 / applications / guestbook Repo : https : // github . com / argoproj / argocd - example - apps . git Target : Path : guestbook SyncWindow : Sync Denied Assigned Windows : deny : 0 2 * * * : 1 h , allow : 0 2 3 3 3 : 1 h Sync Policy : Automated Sync Status : Synced to ( 5 c2d89b ) Health Status : Healthy Windows can be created using the CLI: argocd proj windows add PROJECT \\ --kind allow \\ --schedule \"0 22 * * *\" \\ --duration 1h \\ --applications \"*\" Alternatively, they can be created directly in the AppProject manifest: apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : default spec : syncWindows : - kind : allow schedule : '10 1 * * *' duration : 1h applications : - '*-prod' manualSync : true - kind : deny schedule : '0 22 * * *' duration : 1h namespaces : - default - kind : allow schedule : '0 23 * * *' duration : 1h clusters : - in-cluster - cluster1 In order to perform a sync when syncs are being prevented by a window, you can configure the window to allow manual syncs using the CLI, UI or directly in the AppProject manifest: argocd proj windows enable-manual-sync PROJECT ID To disable argocd proj windows disable-manual-sync PROJECT ID Windows can be listed using the CLI or viewed in the UI: argocd proj windows list PROJECT ID STATUS KIND SCHEDULE DURATION APPLICATIONS NAMESPACES CLUSTERS MANUALSYNC 0 Active allow * * * * * 1h - - prod1 Disabled 1 Inactive deny * * * * 1 3h - default - Disabled 2 Inactive allow 1 2 * * * 1h prod-* - - Enabled 3 Active deny * * * * * 1h - default - Disabled All fields of a window can be updated using either the CLI or UI. The applications , namespaces and clusters fields require the update to contain all of the required values. For example if updating the namespaces field and it already contains default and kube-system then the new value would have to include those in the list. argocd proj windows update PROJECT ID --namespaces default,kube-system,prod1","title":"Sync Windows"},{"location":"user-guide/tool_detection/","text":"Tool Detection \u00b6 The tool used to build an application is detected as follows: If a specific tool is explicitly configured, then that tool is selected to create your application's manifests. If not, then the tool is detected implicitly as follows: Ksonnet if there are two files, one named app.yaml and one named components/params.libsonnet . Helm if there's a file matching Chart.yaml . Kustomize if there's a kustomization.yaml , kustomization.yml , or Kustomization Otherwise it is assumed to be a plain directory application. References \u00b6 reposerver/repository/repository.go/GetAppSourceType server/repository/repository.go/listAppTypes","title":"Tool Detection"},{"location":"user-guide/tool_detection/#tool-detection","text":"The tool used to build an application is detected as follows: If a specific tool is explicitly configured, then that tool is selected to create your application's manifests. If not, then the tool is detected implicitly as follows: Ksonnet if there are two files, one named app.yaml and one named components/params.libsonnet . Helm if there's a file matching Chart.yaml . Kustomize if there's a kustomization.yaml , kustomization.yml , or Kustomization Otherwise it is assumed to be a plain directory application.","title":"Tool Detection"},{"location":"user-guide/tool_detection/#references","text":"reposerver/repository/repository.go/GetAppSourceType server/repository/repository.go/listAppTypes","title":"References"},{"location":"user-guide/tracking_strategies/","text":"Tracking and Deployment Strategies \u00b6 An Argo CD application spec provides several different ways of track Kubernetes resource manifests In all tracking strategies, the app has the option to sync automatically. If auto-sync is configured, the new resources manifests will be applied automatically -- as soon as a difference is detected. Note In all tracking strategies, any parameter overrides take precedence over the Git state. Helm \u00b6 For Helm, all versions are Semantic Versions . As a result, you can either version ranges: Use Case How Examples Pin to a version (e.g. in production) Use the version number 1.2.0 Track patches (e.g. in pre-production) Use a range 1.2.* or >=1.2.0 <1.3.0 Track minor releases (e.g. in QA) Use a range 1.* or >=1.0.0 <2.0.0 Use the latest (e.g. in local development) Use star range * or >=0.0.0 Read about version ranges Git \u00b6 For Git, all versions are Git references: Use Case How Notes Pin to a version (e.g. in production) Either (a) tag the commit with (e.g. v1.2.0 ) and use that tag, or (b) using commit SHA. See commit pinning . Track patches (e.g. in pre-production) Tag/re-tag the commit, e.g. (e.g. v1.2 ) and use that tag. See tag tracking Track minor releases (e.g. in QA) Re-tag the commit as (e.g. v1 ) and use that tag. See tag tracking Use the latest (e.g. in local development) Use HEAD or master (assuming master is your master branch). See HEAD / Branch Tracking HEAD / Branch Tracking \u00b6 If a branch name, or a symbolic reference (like HEAD) is specified, Argo CD will continually compare live state against the resource manifests defined at the tip of the specified branch or the resolved commit of the symbolic reference. To redeploy an app, makes a changes to your manifests, commit/push to the branch/symbolic reference. They will then detected by Argo CD. Tag Tracking \u00b6 If a tag is specified, the manifests at the specified Git tag will be used to perform the sync comparison. This provides some advantages over branch tracking in that a tag is generally considered more stable, and less frequently updated, with some manual judgement of what constitutes a tag. To redeploy an app, the user uses Git to change the meaning of a tag by retagging it to a different commit SHA. Argo CD will detect the new meaning of the tag when performing the comparison/sync. Commit Pinning \u00b6 If a Git commit SHA is specified, the app is effectively pinned to the manifests defined at the specified commit. This is the most restrictive of the techniques and is typically used to control production environments. Since commit SHAs cannot change meaning, the only way to change the live state of an app which is pinned to a commit, is by updating the tracking revision in the application to a different commit containing the new manifests. Note that parameter overrides can still be set on an app which is pinned to a revision.","title":"Tracking and Deployment Strategies"},{"location":"user-guide/tracking_strategies/#tracking-and-deployment-strategies","text":"An Argo CD application spec provides several different ways of track Kubernetes resource manifests In all tracking strategies, the app has the option to sync automatically. If auto-sync is configured, the new resources manifests will be applied automatically -- as soon as a difference is detected. Note In all tracking strategies, any parameter overrides take precedence over the Git state.","title":"Tracking and Deployment Strategies"},{"location":"user-guide/tracking_strategies/#helm","text":"For Helm, all versions are Semantic Versions . As a result, you can either version ranges: Use Case How Examples Pin to a version (e.g. in production) Use the version number 1.2.0 Track patches (e.g. in pre-production) Use a range 1.2.* or >=1.2.0 <1.3.0 Track minor releases (e.g. in QA) Use a range 1.* or >=1.0.0 <2.0.0 Use the latest (e.g. in local development) Use star range * or >=0.0.0 Read about version ranges","title":"Helm"},{"location":"user-guide/tracking_strategies/#git","text":"For Git, all versions are Git references: Use Case How Notes Pin to a version (e.g. in production) Either (a) tag the commit with (e.g. v1.2.0 ) and use that tag, or (b) using commit SHA. See commit pinning . Track patches (e.g. in pre-production) Tag/re-tag the commit, e.g. (e.g. v1.2 ) and use that tag. See tag tracking Track minor releases (e.g. in QA) Re-tag the commit as (e.g. v1 ) and use that tag. See tag tracking Use the latest (e.g. in local development) Use HEAD or master (assuming master is your master branch). See HEAD / Branch Tracking","title":"Git"},{"location":"user-guide/tracking_strategies/#head-branch-tracking","text":"If a branch name, or a symbolic reference (like HEAD) is specified, Argo CD will continually compare live state against the resource manifests defined at the tip of the specified branch or the resolved commit of the symbolic reference. To redeploy an app, makes a changes to your manifests, commit/push to the branch/symbolic reference. They will then detected by Argo CD.","title":"HEAD / Branch Tracking"},{"location":"user-guide/tracking_strategies/#tag-tracking","text":"If a tag is specified, the manifests at the specified Git tag will be used to perform the sync comparison. This provides some advantages over branch tracking in that a tag is generally considered more stable, and less frequently updated, with some manual judgement of what constitutes a tag. To redeploy an app, the user uses Git to change the meaning of a tag by retagging it to a different commit SHA. Argo CD will detect the new meaning of the tag when performing the comparison/sync.","title":"Tag Tracking"},{"location":"user-guide/tracking_strategies/#commit-pinning","text":"If a Git commit SHA is specified, the app is effectively pinned to the manifests defined at the specified commit. This is the most restrictive of the techniques and is typically used to control production environments. Since commit SHAs cannot change meaning, the only way to change the live state of an app which is pinned to a commit, is by updating the tracking revision in the application to a different commit containing the new manifests. Note that parameter overrides can still be set on an app which is pinned to a revision.","title":"Commit Pinning"}]}