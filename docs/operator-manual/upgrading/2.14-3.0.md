# v2.14 to 3.0

Argo CD 3.0 is meant to be a low-risk upgrade, containing some minor breaking changes. For each change, the next
section will describe how to quickly determine if you are impacted, how to remediate the breaking change, and (if
applicable) how to opt out of the change.

Once 3.0 is released, no more 2.x minor versions will be released. We will continue to cut patch releases for the two
most recent minor versions (so 2.14 until 3.2 is released, and 2.13 until 3.1 is released).

## Breaking Changes

### Fine-Grained RBAC for application `update` and `delete` sub-resources

The default behavior of fine-grained policies have changed so they do not apply to sub-resources anymore.
Prior to v3, when `update` or `delete` actions were allowed on an application, it gave the permission to
update and delete the application itself and any of its sub-resources.

Starting with v3, the `update` or `delete` actions only apply on the application. New policies must be defined
to allow the `update/*` or `delete/*` actions on the application to give permissions on sub-resources.

The v2 behavior can be preserved by setting the config value `server.rbac.disableApplicationFineGrainedRBACInheritance`
to `false` in the Argo CD ConfigMap `argocd-cm`.

Read the [RBAC documentation](../rbac.md#fine-grained-permissions-for-updatedelete-action) for more detailed inforamtion.

### Removal of `argocd_app_sync_status`, `argocd_app_health_status` and `argocd_app_created_time` Metrics

The `argocd_app_sync_status`, `argocd_app_health_status` and `argocd_app_created_time`, deprecated and disabled by
default since 1.5.0, have been removed. The information previously provided by these metrics is now available as labels
on the `argocd_app_info` metric.

#### Detection

Starting with 1.5.0, these metrics are only available if `ARGOCD_LEGACY_CONTROLLER_METRICS` is explicitly set to `true`.
If it is not set to true, you can safely upgrade with no changes.

#### Migration

If you are using these metrics, you will need to update your monitoring dashboards and alerts to use the new metric and
labels before upgrading.

### Changes to RBAC with Dex SSO Authentication

When using Dex, the `sub` claim returned in the authentication was used as the subject for RBAC. That value depends on
the Dex internal implementation and should not be considered an immutable value that represent the subject.

The new behavior will request the `federated:id` [scope](https://dexidp.io/docs/configuration/custom-scopes-claims-clients/) from Dex, and the new value used as the RBAC subject will be based
on the `federated_claims.user_id` claim instead of the `sub` claim.

If you were using the Dex sub claim in RBAC policies, you will need to update them to maintain the same access.

You can know the correct `user_id` to use by decoding the current `sub` claims defined in your policies. You can also configure which
value is used as `user_id` for some [connectors](https://dexidp.io/docs/connectors/).

```sh
$> echo "ChdleGFtcGxlQGFyZ29wcm9qLmlvEgJkZXhfY29ubl9pZA" | base64 -d

example@argoproj.iodex_conn_i%
```

```yaml
# Policies based on the Dex sub claim (wrong)
- g, ChdleGFtcGxlQGFyZ29wcm9qLmlvEgJkZXhfY29ubl9pZA, role:example
- p, ChdleGFtcGxlQGFyZ29wcm9qLmlvEgJkZXhfY29ubl9pZA, applications, *, *, allow

# Policies now based on federated_claims.user_id claim (correct)
- g, example@argoproj.io, role:example
- p, example@argoproj.io, applications, *, *, allow
```

If authenticating with the CLI, make sure to use the new version as well to obtain an authentication token with the
appropriate claims.

## Other changes

### Using `cluster.inClusterEnabled: "false"`

When `cluster.inClusterEnabled: "false"` is explicitly configured, Applications currently configured to
sync on the in-cluster cluster will now be in an Unknown state, without the possibility to sync resources.

It will not be possible to create new Applications using the in-cluster cluster. When deleting existing
Application, it will not delete the previously managed resources.

It is recommended to perform any cleanup or migration to existing in-cluster Application before upgrading
when in-cluster is disabled. To perform cleanup post-migration, the in-cluster will need to be enabled temporarily.
