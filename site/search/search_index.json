{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 What Is Argo CD? \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Why Argo CD? \u00b6 Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand. Getting Started \u00b6 Quick Start \u00b6 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Follow our getting started guide . Further documentation is provided for additional features. How it works \u00b6 Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways: kustomize applications helm charts ksonnet applications jsonnet files Plain directory of YAML/json manifests Any custom config management tool configured as a config management plugin Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available. For a quick 10 minute overview of Argo CD, check out the demo presented to the Sig Apps community meeting: Architecture \u00b6 Argo CD is implemented as a kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the Git repo). A deployed application whose live state deviates from the target state is considered OutOfSync . Argo CD reports & visualizes the differences, while providing facilities to automatically or manually sync the live state back to the desired target state. Any modifications made to the desired target state in the Git repo can be automatically applied and reflected in the specified target environments. For additional details, see architecture overview . Features \u00b6 Automated deployment of applications to specified target environments Support for multiple config management/templating tools (Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML) Ability to manage and deploy to multiple clusters SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn) Multi-tenancy and RBAC policies for authorization Rollback/Roll-anywhere to any application configuration committed in Git repository Health status analysis of application resources Automated configuration drift detection and visualization Automated or manual syncing of applications to its desired state Web UI which provides real-time view of application activity CLI for automation and CI integration Webhook integration (GitHub, BitBucket, GitLab) Access tokens for automation PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green & canary upgrades) Audit trails for application events and API calls Prometheus metrics Parameter overrides for overriding ksonnet/helm parameters in Git Community Blogs And Presentations \u00b6 GitOps with Argo CD: Simplify and Automate Deployments Using GitOps with IBM Multicloud Manager KubeCon talk: CI/CD in Light Speed with K8s and Argo CD KubeCon talk: Machine Learning as Code Among other things, desribes how Kubeflow uses Argo CD to implement GitOPs for ML SIG Apps demo: Argo CD - GitOps Continuous Delivery for Kubernetes Development Status \u00b6 Argo CD is actively developed and is being used in production to deploy SaaS services at Intuit","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#what-is-argo-cd","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.","title":"What Is Argo CD?"},{"location":"#why-argo-cd","text":"Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.","title":"Why Argo CD?"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#quick-start","text":"kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Follow our getting started guide . Further documentation is provided for additional features.","title":"Quick Start"},{"location":"#how-it-works","text":"Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways: kustomize applications helm charts ksonnet applications jsonnet files Plain directory of YAML/json manifests Any custom config management tool configured as a config management plugin Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available. For a quick 10 minute overview of Argo CD, check out the demo presented to the Sig Apps community meeting:","title":"How it works"},{"location":"#architecture","text":"Argo CD is implemented as a kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the Git repo). A deployed application whose live state deviates from the target state is considered OutOfSync . Argo CD reports & visualizes the differences, while providing facilities to automatically or manually sync the live state back to the desired target state. Any modifications made to the desired target state in the Git repo can be automatically applied and reflected in the specified target environments. For additional details, see architecture overview .","title":"Architecture"},{"location":"#features","text":"Automated deployment of applications to specified target environments Support for multiple config management/templating tools (Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML) Ability to manage and deploy to multiple clusters SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn) Multi-tenancy and RBAC policies for authorization Rollback/Roll-anywhere to any application configuration committed in Git repository Health status analysis of application resources Automated configuration drift detection and visualization Automated or manual syncing of applications to its desired state Web UI which provides real-time view of application activity CLI for automation and CI integration Webhook integration (GitHub, BitBucket, GitLab) Access tokens for automation PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green & canary upgrades) Audit trails for application events and API calls Prometheus metrics Parameter overrides for overriding ksonnet/helm parameters in Git","title":"Features"},{"location":"#community-blogs-and-presentations","text":"GitOps with Argo CD: Simplify and Automate Deployments Using GitOps with IBM Multicloud Manager KubeCon talk: CI/CD in Light Speed with K8s and Argo CD KubeCon talk: Machine Learning as Code Among other things, desribes how Kubeflow uses Argo CD to implement GitOPs for ML SIG Apps demo: Argo CD - GitOps Continuous Delivery for Kubernetes","title":"Community Blogs And Presentations"},{"location":"#development-status","text":"Argo CD is actively developed and is being used in production to deploy SaaS services at Intuit","title":"Development Status"},{"location":"faq/","text":"FAQ \u00b6 Why is my application still OutOfSync immediately after a successful Sync? \u00b6 See Diffing documentation for reasons resources can be OutOfSync, and ways to configure Argo CD to ignore fields when differences are expected. Why is my application stuck in Progressing state? \u00b6 Argo CD provides health for several standard Kubernetes types. The Ingress and StatefulSet types have known issues which might cause health check to return Progressing state instead of Healthy . Ingress is considered healthy if status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . Some ingress controllers ( contour , traefik ) don't update status.loadBalancer.ingress field which causes Ingress to stuck in Progressing state forever. StatufulSet is considered healthy if value of status.updatedReplicas field matches to spec.replicas field. Due to Kubernetes bug kubernetes/kubernetes#68573 the status.updatedReplicas is not populated. So unless you run Kubernetes version which include the fix kubernetes/kubernetes#67570 StatefulSet might stay in Progressing state. As workaround Argo CD allows providing health check customization which overrides default behavior. I forgot the admin password, how do I reset it? \u00b6 Edit the argocd-secret secret and update the admin.password field with a new bcrypt hash. You can use a site like https://www.browserling.com/tools/bcrypt to generate a new hash. Another option is to delete both the admin.password and admin.passwordMtime keys and restart argocd-server.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#why-is-my-application-still-outofsync-immediately-after-a-successful-sync","text":"See Diffing documentation for reasons resources can be OutOfSync, and ways to configure Argo CD to ignore fields when differences are expected.","title":"Why is my application still OutOfSync immediately after a successful Sync?"},{"location":"faq/#why-is-my-application-stuck-in-progressing-state","text":"Argo CD provides health for several standard Kubernetes types. The Ingress and StatefulSet types have known issues which might cause health check to return Progressing state instead of Healthy . Ingress is considered healthy if status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . Some ingress controllers ( contour , traefik ) don't update status.loadBalancer.ingress field which causes Ingress to stuck in Progressing state forever. StatufulSet is considered healthy if value of status.updatedReplicas field matches to spec.replicas field. Due to Kubernetes bug kubernetes/kubernetes#68573 the status.updatedReplicas is not populated. So unless you run Kubernetes version which include the fix kubernetes/kubernetes#67570 StatefulSet might stay in Progressing state. As workaround Argo CD allows providing health check customization which overrides default behavior.","title":"Why is my application stuck in Progressing state?"},{"location":"faq/#i-forgot-the-admin-password-how-do-i-reset-it","text":"Edit the argocd-secret secret and update the admin.password field with a new bcrypt hash. You can use a site like https://www.browserling.com/tools/bcrypt to generate a new hash. Another option is to delete both the admin.password and admin.passwordMtime keys and restart argocd-server.","title":"I forgot the admin password, how do I reset it?"},{"location":"getting_started/","text":"Getting Started \u00b6 Requirements \u00b6 Installed kubectl command-line tool Have a kubeconfig file (default location is ~/.kube/config ). 1. Install Argo CD \u00b6 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml This will create a new namespace, argocd , where Argo CD services and application resources will live. On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com 2. Download Argo CD CLI \u00b6 Download the latest Argo CD version from [https://github.com/argoproj/argo-cd/releases/latest]. Also available in Mac Homebrew: brew tap argoproj/tap brew install argoproj/tap/argocd 3. Access The Argo CD API Server \u00b6 By default, the Argo CD API server is not exposed with an external IP. To access the API server, choose one of the following techniques to expose the Argo CD API server: Service Type Load Balancer \u00b6 Change the argocd-server service type to LoadBalancer : kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Ingress \u00b6 Follow the ingress documentation on how to configure Argo CD with ingress. Port Forwarding \u00b6 Kubectl port-forwarding can also be used to connect to the API server without exposing the service. kubectl port-forward svc/argocd-server -n argocd 8080 :443 The API server can then be accessed using the localhost:8080 4. Login Using The CLI \u00b6 Login as the admin user. The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the command: kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 Using the above password, login to Argo CD's IP or hostname: argocd login <ARGOCD_SERVER> Change the password using the command: argocd account update-password 5. Register A Cluster To Deploy Apps To (Optional) \u00b6 This step registers a cluster's credentials to Argo CD, and is only necessary when deploying to an external cluster. When deploying internally (to the same cluster that Argo CD is running in), https://kubernetes.default.svc should be used as the application's K8s API server address. First list all clusters contexts in your current kubconfig: argocd cluster add Choose a context name from the list and supply it to argocd cluster add CONTEXTNAME . For example, for docker-for-desktop context, run: argocd cluster add docker-for-desktop The above command installs a ServiceAccount ( argocd-manager ), into the kube-system namespace of that kubectl context, and binds the service account to an admin-level ClusterRole. Argo CD uses this service account token to perform its management tasks (i.e. deploy/monitoring). Note The rules of the argocd-manager-role role can be modified such that it only has create , update , patch , delete privileges to a limited set of namespaces, groups, kinds. However get , list , watch privileges are required at the cluster-scope for Argo CD to function. 6. Create An Application From A Git Repository \u00b6 An example repository containing a guestbook application is available at https://github.com/argoproj/argocd-example-apps.git to demonstrate how Argo CD works. Creating Apps Via CLI \u00b6 argocd app create guestbook \\ --repo https://github.com/argoproj/argocd-example-apps.git \\ --path guestbook \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default Creating Apps Via UI \u00b6 Open a browser to the Argo CD external UI, and login using the credentials, IP/hostname set in step 4. Connect the https://github.com/argoproj/argocd-example-apps.git repo to Argo CD: After connecting a repository, select the guestbook application for creation: 7. Sync (Deploy) The Application \u00b6 Once the guestbook application is created, you can now view its status: $ argocd app get guestbook Name: guestbook Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.164.88/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook Sync Policy: <none> Sync Status: OutOfSync from ( 1ff8a67 ) Health Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH apps Deployment default guestbook-ui OutOfSync Missing Service default guestbook-ui OutOfSync Missing The application status is initially OutOfSync state, since the application has yet to be deployed, and no Kubernetes resources have been created. To sync (deploy) the application, run: argocd app sync guestbook This command retrieves the manifests from the repository and performs a kubectl apply of the manifests. The guestbook app is now running and you can now view its resource components, logs, events, and assessed health status: From UI: \u00b6","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#requirements","text":"Installed kubectl command-line tool Have a kubeconfig file (default location is ~/.kube/config ).","title":"Requirements"},{"location":"getting_started/#1-install-argo-cd","text":"kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml This will create a new namespace, argocd , where Argo CD services and application resources will live. On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com","title":"1. Install Argo CD"},{"location":"getting_started/#2-download-argo-cd-cli","text":"Download the latest Argo CD version from [https://github.com/argoproj/argo-cd/releases/latest]. Also available in Mac Homebrew: brew tap argoproj/tap brew install argoproj/tap/argocd","title":"2. Download Argo CD CLI"},{"location":"getting_started/#3-access-the-argo-cd-api-server","text":"By default, the Argo CD API server is not exposed with an external IP. To access the API server, choose one of the following techniques to expose the Argo CD API server:","title":"3. Access The Argo CD API Server"},{"location":"getting_started/#service-type-load-balancer","text":"Change the argocd-server service type to LoadBalancer : kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'","title":"Service Type Load Balancer"},{"location":"getting_started/#ingress","text":"Follow the ingress documentation on how to configure Argo CD with ingress.","title":"Ingress"},{"location":"getting_started/#port-forwarding","text":"Kubectl port-forwarding can also be used to connect to the API server without exposing the service. kubectl port-forward svc/argocd-server -n argocd 8080 :443 The API server can then be accessed using the localhost:8080","title":"Port Forwarding"},{"location":"getting_started/#4-login-using-the-cli","text":"Login as the admin user. The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the command: kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 Using the above password, login to Argo CD's IP or hostname: argocd login <ARGOCD_SERVER> Change the password using the command: argocd account update-password","title":"4. Login Using The CLI"},{"location":"getting_started/#5-register-a-cluster-to-deploy-apps-to-optional","text":"This step registers a cluster's credentials to Argo CD, and is only necessary when deploying to an external cluster. When deploying internally (to the same cluster that Argo CD is running in), https://kubernetes.default.svc should be used as the application's K8s API server address. First list all clusters contexts in your current kubconfig: argocd cluster add Choose a context name from the list and supply it to argocd cluster add CONTEXTNAME . For example, for docker-for-desktop context, run: argocd cluster add docker-for-desktop The above command installs a ServiceAccount ( argocd-manager ), into the kube-system namespace of that kubectl context, and binds the service account to an admin-level ClusterRole. Argo CD uses this service account token to perform its management tasks (i.e. deploy/monitoring). Note The rules of the argocd-manager-role role can be modified such that it only has create , update , patch , delete privileges to a limited set of namespaces, groups, kinds. However get , list , watch privileges are required at the cluster-scope for Argo CD to function.","title":"5. Register A Cluster To Deploy Apps To (Optional)"},{"location":"getting_started/#6-create-an-application-from-a-git-repository","text":"An example repository containing a guestbook application is available at https://github.com/argoproj/argocd-example-apps.git to demonstrate how Argo CD works.","title":"6. Create An Application From A Git Repository"},{"location":"getting_started/#creating-apps-via-cli","text":"argocd app create guestbook \\ --repo https://github.com/argoproj/argocd-example-apps.git \\ --path guestbook \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default","title":"Creating Apps Via CLI"},{"location":"getting_started/#creating-apps-via-ui","text":"Open a browser to the Argo CD external UI, and login using the credentials, IP/hostname set in step 4. Connect the https://github.com/argoproj/argocd-example-apps.git repo to Argo CD: After connecting a repository, select the guestbook application for creation:","title":"Creating Apps Via UI"},{"location":"getting_started/#7-sync-deploy-the-application","text":"Once the guestbook application is created, you can now view its status: $ argocd app get guestbook Name: guestbook Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.164.88/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook Sync Policy: <none> Sync Status: OutOfSync from ( 1ff8a67 ) Health Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH apps Deployment default guestbook-ui OutOfSync Missing Service default guestbook-ui OutOfSync Missing The application status is initially OutOfSync state, since the application has yet to be deployed, and no Kubernetes resources have been created. To sync (deploy) the application, run: argocd app sync guestbook This command retrieves the manifests from the repository and performs a kubectl apply of the manifests. The guestbook app is now running and you can now view its resource components, logs, events, and assessed health status:","title":"7. Sync (Deploy) The Application"},{"location":"getting_started/#from-ui","text":"","title":"From UI:"},{"location":"developer-guide/","text":"Overview \u00b6 Warning This part of the manual is aimed at people wanting to develop third-party applications that interact with Argo CD, e.g. An chat bot An Slack integration","title":"Overview"},{"location":"developer-guide/#overview","text":"Warning This part of the manual is aimed at people wanting to develop third-party applications that interact with Argo CD, e.g. An chat bot An Slack integration","title":"Overview"},{"location":"developer-guide/releasing/","text":"Releasing \u00b6 Tag, build, and push argo-cd-ui cd argo-cd-ui git checkout -b release-X.Y git tag vX.Y.Z git push upstream release-X.Y --tags IMAGE_NAMESPACE = argoproj IMAGE_TAG = vX.Y.Z DOCKER_PUSH = true yarn docker Create release-X.Y branch (if creating initial X.Y release) git checkout -b release-X.Y git push upstream release-X.Y Update VERSION and manifests with new version vi VERSION # ensure value is desired X.Y.Z semantic version make manifests IMAGE_TAG = vX.Y.Z git commit -a -m \"Update manifests to vX.Y.Z\" git push upstream release-X.Y Tag, build, and push release to docker hub git tag vX.Y.Z make release IMAGE_NAMESPACE = argoproj IMAGE_TAG = vX.Y.Z DOCKER_PUSH = true git push upstream vX.Y.Z Update argocd brew formula git clone https://github.com/argoproj/homebrew-tap cd homebrew-tap ./update.sh ~/go/src/github.com/argoproj/argo-cd/dist/argocd-darwin-amd64 git commit -a -m \"Update argocd to vX.Y.Z\" git push Update documentation: Edit CHANGELOG.md with release notes Update stable tag git tag stable --force && git push upstream stable --force Create GitHub release from new tag and upload binaries (e.g. dist/argocd-darwin-amd64)","title":"Releasing"},{"location":"developer-guide/releasing/#releasing","text":"Tag, build, and push argo-cd-ui cd argo-cd-ui git checkout -b release-X.Y git tag vX.Y.Z git push upstream release-X.Y --tags IMAGE_NAMESPACE = argoproj IMAGE_TAG = vX.Y.Z DOCKER_PUSH = true yarn docker Create release-X.Y branch (if creating initial X.Y release) git checkout -b release-X.Y git push upstream release-X.Y Update VERSION and manifests with new version vi VERSION # ensure value is desired X.Y.Z semantic version make manifests IMAGE_TAG = vX.Y.Z git commit -a -m \"Update manifests to vX.Y.Z\" git push upstream release-X.Y Tag, build, and push release to docker hub git tag vX.Y.Z make release IMAGE_NAMESPACE = argoproj IMAGE_TAG = vX.Y.Z DOCKER_PUSH = true git push upstream vX.Y.Z Update argocd brew formula git clone https://github.com/argoproj/homebrew-tap cd homebrew-tap ./update.sh ~/go/src/github.com/argoproj/argo-cd/dist/argocd-darwin-amd64 git commit -a -m \"Update argocd to vX.Y.Z\" git push Update documentation: Edit CHANGELOG.md with release notes Update stable tag git tag stable --force && git push upstream stable --force Create GitHub release from new tag and upload binaries (e.g. dist/argocd-darwin-amd64)","title":"Releasing"},{"location":"developer-guide/site/","text":"Site \u00b6 The web site is build using mkdocs and mkdocs-material . To test: mkdocs serve To deploy mkdocs gh-deploy","title":"Site"},{"location":"developer-guide/site/#site","text":"The web site is build using mkdocs and mkdocs-material . To test: mkdocs serve To deploy mkdocs gh-deploy","title":"Site"},{"location":"developer-guide/test-e2e/","text":"E2E tests \u00b6 The directory contains E2E tests and test applications. Tests assume that Argo CD services are installed into argocd-e2e namespace or cluster in current context. One throw-away namespace argocd-e2e*** is created prior to tests execute. The throw-away namespace is used as a target namespace for test applications. The test/e2e/testdata directory contains various Argo CD applications. Before test execution directory is copies into /tmp/argocd-e2e*** temp directory and used in tests as a Git repository via file url: file:///tmp/argocd-e2e*** . Use the following steps to run tests locally: (Do it once) Create namespace argocd-e2e and apply base manifests: kubectl create ns -n argocd-e2e && kustomize build test/manifests/base | kubectl apply -n argocd-e2e -f - Change kubectl context namespace to argocd-e2e and start services using goreman start Keep Argo CD services running and run tests using make test-e2e The tests are executed by Argo Workflow defined at .argo-ci/ci.yaml . CI job The build argo cd image, deploy argo cd components into throw-away kubernetes cluster provisioned using k3s and run e2e tests against it.","title":"E2E Testing"},{"location":"developer-guide/test-e2e/#e2e-tests","text":"The directory contains E2E tests and test applications. Tests assume that Argo CD services are installed into argocd-e2e namespace or cluster in current context. One throw-away namespace argocd-e2e*** is created prior to tests execute. The throw-away namespace is used as a target namespace for test applications. The test/e2e/testdata directory contains various Argo CD applications. Before test execution directory is copies into /tmp/argocd-e2e*** temp directory and used in tests as a Git repository via file url: file:///tmp/argocd-e2e*** . Use the following steps to run tests locally: (Do it once) Create namespace argocd-e2e and apply base manifests: kubectl create ns -n argocd-e2e && kustomize build test/manifests/base | kubectl apply -n argocd-e2e -f - Change kubectl context namespace to argocd-e2e and start services using goreman start Keep Argo CD services running and run tests using make test-e2e The tests are executed by Argo Workflow defined at .argo-ci/ci.yaml . CI job The build argo cd image, deploy argo cd components into throw-away kubernetes cluster provisioned using k3s and run e2e tests against it.","title":"E2E tests"},{"location":"operator-manual/architecture/","text":"Architectural Overview \u00b6 Components \u00b6 API Server \u00b6 The API server is a gRPC/REST server which exposes the API consumed by the Web UI, CLI, and CI/CD systems. It has the following responsibilities: application management and status reporting invoking of application operations (e.g. sync, rollback, user-defined actions) repository and cluster credential management (stored as K8s secrets) authentication and auth delegation to external identity providers RBAC enforcement listener/forwarder for Git webhook events Repository Server \u00b6 The repository server is an internal service which maintains a local cache of the Git repository holding the application manifests. It is responsible for generating and returning the Kubernetes manifests when provided the following inputs: repository URL revision (commit, tag, branch) application path template specific settings: parameters, ksonnet environments, helm values.yaml Application Controller \u00b6 The application controller is a Kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the repo). It detects OutOfSync application state and optionally takes corrective action. It is responsible for invoking any user-defined hooks for lifcecycle events (PreSync, Sync, PostSync)","title":"Architectural Overview"},{"location":"operator-manual/architecture/#architectural-overview","text":"","title":"Architectural Overview"},{"location":"operator-manual/architecture/#components","text":"","title":"Components"},{"location":"operator-manual/architecture/#api-server","text":"The API server is a gRPC/REST server which exposes the API consumed by the Web UI, CLI, and CI/CD systems. It has the following responsibilities: application management and status reporting invoking of application operations (e.g. sync, rollback, user-defined actions) repository and cluster credential management (stored as K8s secrets) authentication and auth delegation to external identity providers RBAC enforcement listener/forwarder for Git webhook events","title":"API Server"},{"location":"operator-manual/architecture/#repository-server","text":"The repository server is an internal service which maintains a local cache of the Git repository holding the application manifests. It is responsible for generating and returning the Kubernetes manifests when provided the following inputs: repository URL revision (commit, tag, branch) application path template specific settings: parameters, ksonnet environments, helm values.yaml","title":"Repository Server"},{"location":"operator-manual/architecture/#application-controller","text":"The application controller is a Kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the repo). It detects OutOfSync application state and optionally takes corrective action. It is responsible for invoking any user-defined hooks for lifcecycle events (PreSync, Sync, PostSync)","title":"Application Controller"},{"location":"operator-manual/custom_tools/","text":"Custom Tooling \u00b6 Argo CD bundles preferred versions of its supported templating tools (helm, kustomize, ks, jsonnet) as part of its container images. Sometimes, it may be desired to use a specific version of a tool other than what Argo CD bundles. Some reasons to do this might be: To upgrade/downgrade to a specific version of a tool due to bugs or bug fixes. To install additional dependencies which to be used by kustomize's configmap/secret generators (e.g. curl, vault, gpg, AWS CLI) To install a config management plugin As the Argo CD repo-server is the single service responsible for generating Kubernetes manifests, it can be customized to use alternative toolchain required by your environment. Adding Tools Via Volume Mounts \u00b6 The first technique is to use an init container and a volumeMount to copy a different verison of a tool into the repo-server container. In the following example, an init container is overwriting the helm binary with a different version than what is bundled in Argo CD: spec : # 1. Define an emptyDir volume which will hold the custom binaries volumes : - name : custom-tools emptyDir : {} # 2. Use an init container to download/copy custom binaries into the emptyDir initContainers : - name : download-tools image : alpine:3.8 command : [ sh , -c ] args : - wget -qO- https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz | tar -xvzf - && mv linux-amd64/helm /custom-tools/ volumeMounts : - mountPath : /custom-tools name : custom-tools # 3. Volume mount the custom binary to the bin directory (overriding the existing version) containers : - name : argocd-repo-server volumeMounts : - mountPath : /usr/local/bin/helm name : custom-tools subPath : helm BYOI (Build Your Own Image) \u00b6 Sometimes replacing a binary isn't sufficient and you need to install other dependencies. The following example builds an entirely customized repo-server from a Dockerfile, installing extra dependencies that may be needed for generating manifests. FROM argoproj/argocd:latest # Switch to root for the ability to perform install USER root # Install tools needed for your repo-server to retrieve & decrypt secrets, render manifests # (e.g. curl, awscli, gpg, sops) RUN apt-get update && \\ apt-get install -y \\ curl \\ awscli \\ gpg && \\ apt-get clean && \\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* && \\ curl -o /usr/local/bin/sops -L https://github.com/mozilla/sops/releases/download/3.2.0/sops-3.2.0.linux && \\ chmod +x /usr/local/bin/sops # Switch back to non-root user USER argocd","title":"Custom Tooling"},{"location":"operator-manual/custom_tools/#custom-tooling","text":"Argo CD bundles preferred versions of its supported templating tools (helm, kustomize, ks, jsonnet) as part of its container images. Sometimes, it may be desired to use a specific version of a tool other than what Argo CD bundles. Some reasons to do this might be: To upgrade/downgrade to a specific version of a tool due to bugs or bug fixes. To install additional dependencies which to be used by kustomize's configmap/secret generators (e.g. curl, vault, gpg, AWS CLI) To install a config management plugin As the Argo CD repo-server is the single service responsible for generating Kubernetes manifests, it can be customized to use alternative toolchain required by your environment.","title":"Custom Tooling"},{"location":"operator-manual/custom_tools/#adding-tools-via-volume-mounts","text":"The first technique is to use an init container and a volumeMount to copy a different verison of a tool into the repo-server container. In the following example, an init container is overwriting the helm binary with a different version than what is bundled in Argo CD: spec : # 1. Define an emptyDir volume which will hold the custom binaries volumes : - name : custom-tools emptyDir : {} # 2. Use an init container to download/copy custom binaries into the emptyDir initContainers : - name : download-tools image : alpine:3.8 command : [ sh , -c ] args : - wget -qO- https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz | tar -xvzf - && mv linux-amd64/helm /custom-tools/ volumeMounts : - mountPath : /custom-tools name : custom-tools # 3. Volume mount the custom binary to the bin directory (overriding the existing version) containers : - name : argocd-repo-server volumeMounts : - mountPath : /usr/local/bin/helm name : custom-tools subPath : helm","title":"Adding Tools Via Volume Mounts"},{"location":"operator-manual/custom_tools/#byoi-build-your-own-image","text":"Sometimes replacing a binary isn't sufficient and you need to install other dependencies. The following example builds an entirely customized repo-server from a Dockerfile, installing extra dependencies that may be needed for generating manifests. FROM argoproj/argocd:latest # Switch to root for the ability to perform install USER root # Install tools needed for your repo-server to retrieve & decrypt secrets, render manifests # (e.g. curl, awscli, gpg, sops) RUN apt-get update && \\ apt-get install -y \\ curl \\ awscli \\ gpg && \\ apt-get clean && \\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* && \\ curl -o /usr/local/bin/sops -L https://github.com/mozilla/sops/releases/download/3.2.0/sops-3.2.0.linux && \\ chmod +x /usr/local/bin/sops # Switch back to non-root user USER argocd","title":"BYOI (Build Your Own Image)"},{"location":"operator-manual/declarative-setup/","text":"Declarative Setup \u00b6 Argo CD applications, projects and settings can be defined declaratively using Kubernetes manifests. Quick Reference \u00b6 Name Kind Description argocd-cm.yaml ConfigMap General Argo CD configuration argocd-secret.yaml Secret Password, Certificates, Signing Key argocd-rbac-cm.yaml ConfigMap RBAC Configuration application.yaml Application Example application spec project.yaml AppProject Example project spec Applications \u00b6 The Application CRD is the Kubernetes resource object representing a deployed application instance in an environment. It is defined by two key pieces of information: source reference to the desired state in Git (repository, revision, path, environment) destination reference to the target cluster and namespace. A minimal Application spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook spec : project : default source : repoURL : https://github.com/argoproj/argocd-example-apps.git targetRevision : HEAD path : guestbook destination : server : https://kubernetes.default.svc namespace : guestbook See application.yaml for additional fields Projects \u00b6 The AppProject CRD is the Kubernetes resource object representing a logical grouping of applications. It is defined by the following key pieces of information: sourceRepos reference to the repositories that applications within the project can pull manifests from. destinations reference to clusters and namespaces that applications within the project can deploy into. roles list of entities with definitions of their access to resources within the project. An example spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : my-project spec : description : Example Project # Allow manifests to deploy from any Git repos sourceRepos : - '*' # Only permit applications to deploy to the guestbook namespace in the same cluster destinations : - namespace : guestbook server : https://kubernetes.default.svc # Deny all cluster-scoped resources from being created, except for Namespace clusterResourceWhitelist : - group : '' kind : Namespace # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy namespaceResourceBlacklist : - group : '' kind : ResourceQuota - group : '' kind : LimitRange - group : '' kind : NetworkPolicy roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - my-oidc-group # A role which provides sync privileges to only the guestbook-dev application, e.g. to provide # sync privileges to a CI system - name : ci-role description : Sync privileges for guestbook-dev policies : - p, proj:my-project:ci-role, applications, sync, my-project/guestbook-dev, allow # NOTE: JWT tokens can only be generated by the API server and the token is not persisted # anywhere by Argo CD. It can be prematurely revoked by removing the entry from this list. jwtTokens : - iat : 1535390316 Repositories \u00b6 Repository credentials are stored in secret. Use following steps to configure a repo: Create secret which contains repository credentials. Consider using bitnami-labs/sealed-secrets to store encrypted secret definition as a Kubernetes manifest. Register repository in the argocd-cm config map. Each repository must have url field and, depending on whether you connect using HTTPS or SSH, usernameSecret and passwordSecret (for HTTPS) or sshPrivateKeySecret (for SSH). Example for HTTPS: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm data : repositories : | - url: https://github.com/argoproj/my-private-repository passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username Example for SSH: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm data : repositories : | - url: git@github.com:argoproj/my-private-repository sshPrivateKeySecret: name: my-secret key: sshPrivateKey Clusters \u00b6 Cluster credentials are stored in secrets same as repository credentials but does not require entry in argocd-cm config map. Each secret must have label argocd.argoproj.io/secret-type: cluster . The secret data must include following fields: name - cluster name server - cluster api server url config - JSON representation of following data structure: # Basic authentication settings username : string password : string # Bearer authentication settings bearerToken : string # IAM authentication configuration awsAuthConfig : clusterName : string roleARN : string # Transport layer security configuration settings tlsClientConfig : # PEM-encoded bytes (typically read from a client certificate file). caData : string # PEM-encoded bytes (typically read from a client certificate file). certData : string # Server should be accessed without verifying the TLS certificate insecure : boolean # PEM-encoded bytes (typically read from a client certificate key file). keyData : string # ServerName is passed to the server for SNI and is used in the client to check server # ceritificates against. If ServerName is empty, the hostname used to contact the # server is used. serverName : string Cluster secret example: apiVersion : v1 kind : Secret metadata : name : mycluster-secret labels : argocd.argoproj.io/secret-type : cluster type : Opaque stringData : name : mycluster.com server : https://mycluster.com config : | { \"bearerToken\": \"<authentication token>\", \"tlsClientConfig\": { \"insecure\": false, \"caData\": \"<base64 encoded certificate>\" } } Helm Chart Repositories \u00b6 Non standard Helm Chart repositories have to be registered under the helm.repositories key in the argocd-cm ConfigMap. Each repository must have url and name fields. For private Helm repos you may need to configure access credentials and HTTPS settings using usernameSecret , passwordSecret , caSecret , certSecret and keySecret fields. Example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm data : helm.repositories : | - url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts name: istio.io - url: https://argoproj.github.io/argo-helm name: argo usernameSecret: name: my-secret key: username passwordSecret: name: my-secret key: password caSecret: name: my-secret key: ca certSecret: name: my-secret key: cert keySecret: name: my-secret key: key Resource Exclusion \u00b6 Resources can be excluded from discovery and sync so that ArgoCD is unaware of them. For example, events.k8s.io and metrics.k8s.io are always excluded. Use cases: You have temporal issues and you want to exclude problematic resources. There are many of a kind of resources that impacts ArgoCD's performance. Restrict ArgoCD's access to certain kinds of resources, e.g. secrets. See security.md#cluster-rbac . To configure this, edit the argcd-cm config map: kubectl edit configmap argocd-cm -n argocdconfigmap/argocd-cm edited Add resource.exclusions , e.g.: apiVersion : v1 data : resource.exclusions : | - apiGroups: - \"*\" kinds: - \"*\" clusters: - https://192.168.0.20 kind : ConfigMap The resource.exclusions node is a list of objects. Each object can have: apiGroups A list of globs to match the API group. kinds A list of kinds to match. Can be \"*\" to match all. cluster A list of globs to match the cluster. If all three match, then the resource is ignored. Notes: Quote globs in your YAML to avoid parsing errors. Invalid globs result in the whole rule being ignored. If you add a rule that matches existing resources, these will appear in the interface as OutOfSync . SSO & RBAC \u00b6 SSO configuration details: SSO RBAC configuration details: RBAC Manage Argo CD Using Argo CD \u00b6 Argo CD is able to manage itself since all settings are represented by Kubernetes manifests. The suggested way is to create Kustomize based application which uses base Argo CD manifests from [https://github.com/argoproj/argo-cd] and apply required changes on top. Example of kustomization.yaml : bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.10.6 # additional resources like ingress rules, cluster and repository secrets. resources : - clusters-secrets.yaml - repos-secrets.yaml # changes to config maps patchesStrategicMerge : - overlays/argo-cd-cm.yaml The live example of self managed Argo CD config is available at https://cd.apps.argoproj.io and with configuration stored at argoproj/argoproj-deployments . Note You will need to sign-in using your github account to get access to https://cd.apps.argoproj.io","title":"Declarative Setup"},{"location":"operator-manual/declarative-setup/#declarative-setup","text":"Argo CD applications, projects and settings can be defined declaratively using Kubernetes manifests.","title":"Declarative Setup"},{"location":"operator-manual/declarative-setup/#quick-reference","text":"Name Kind Description argocd-cm.yaml ConfigMap General Argo CD configuration argocd-secret.yaml Secret Password, Certificates, Signing Key argocd-rbac-cm.yaml ConfigMap RBAC Configuration application.yaml Application Example application spec project.yaml AppProject Example project spec","title":"Quick Reference"},{"location":"operator-manual/declarative-setup/#applications","text":"The Application CRD is the Kubernetes resource object representing a deployed application instance in an environment. It is defined by two key pieces of information: source reference to the desired state in Git (repository, revision, path, environment) destination reference to the target cluster and namespace. A minimal Application spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook spec : project : default source : repoURL : https://github.com/argoproj/argocd-example-apps.git targetRevision : HEAD path : guestbook destination : server : https://kubernetes.default.svc namespace : guestbook See application.yaml for additional fields","title":"Applications"},{"location":"operator-manual/declarative-setup/#projects","text":"The AppProject CRD is the Kubernetes resource object representing a logical grouping of applications. It is defined by the following key pieces of information: sourceRepos reference to the repositories that applications within the project can pull manifests from. destinations reference to clusters and namespaces that applications within the project can deploy into. roles list of entities with definitions of their access to resources within the project. An example spec is as follows: apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : my-project spec : description : Example Project # Allow manifests to deploy from any Git repos sourceRepos : - '*' # Only permit applications to deploy to the guestbook namespace in the same cluster destinations : - namespace : guestbook server : https://kubernetes.default.svc # Deny all cluster-scoped resources from being created, except for Namespace clusterResourceWhitelist : - group : '' kind : Namespace # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy namespaceResourceBlacklist : - group : '' kind : ResourceQuota - group : '' kind : LimitRange - group : '' kind : NetworkPolicy roles : # A role which provides read-only access to all applications in the project - name : read-only description : Read-only privileges to my-project policies : - p, proj:my-project:read-only, applications, get, my-project/*, allow groups : - my-oidc-group # A role which provides sync privileges to only the guestbook-dev application, e.g. to provide # sync privileges to a CI system - name : ci-role description : Sync privileges for guestbook-dev policies : - p, proj:my-project:ci-role, applications, sync, my-project/guestbook-dev, allow # NOTE: JWT tokens can only be generated by the API server and the token is not persisted # anywhere by Argo CD. It can be prematurely revoked by removing the entry from this list. jwtTokens : - iat : 1535390316","title":"Projects"},{"location":"operator-manual/declarative-setup/#repositories","text":"Repository credentials are stored in secret. Use following steps to configure a repo: Create secret which contains repository credentials. Consider using bitnami-labs/sealed-secrets to store encrypted secret definition as a Kubernetes manifest. Register repository in the argocd-cm config map. Each repository must have url field and, depending on whether you connect using HTTPS or SSH, usernameSecret and passwordSecret (for HTTPS) or sshPrivateKeySecret (for SSH). Example for HTTPS: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm data : repositories : | - url: https://github.com/argoproj/my-private-repository passwordSecret: name: my-secret key: password usernameSecret: name: my-secret key: username Example for SSH: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm data : repositories : | - url: git@github.com:argoproj/my-private-repository sshPrivateKeySecret: name: my-secret key: sshPrivateKey","title":"Repositories"},{"location":"operator-manual/declarative-setup/#clusters","text":"Cluster credentials are stored in secrets same as repository credentials but does not require entry in argocd-cm config map. Each secret must have label argocd.argoproj.io/secret-type: cluster . The secret data must include following fields: name - cluster name server - cluster api server url config - JSON representation of following data structure: # Basic authentication settings username : string password : string # Bearer authentication settings bearerToken : string # IAM authentication configuration awsAuthConfig : clusterName : string roleARN : string # Transport layer security configuration settings tlsClientConfig : # PEM-encoded bytes (typically read from a client certificate file). caData : string # PEM-encoded bytes (typically read from a client certificate file). certData : string # Server should be accessed without verifying the TLS certificate insecure : boolean # PEM-encoded bytes (typically read from a client certificate key file). keyData : string # ServerName is passed to the server for SNI and is used in the client to check server # ceritificates against. If ServerName is empty, the hostname used to contact the # server is used. serverName : string Cluster secret example: apiVersion : v1 kind : Secret metadata : name : mycluster-secret labels : argocd.argoproj.io/secret-type : cluster type : Opaque stringData : name : mycluster.com server : https://mycluster.com config : | { \"bearerToken\": \"<authentication token>\", \"tlsClientConfig\": { \"insecure\": false, \"caData\": \"<base64 encoded certificate>\" } }","title":"Clusters"},{"location":"operator-manual/declarative-setup/#helm-chart-repositories","text":"Non standard Helm Chart repositories have to be registered under the helm.repositories key in the argocd-cm ConfigMap. Each repository must have url and name fields. For private Helm repos you may need to configure access credentials and HTTPS settings using usernameSecret , passwordSecret , caSecret , certSecret and keySecret fields. Example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm data : helm.repositories : | - url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts name: istio.io - url: https://argoproj.github.io/argo-helm name: argo usernameSecret: name: my-secret key: username passwordSecret: name: my-secret key: password caSecret: name: my-secret key: ca certSecret: name: my-secret key: cert keySecret: name: my-secret key: key","title":"Helm Chart Repositories"},{"location":"operator-manual/declarative-setup/#resource-exclusion","text":"Resources can be excluded from discovery and sync so that ArgoCD is unaware of them. For example, events.k8s.io and metrics.k8s.io are always excluded. Use cases: You have temporal issues and you want to exclude problematic resources. There are many of a kind of resources that impacts ArgoCD's performance. Restrict ArgoCD's access to certain kinds of resources, e.g. secrets. See security.md#cluster-rbac . To configure this, edit the argcd-cm config map: kubectl edit configmap argocd-cm -n argocdconfigmap/argocd-cm edited Add resource.exclusions , e.g.: apiVersion : v1 data : resource.exclusions : | - apiGroups: - \"*\" kinds: - \"*\" clusters: - https://192.168.0.20 kind : ConfigMap The resource.exclusions node is a list of objects. Each object can have: apiGroups A list of globs to match the API group. kinds A list of kinds to match. Can be \"*\" to match all. cluster A list of globs to match the cluster. If all three match, then the resource is ignored. Notes: Quote globs in your YAML to avoid parsing errors. Invalid globs result in the whole rule being ignored. If you add a rule that matches existing resources, these will appear in the interface as OutOfSync .","title":"Resource Exclusion"},{"location":"operator-manual/declarative-setup/#sso-rbac","text":"SSO configuration details: SSO RBAC configuration details: RBAC","title":"SSO &amp; RBAC"},{"location":"operator-manual/declarative-setup/#manage-argo-cd-using-argo-cd","text":"Argo CD is able to manage itself since all settings are represented by Kubernetes manifests. The suggested way is to create Kustomize based application which uses base Argo CD manifests from [https://github.com/argoproj/argo-cd] and apply required changes on top. Example of kustomization.yaml : bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.10.6 # additional resources like ingress rules, cluster and repository secrets. resources : - clusters-secrets.yaml - repos-secrets.yaml # changes to config maps patchesStrategicMerge : - overlays/argo-cd-cm.yaml The live example of self managed Argo CD config is available at https://cd.apps.argoproj.io and with configuration stored at argoproj/argoproj-deployments . Note You will need to sign-in using your github account to get access to https://cd.apps.argoproj.io","title":"Manage Argo CD Using Argo CD"},{"location":"operator-manual/ingress/","text":"Ingress Configuration \u00b6 Argo CD runs both a gRPC server (used by the CLI), as well as a HTTP/HTTPS server (used by the UI). Both protocols are exposed by the argocd-server service object on the following ports: 443 - gRPC/HTTPS 80 - HTTP (redirects to HTTPS) There are several ways how Ingress can be configured. kubernetes/ingress-nginx \u00b6 Option 1: SSL-Passthrough \u00b6 Because Argo CD serves multiple protocols (gRPC/HTTPS) on the same port (443), this provides a challenge when attempting to define a single nginx ingress object and rule for the argocd-service, since the nginx.ingress.kubernetes.io/backend-protocol annotation accepts only a single value for the backend protocol (e.g. HTTP, HTTPS, GRPC, GRPCS). In order to expose the Argo CD API server with a single ingress rule and hostname, the nginx.ingress.kubernetes.io/ssl-passthrough annotation must be used to passthrough TLS connections and terminate TLS at the Argo CD API server. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-ingress annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/ssl-passthrough : \"true\" spec : rules : - host : argocd.example.com http : paths : - backend : serviceName : argocd-server servicePort : https The above rule terminates TLS at the Argo CD API server, which detects the protocol being used, and responds appropriately. Note that the nginx.ingress.kubernetes.io/ssl-passthrough annotation requires that the --enable-ssl-passthrough flag be added to the command line arguments to nginx-ingress-controller . Option 2: Multiple Ingress Objects And Hosts \u00b6 Since ingress-nginx Ingress supports only a single protocol per Ingress object, an alternative way would be to define two Ingress objects. One for HTTP/HTTPS, and the other for gRPC: HTTP/HTTPS Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-http-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTP\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : http host : argocd.example.com tls : - hosts : - argocd.example.com secretName : argocd-secret gRPC Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-grpc-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/backend-protocol : \"GRPC\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : https host : grpc.argocd.example.com tls : - hosts : - grpc.argocd.example.com secretName : argocd-secret The API server should then be run with TLS disabled. Edit the argocd-server deployment to add the --insecure flag to the argocd-server command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --insecure The obvious disadvantage to this approach is that this technique require two separate hostnames for the API server -- one for gRPC and the other for HTTP/HTTPS. However it allow TLS termination to happen at the ingress controller. AWS Application Load Balancers (ALBs) And Classic ELB (HTTP Mode) \u00b6 Neither ALBs and Classic ELB in HTTP mode, do not have full support for HTTP2/gRPC which is the protocol used by the argocd CLI. Thus, when using an AWS load balancer, either Classic ELB in passthrough mode is needed, or NLBs. UI Base Path \u00b6 If Argo CD UI is available under non-root path (e.g. /argo-cd instead of / ) then UI path should be configured in API server. To configure UI path add --basehref flag into argocd-server deployment command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --basehref - /argo-cd NOTE: flag --basehref only changes UI base URL. API server keep using / path so you need to add URL rewrite rule to proxy config. Example nginx.conf with URL rewrite: worker_processes 1 ; events { worker_connections 1024 ; } http { sendfile on ; server { listen 443 ; location /argo-cd { rewrite /argo-cd/(.*) /$1 break ; proxy_pass https : // localhost : 8080 ; proxy_redirect off ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Host $server_name ; } } }","title":"Ingress Configuration"},{"location":"operator-manual/ingress/#ingress-configuration","text":"Argo CD runs both a gRPC server (used by the CLI), as well as a HTTP/HTTPS server (used by the UI). Both protocols are exposed by the argocd-server service object on the following ports: 443 - gRPC/HTTPS 80 - HTTP (redirects to HTTPS) There are several ways how Ingress can be configured.","title":"Ingress Configuration"},{"location":"operator-manual/ingress/#kubernetesingress-nginx","text":"","title":"kubernetes/ingress-nginx"},{"location":"operator-manual/ingress/#option-1-ssl-passthrough","text":"Because Argo CD serves multiple protocols (gRPC/HTTPS) on the same port (443), this provides a challenge when attempting to define a single nginx ingress object and rule for the argocd-service, since the nginx.ingress.kubernetes.io/backend-protocol annotation accepts only a single value for the backend protocol (e.g. HTTP, HTTPS, GRPC, GRPCS). In order to expose the Argo CD API server with a single ingress rule and hostname, the nginx.ingress.kubernetes.io/ssl-passthrough annotation must be used to passthrough TLS connections and terminate TLS at the Argo CD API server. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-ingress annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/ssl-passthrough : \"true\" spec : rules : - host : argocd.example.com http : paths : - backend : serviceName : argocd-server servicePort : https The above rule terminates TLS at the Argo CD API server, which detects the protocol being used, and responds appropriately. Note that the nginx.ingress.kubernetes.io/ssl-passthrough annotation requires that the --enable-ssl-passthrough flag be added to the command line arguments to nginx-ingress-controller .","title":"Option 1: SSL-Passthrough"},{"location":"operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts","text":"Since ingress-nginx Ingress supports only a single protocol per Ingress object, an alternative way would be to define two Ingress objects. One for HTTP/HTTPS, and the other for gRPC: HTTP/HTTPS Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-http-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/force-ssl-redirect : \"true\" nginx.ingress.kubernetes.io/backend-protocol : \"HTTP\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : http host : argocd.example.com tls : - hosts : - argocd.example.com secretName : argocd-secret gRPC Ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : argocd-server-grpc-ingress annotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/backend-protocol : \"GRPC\" spec : rules : - http : paths : - backend : serviceName : argocd-server servicePort : https host : grpc.argocd.example.com tls : - hosts : - grpc.argocd.example.com secretName : argocd-secret The API server should then be run with TLS disabled. Edit the argocd-server deployment to add the --insecure flag to the argocd-server command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --insecure The obvious disadvantage to this approach is that this technique require two separate hostnames for the API server -- one for gRPC and the other for HTTP/HTTPS. However it allow TLS termination to happen at the ingress controller.","title":"Option 2: Multiple Ingress Objects And Hosts"},{"location":"operator-manual/ingress/#aws-application-load-balancers-albs-and-classic-elb-http-mode","text":"Neither ALBs and Classic ELB in HTTP mode, do not have full support for HTTP2/gRPC which is the protocol used by the argocd CLI. Thus, when using an AWS load balancer, either Classic ELB in passthrough mode is needed, or NLBs.","title":"AWS Application Load Balancers (ALBs) And Classic ELB (HTTP Mode)"},{"location":"operator-manual/ingress/#ui-base-path","text":"If Argo CD UI is available under non-root path (e.g. /argo-cd instead of / ) then UI path should be configured in API server. To configure UI path add --basehref flag into argocd-server deployment command: spec : template : spec : name : argocd-server containers : - command : - /argocd-server - --staticassets - /shared/app - --repo-server - argocd-repo-server:8081 - --basehref - /argo-cd NOTE: flag --basehref only changes UI base URL. API server keep using / path so you need to add URL rewrite rule to proxy config. Example nginx.conf with URL rewrite: worker_processes 1 ; events { worker_connections 1024 ; } http { sendfile on ; server { listen 443 ; location /argo-cd { rewrite /argo-cd/(.*) /$1 break ; proxy_pass https : // localhost : 8080 ; proxy_redirect off ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Host $server_name ; } } }","title":"UI Base Path"},{"location":"operator-manual/metrics/","text":"Prometheus Metrics \u00b6 Argo CD exposes two sets of Prometheus metrics Application Metrics \u00b6 Metrics about applications. Scraped at the argocd-metrics:8082/metrics endpoint. Gauge for application health status Gauge for application sync status Counter for application sync history API Server Metrics \u00b6 Metrics about API Server API request and response activity (request totals, response codes, etc...). Scraped at the argocd-server-metrics:8083/metrics endpoint. Prometheus Operator \u00b6 If using Prometheus Operator, the following ServiceMonitor example manifests can be used. Change metadata.labels.release to the name of label selected by your Prometheus. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-server-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-repo-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-repo-server endpoints : - port : metrics Dashboards \u00b6 You can find an example Grafana dashboard here","title":"Prometheus Metrics"},{"location":"operator-manual/metrics/#prometheus-metrics","text":"Argo CD exposes two sets of Prometheus metrics","title":"Prometheus Metrics"},{"location":"operator-manual/metrics/#application-metrics","text":"Metrics about applications. Scraped at the argocd-metrics:8082/metrics endpoint. Gauge for application health status Gauge for application sync status Counter for application sync history","title":"Application Metrics"},{"location":"operator-manual/metrics/#api-server-metrics","text":"Metrics about API Server API request and response activity (request totals, response codes, etc...). Scraped at the argocd-server-metrics:8083/metrics endpoint.","title":"API Server Metrics"},{"location":"operator-manual/metrics/#prometheus-operator","text":"If using Prometheus Operator, the following ServiceMonitor example manifests can be used. Change metadata.labels.release to the name of label selected by your Prometheus. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-server-metrics endpoints : - port : metrics apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-repo-server-metrics labels : release : prometheus-operator spec : selector : matchLabels : app.kubernetes.io/name : argocd-repo-server endpoints : - port : metrics","title":"Prometheus Operator"},{"location":"operator-manual/metrics/#dashboards","text":"You can find an example Grafana dashboard here","title":"Dashboards"},{"location":"operator-manual/rbac/","text":"RBAC \u00b6 Overview \u00b6 The RBAC feature enables restriction of access to Argo CD resources. Argo CD does not have its own user management system and has only one built-in user admin . The admin user is a superuser and it has unrestricted access to the system. RBAC requires SSO configuration . Once SSO is configured, additional RBAC roles can be defined, and SSO groups can man be mapped to roles. Configure RBAC \u00b6 RBAC configuration allows defining roles and groups. Argo CD has two pre-defined roles: role:readonly - read-only access to all resources role:admin - unrestricted access to all resources These role definitions can be seen in builtin-policy.csv Additional roles and groups can be configured in argocd-rbac-cm ConfigMap. The example below configures a custom role, named org-admin . The role is assigned to any user which belongs to your-github-org:your-team group. All other users get the default policy of role:readonly , which cannot modify Argo CD settings. ConfigMap argocd-rbac-cm example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm data : policy.default : role:readonly policy.csv : | p, role:org-admin, applications, *, */*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, your-github-org:your-team, role:org-admin","title":"RBAC"},{"location":"operator-manual/rbac/#rbac","text":"","title":"RBAC"},{"location":"operator-manual/rbac/#overview","text":"The RBAC feature enables restriction of access to Argo CD resources. Argo CD does not have its own user management system and has only one built-in user admin . The admin user is a superuser and it has unrestricted access to the system. RBAC requires SSO configuration . Once SSO is configured, additional RBAC roles can be defined, and SSO groups can man be mapped to roles.","title":"Overview"},{"location":"operator-manual/rbac/#configure-rbac","text":"RBAC configuration allows defining roles and groups. Argo CD has two pre-defined roles: role:readonly - read-only access to all resources role:admin - unrestricted access to all resources These role definitions can be seen in builtin-policy.csv Additional roles and groups can be configured in argocd-rbac-cm ConfigMap. The example below configures a custom role, named org-admin . The role is assigned to any user which belongs to your-github-org:your-team group. All other users get the default policy of role:readonly , which cannot modify Argo CD settings. ConfigMap argocd-rbac-cm example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm data : policy.default : role:readonly policy.csv : | p, role:org-admin, applications, *, */*, allow p, role:org-admin, clusters, get, *, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, your-github-org:your-team, role:org-admin","title":"Configure RBAC"},{"location":"operator-manual/security/","text":"Security \u00b6 Argo CD has undergone rigourous internal security reviews and penetration testing to satisfy PCI compliance requirements. The following are some security topics and implementation details of Argo CD. Authentication \u00b6 Authentication to Argo CD API server is performed exclusively using JSON Web Tokens (JWTs). Username/password bearer tokens are not used for authentication. The JWT is obtained/managed in one of the following ways: For the local admin user, a username/password is exchanged for a JWT using the /api/v1/session endpoint. This token is signed & issued by the Argo CD API server itself, and has no expiration. When the admin password is updated, all existing admin JWT tokens are immediately revoked. The password is stored as a bcrypt hash in the argocd-secret Secret. For Single Sign-On users, the user completes an OAuth2 login flow to the configured OIDC identity provider (either delegated through the bundled Dex provider, or directly to a self-managed OIDC provider). This JWT is signed & issued by the IDP, and expiration and revokation is handled by the provider. Dex tokens expire after 24 hours. Automation tokens are generated for a project using the /api/v1/projects/{project}/roles/{role}/token endpoint, and are signed & issued by Argo CD. These tokens are limited in scope and privilege, and can only be used to manage application resources in the project which it belongs to. Project JWTs have a configurable expiration and can be immediately revoked by deleting the JWT reference ID from the project role. Authorization \u00b6 Authorization is performed by iterating the list of group membership in a user's JWT groups claims, and comparing each group against the roles/rules in the RBAC policy. Any matched rule permits access to the API request. TLS \u00b6 All network communication is performed over TLS including service-to-service communication between the three components (argocd-server, argocd-repo-server, argocd-application-controller). The Argo CD API server can enforce the use of TLS 1.2 using the flag: --tlsminversion 1.2 . Sensitive Information \u00b6 Secrets \u00b6 Argo CD never returns sensitive data from its API, and redacts all sensitive data in API payloads and logs. This includes: cluster credentials Git credentials OAuth2 client secrets Kubernetes Secret values External Cluster Credentials \u00b6 To manage external clusters, Argo CD stores the credentials of the external cluster as a Kubernetes Secret in the argocd namespace. This secret contains the K8s API bearer token associated with the argocd-manager ServiceAccount created during argocd cluster add , along with connection options to that API server (TLS configuration/certs, aws-iam-authenticator RoleARN, etc...). The information is used to reconstruct a REST config and kubeconfig to the cluster used by Argo CD services. To rotate the bearer token used by Argo CD, the token can be deleted (e.g. using kubectl) which causes kuberentes to generate a new secret with a new bearer token. The new token can be re-inputted to Argo CD by re-running argocd cluster add . Run the following commands against the managed cluster: # run using a kubeconfig for the externally managed cluster kubectl delete secret argocd-manager-token-XXXXXX -n kube-system argocd cluster add CONTEXTNAME To revoke Argo CD's access to a managed cluster, delete the RBAC artifacts against the managed cluster, and remove the cluster entry from Argo CD: # run using a kubeconfig for the externally managed cluster kubectl delete sa argocd-manager -n kube-system kubectl delete clusterrole argocd-manager-role kubectl delete clusterrolebinding argocd-manager-role-binding argocd cluster rm https://your-kubernetes-cluster-addr NOTE: for AWS EKS clusters, aws-iam-authenticator is used to authenticate to the external cluster, which uses IAM roles in lieu of locally stored tokens, so token rotation is not needed, and revokation is handled through IAM. Cluster RBAC \u00b6 By default, Argo CD uses a clusteradmin level role in order to: 1. watch & operate on cluster state 2. deploy resources to the cluster Although Argo CD requires cluster-wide read privileges to resources in the managed cluster to function properly, it does not necessarily need full write privileges to the cluster. The ClusterRole used by argocd-server and argocd-application-controller can be modified such that write privileges are limited to only the namespaces and resources that you wish Argo CD to manage. To fine-tune privileges of externally managed clusters, edit the ClusterRole of the argocd-manager-role # run using a kubeconfig for the externally managed cluster kubectl edit clusterrole argocd-manager-role To fine-tune privileges which Argo CD has against its own cluster (i.e. https://kubernetes.default.svc), edit the following cluster roles where Argo CD is running in: # run using a kubeconfig to the cluster Argo CD is running in kubectl edit clusterrole argocd-server kubectl edit clusterrole argocd-application-controller Note If you to deny ArgoCD access to a kind of resource then add it as an excluded resource . Auditing \u00b6 As a GitOps deployment tool, the Git commit history provides a natural audit log of what changes were made to application configuration, when they were made, and by whom. However, this audit log only applies to what happened in Git and does not necessarily correlate one-to-one with events that happen in a cluster. For example, User A could have made multiple commits to application manifests, but User B could have just only synced those changes to the cluster sometime later. To complement the Git revision history, Argo CD emits Kubernetes Events of application activity, indicating the responsible actor when applicable. For example: $ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 1m 1m 1 guestbook.157f7c5edd33aeac Application Normal ResourceCreated argocd-server admin created application 1m 1m 1 guestbook.157f7c5f0f747acf Application Normal ResourceUpdated argocd-application-controller Updated sync status: -> OutOfSync 1m 1m 1 guestbook.157f7c5f0fbebbff Application Normal ResourceUpdated argocd-application-controller Updated health status: -> Missing 1m 1m 1 guestbook.157f7c6069e14f4d Application Normal OperationStarted argocd-server admin initiated sync to HEAD ( 8a1cb4a02d3538e54907c827352f66f20c3d7b0d ) 1m 1m 1 guestbook.157f7c60a55a81a8 Application Normal OperationCompleted argocd-application-controller Sync operation to 8a1cb4a02d3538e54907c827352f66f20c3d7b0d succeeded 1m 1m 1 guestbook.157f7c60af1ccae2 Application Normal ResourceUpdated argocd-application-controller Updated sync status: OutOfSync -> Synced 1m 1m 1 guestbook.157f7c60af5bc4f0 Application Normal ResourceUpdated argocd-application-controller Updated health status: Missing -> Progressing 1m 1m 1 guestbook.157f7c651990e848 Application Normal ResourceUpdated argocd-application-controller Updated health status: Progressing -> Healthy These events can be then be persisted for longer periods of time using other tools as Event Exporter or Event Router . WebHook Payloads \u00b6 Payloads from webhook events are considered untrusted. Argo CD only examines the payload to infer the involved applications of the webhook event (e.g. which repo was modified), then refreshes the related application for reconciliation. This refresh is the same refresh which occurs regularly at three minute intervals, just fast-tracked by the webhook event. Reporting Vulnerabilities \u00b6 Please report security vulnerabilities by e-mailing: Jesse_Suen@intuit.com Alexander_Matyushentsev@intuit.com Edward_Lee@intuit.com","title":"Security"},{"location":"operator-manual/security/#security","text":"Argo CD has undergone rigourous internal security reviews and penetration testing to satisfy PCI compliance requirements. The following are some security topics and implementation details of Argo CD.","title":"Security"},{"location":"operator-manual/security/#authentication","text":"Authentication to Argo CD API server is performed exclusively using JSON Web Tokens (JWTs). Username/password bearer tokens are not used for authentication. The JWT is obtained/managed in one of the following ways: For the local admin user, a username/password is exchanged for a JWT using the /api/v1/session endpoint. This token is signed & issued by the Argo CD API server itself, and has no expiration. When the admin password is updated, all existing admin JWT tokens are immediately revoked. The password is stored as a bcrypt hash in the argocd-secret Secret. For Single Sign-On users, the user completes an OAuth2 login flow to the configured OIDC identity provider (either delegated through the bundled Dex provider, or directly to a self-managed OIDC provider). This JWT is signed & issued by the IDP, and expiration and revokation is handled by the provider. Dex tokens expire after 24 hours. Automation tokens are generated for a project using the /api/v1/projects/{project}/roles/{role}/token endpoint, and are signed & issued by Argo CD. These tokens are limited in scope and privilege, and can only be used to manage application resources in the project which it belongs to. Project JWTs have a configurable expiration and can be immediately revoked by deleting the JWT reference ID from the project role.","title":"Authentication"},{"location":"operator-manual/security/#authorization","text":"Authorization is performed by iterating the list of group membership in a user's JWT groups claims, and comparing each group against the roles/rules in the RBAC policy. Any matched rule permits access to the API request.","title":"Authorization"},{"location":"operator-manual/security/#tls","text":"All network communication is performed over TLS including service-to-service communication between the three components (argocd-server, argocd-repo-server, argocd-application-controller). The Argo CD API server can enforce the use of TLS 1.2 using the flag: --tlsminversion 1.2 .","title":"TLS"},{"location":"operator-manual/security/#sensitive-information","text":"","title":"Sensitive Information"},{"location":"operator-manual/security/#secrets","text":"Argo CD never returns sensitive data from its API, and redacts all sensitive data in API payloads and logs. This includes: cluster credentials Git credentials OAuth2 client secrets Kubernetes Secret values","title":"Secrets"},{"location":"operator-manual/security/#external-cluster-credentials","text":"To manage external clusters, Argo CD stores the credentials of the external cluster as a Kubernetes Secret in the argocd namespace. This secret contains the K8s API bearer token associated with the argocd-manager ServiceAccount created during argocd cluster add , along with connection options to that API server (TLS configuration/certs, aws-iam-authenticator RoleARN, etc...). The information is used to reconstruct a REST config and kubeconfig to the cluster used by Argo CD services. To rotate the bearer token used by Argo CD, the token can be deleted (e.g. using kubectl) which causes kuberentes to generate a new secret with a new bearer token. The new token can be re-inputted to Argo CD by re-running argocd cluster add . Run the following commands against the managed cluster: # run using a kubeconfig for the externally managed cluster kubectl delete secret argocd-manager-token-XXXXXX -n kube-system argocd cluster add CONTEXTNAME To revoke Argo CD's access to a managed cluster, delete the RBAC artifacts against the managed cluster, and remove the cluster entry from Argo CD: # run using a kubeconfig for the externally managed cluster kubectl delete sa argocd-manager -n kube-system kubectl delete clusterrole argocd-manager-role kubectl delete clusterrolebinding argocd-manager-role-binding argocd cluster rm https://your-kubernetes-cluster-addr NOTE: for AWS EKS clusters, aws-iam-authenticator is used to authenticate to the external cluster, which uses IAM roles in lieu of locally stored tokens, so token rotation is not needed, and revokation is handled through IAM.","title":"External Cluster Credentials"},{"location":"operator-manual/security/#cluster-rbac","text":"By default, Argo CD uses a clusteradmin level role in order to: 1. watch & operate on cluster state 2. deploy resources to the cluster Although Argo CD requires cluster-wide read privileges to resources in the managed cluster to function properly, it does not necessarily need full write privileges to the cluster. The ClusterRole used by argocd-server and argocd-application-controller can be modified such that write privileges are limited to only the namespaces and resources that you wish Argo CD to manage. To fine-tune privileges of externally managed clusters, edit the ClusterRole of the argocd-manager-role # run using a kubeconfig for the externally managed cluster kubectl edit clusterrole argocd-manager-role To fine-tune privileges which Argo CD has against its own cluster (i.e. https://kubernetes.default.svc), edit the following cluster roles where Argo CD is running in: # run using a kubeconfig to the cluster Argo CD is running in kubectl edit clusterrole argocd-server kubectl edit clusterrole argocd-application-controller Note If you to deny ArgoCD access to a kind of resource then add it as an excluded resource .","title":"Cluster RBAC"},{"location":"operator-manual/security/#auditing","text":"As a GitOps deployment tool, the Git commit history provides a natural audit log of what changes were made to application configuration, when they were made, and by whom. However, this audit log only applies to what happened in Git and does not necessarily correlate one-to-one with events that happen in a cluster. For example, User A could have made multiple commits to application manifests, but User B could have just only synced those changes to the cluster sometime later. To complement the Git revision history, Argo CD emits Kubernetes Events of application activity, indicating the responsible actor when applicable. For example: $ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 1m 1m 1 guestbook.157f7c5edd33aeac Application Normal ResourceCreated argocd-server admin created application 1m 1m 1 guestbook.157f7c5f0f747acf Application Normal ResourceUpdated argocd-application-controller Updated sync status: -> OutOfSync 1m 1m 1 guestbook.157f7c5f0fbebbff Application Normal ResourceUpdated argocd-application-controller Updated health status: -> Missing 1m 1m 1 guestbook.157f7c6069e14f4d Application Normal OperationStarted argocd-server admin initiated sync to HEAD ( 8a1cb4a02d3538e54907c827352f66f20c3d7b0d ) 1m 1m 1 guestbook.157f7c60a55a81a8 Application Normal OperationCompleted argocd-application-controller Sync operation to 8a1cb4a02d3538e54907c827352f66f20c3d7b0d succeeded 1m 1m 1 guestbook.157f7c60af1ccae2 Application Normal ResourceUpdated argocd-application-controller Updated sync status: OutOfSync -> Synced 1m 1m 1 guestbook.157f7c60af5bc4f0 Application Normal ResourceUpdated argocd-application-controller Updated health status: Missing -> Progressing 1m 1m 1 guestbook.157f7c651990e848 Application Normal ResourceUpdated argocd-application-controller Updated health status: Progressing -> Healthy These events can be then be persisted for longer periods of time using other tools as Event Exporter or Event Router .","title":"Auditing"},{"location":"operator-manual/security/#webhook-payloads","text":"Payloads from webhook events are considered untrusted. Argo CD only examines the payload to infer the involved applications of the webhook event (e.g. which repo was modified), then refreshes the related application for reconciliation. This refresh is the same refresh which occurs regularly at three minute intervals, just fast-tracked by the webhook event.","title":"WebHook Payloads"},{"location":"operator-manual/security/#reporting-vulnerabilities","text":"Please report security vulnerabilities by e-mailing: Jesse_Suen@intuit.com Alexander_Matyushentsev@intuit.com Edward_Lee@intuit.com","title":"Reporting Vulnerabilities"},{"location":"operator-manual/sso/","text":"SSO Configuration \u00b6 Overview \u00b6 Argo CD does not have any local users other than the built-in admin user. All other users are expected to login via SSO. There are two ways that SSO can be configured: Bundled Dex OIDC provider - use this option your current provider does not support OIDC (e.g. SAML, LDAP) or if you wish to leverage any of Dex's connector features (e.g. the ability to map GitHub organizations and teams to OIDC groups claims). Existing OIDC provider - use this if you already have an OIDC provider which you are using (e.g. Okta, OneLogin, Auth0, Microsoft), where you manage your users, groups, and memberships. Dex \u00b6 Argo CD embeds and bundles Dex as part of its installation, for the purpose of delegating authentication to an external identity provider. Multiple types of identity providers are supported (OIDC, SAML, LDAP, GitHub, etc...). SSO configuration of Argo CD requires editing the argocd-cm ConfigMap with Dex connector settings. This document describes how to configure Argo CD SSO using GitHub (OAuth2) as an example, but the steps should be similar for other identity providers. 1. Register the application in the identity provider \u00b6 In GitHub, register a new application. The callback address should be the /api/dex/callback endpoint of your Argo CD URL (e.g. https://argocd.example.com/api/dex/callback). After registering the app, you will receive an OAuth2 client ID and secret. These values will be inputted into the Argo CD configmap. 2. Configure Argo CD for SSO \u00b6 Edit the argocd-cm configmap: kubectl edit configmap argocd-cm -n argocd In the url key, input the base URL of Argo CD. In this example, it is https://argocd.example.com In the dex.config key, add the github connector to the connectors sub field. See Dex's GitHub connector documentation for explanation of the fields. A minimal config should populate the clientID, clientSecret generated in Step 1. You will very likely want to restrict logins to one or more GitHub organization. In the connectors.config.orgs list, add one or more GitHub organizations. Any member of the org will then be able to login to Argo CD to perform management tasks. data : url : https://argocd.example.com dex.config : | connectors: # GitHub example - type: github id: github name: GitHub config: clientID: aabbccddeeff00112233 clientSecret: $dex.github.clientSecret orgs: - name: your-github-org # GitHub enterprise example - type: github id: acme-github name: Acme GitHub config: hostName: github.acme.com clientID: abcdefghijklmnopqrst clientSecret: $dex.acme.clientSecret orgs: - name: your-github-org After saving, the changes should take affect automatically. NOTES: Any values which start with '$' will look to a key in argocd-secret of the same name (minus the $), to obtain the actual value. This allows you to store the clientSecret as a kubernetes secret. There is no need to set redirectURI in the connectors.config as shown in the dex documentation. Argo CD will automatically use the correct redirectURI for any OAuth2 connectors, to match the correct external callback URL (e.g. https://argocd.example.com/api/dex/callback) Existing OIDC Provider \u00b6 To configure Argo CD to delegate authenticate to your existing OIDC provider, add the OAuth2 configuration to the argocd-cm ConfigMap under the oidc.config key: data : url : https://argocd.example.com oidc.config : | name: Okta issuer: https://dev-123456.oktapreview.com clientID: aaaabbbbccccddddeee clientSecret: $oidc.okta.clientSecret # Some OIDC providers require a separate clientID for different callback URLs. # For example, if configuring Argo CD with self-hosted Dex, you will need a separate client ID # for the 'localhost' (CLI) client to Dex. This field is optional. If omitted, the CLI will # use the same clientID as the Argo CD server cliClientID: vvvvwwwwxxxxyyyyzzzz","title":"SSO Configuration"},{"location":"operator-manual/sso/#sso-configuration","text":"","title":"SSO Configuration"},{"location":"operator-manual/sso/#overview","text":"Argo CD does not have any local users other than the built-in admin user. All other users are expected to login via SSO. There are two ways that SSO can be configured: Bundled Dex OIDC provider - use this option your current provider does not support OIDC (e.g. SAML, LDAP) or if you wish to leverage any of Dex's connector features (e.g. the ability to map GitHub organizations and teams to OIDC groups claims). Existing OIDC provider - use this if you already have an OIDC provider which you are using (e.g. Okta, OneLogin, Auth0, Microsoft), where you manage your users, groups, and memberships.","title":"Overview"},{"location":"operator-manual/sso/#dex","text":"Argo CD embeds and bundles Dex as part of its installation, for the purpose of delegating authentication to an external identity provider. Multiple types of identity providers are supported (OIDC, SAML, LDAP, GitHub, etc...). SSO configuration of Argo CD requires editing the argocd-cm ConfigMap with Dex connector settings. This document describes how to configure Argo CD SSO using GitHub (OAuth2) as an example, but the steps should be similar for other identity providers.","title":"Dex"},{"location":"operator-manual/sso/#1-register-the-application-in-the-identity-provider","text":"In GitHub, register a new application. The callback address should be the /api/dex/callback endpoint of your Argo CD URL (e.g. https://argocd.example.com/api/dex/callback). After registering the app, you will receive an OAuth2 client ID and secret. These values will be inputted into the Argo CD configmap.","title":"1. Register the application in the identity provider"},{"location":"operator-manual/sso/#2-configure-argo-cd-for-sso","text":"Edit the argocd-cm configmap: kubectl edit configmap argocd-cm -n argocd In the url key, input the base URL of Argo CD. In this example, it is https://argocd.example.com In the dex.config key, add the github connector to the connectors sub field. See Dex's GitHub connector documentation for explanation of the fields. A minimal config should populate the clientID, clientSecret generated in Step 1. You will very likely want to restrict logins to one or more GitHub organization. In the connectors.config.orgs list, add one or more GitHub organizations. Any member of the org will then be able to login to Argo CD to perform management tasks. data : url : https://argocd.example.com dex.config : | connectors: # GitHub example - type: github id: github name: GitHub config: clientID: aabbccddeeff00112233 clientSecret: $dex.github.clientSecret orgs: - name: your-github-org # GitHub enterprise example - type: github id: acme-github name: Acme GitHub config: hostName: github.acme.com clientID: abcdefghijklmnopqrst clientSecret: $dex.acme.clientSecret orgs: - name: your-github-org After saving, the changes should take affect automatically. NOTES: Any values which start with '$' will look to a key in argocd-secret of the same name (minus the $), to obtain the actual value. This allows you to store the clientSecret as a kubernetes secret. There is no need to set redirectURI in the connectors.config as shown in the dex documentation. Argo CD will automatically use the correct redirectURI for any OAuth2 connectors, to match the correct external callback URL (e.g. https://argocd.example.com/api/dex/callback)","title":"2. Configure Argo CD for SSO"},{"location":"operator-manual/sso/#existing-oidc-provider","text":"To configure Argo CD to delegate authenticate to your existing OIDC provider, add the OAuth2 configuration to the argocd-cm ConfigMap under the oidc.config key: data : url : https://argocd.example.com oidc.config : | name: Okta issuer: https://dev-123456.oktapreview.com clientID: aaaabbbbccccddddeee clientSecret: $oidc.okta.clientSecret # Some OIDC providers require a separate clientID for different callback URLs. # For example, if configuring Argo CD with self-hosted Dex, you will need a separate client ID # for the 'localhost' (CLI) client to Dex. This field is optional. If omitted, the CLI will # use the same clientID as the Argo CD server cliClientID: vvvvwwwwxxxxyyyyzzzz","title":"Existing OIDC Provider"},{"location":"operator-manual/webhook/","text":"Git Webhook Configuration \u00b6 Overview \u00b6 Argo CD polls Git repositories every three minutes to detect changes to the manifests. To eliminate this delay from polling, the API server can be configured to receive webhook events. Argo CD supports Git webhook notifications from GitHub, GitLab, and BitBucket. The following explains how to configure a Git webhook for GitHub, but the same process should be applicable to other providers. 1. Create The WebHook In The Git Provider \u00b6 In your Git provider, navigate to the settings page where webhooks can be configured. The payload URL configured in the Git provider should use the /api/webhook endpoint of your Argo CD instance (e.g. [https://argocd.example.com/api/webhook]). If you wish to use a shared secret, input an arbitrary value in the secret. This value will be used when configuring the webhook in the next step. 2. Configure Argo CD With The WebHook Secret Optional) \u00b6 Configuring a webhook shared secret is optional, since Argo CD will still refresh applications related to the Git repository, even with unauthenticated webhook events. This is safe to do since the contents of webhook payloads are considered untrusted, and will only result in a refresh of the application (a process which already occurs at three-minute intervals). If Argo CD is publicly accessible, then configuring a webhook secret is recommended to prevent a DDoS attack. In the argocd-secret kubernetes secret, configure one of the following keys with the Git provider's webhook secret configured in step 1. Provider K8s Secret Key GitHub github.webhook.secret GitLab gitlab.webhook.secret BitBucket bitbucket.webhook.uuid Edit the Argo CD kubernetes secret: kubectl edit secret argocd-secret -n argocd TIP: for ease of entering secrets, kubernetes supports inputting secrets in the stringData field, which saves you the trouble of base64 encoding the values and copying it to the data field. Simply copy the shared webhook secret created in step 1, to the corresponding GitHub/GitLab/BitBucket key under the stringData field: apiVersion : v1 kind : Secret metadata : name : argocd-secret namespace : argocd type : Opaque data : ... stringData : # github webhook secret github.webhook.secret : shhhh! it's a github secret # gitlab webhook secret gitlab.webhook.secret : shhhh! it's a gitlab secret # bitbucket webhook secret bitbucket.webhook.uuid : your-bitbucket-uuid After saving, the changes should take affect automatically.","title":"Git Webhook Configuration"},{"location":"operator-manual/webhook/#git-webhook-configuration","text":"","title":"Git Webhook Configuration"},{"location":"operator-manual/webhook/#overview","text":"Argo CD polls Git repositories every three minutes to detect changes to the manifests. To eliminate this delay from polling, the API server can be configured to receive webhook events. Argo CD supports Git webhook notifications from GitHub, GitLab, and BitBucket. The following explains how to configure a Git webhook for GitHub, but the same process should be applicable to other providers.","title":"Overview"},{"location":"operator-manual/webhook/#1-create-the-webhook-in-the-git-provider","text":"In your Git provider, navigate to the settings page where webhooks can be configured. The payload URL configured in the Git provider should use the /api/webhook endpoint of your Argo CD instance (e.g. [https://argocd.example.com/api/webhook]). If you wish to use a shared secret, input an arbitrary value in the secret. This value will be used when configuring the webhook in the next step.","title":"1. Create The WebHook In The Git Provider"},{"location":"operator-manual/webhook/#2-configure-argo-cd-with-the-webhook-secret-optional","text":"Configuring a webhook shared secret is optional, since Argo CD will still refresh applications related to the Git repository, even with unauthenticated webhook events. This is safe to do since the contents of webhook payloads are considered untrusted, and will only result in a refresh of the application (a process which already occurs at three-minute intervals). If Argo CD is publicly accessible, then configuring a webhook secret is recommended to prevent a DDoS attack. In the argocd-secret kubernetes secret, configure one of the following keys with the Git provider's webhook secret configured in step 1. Provider K8s Secret Key GitHub github.webhook.secret GitLab gitlab.webhook.secret BitBucket bitbucket.webhook.uuid Edit the Argo CD kubernetes secret: kubectl edit secret argocd-secret -n argocd TIP: for ease of entering secrets, kubernetes supports inputting secrets in the stringData field, which saves you the trouble of base64 encoding the values and copying it to the data field. Simply copy the shared webhook secret created in step 1, to the corresponding GitHub/GitLab/BitBucket key under the stringData field: apiVersion : v1 kind : Secret metadata : name : argocd-secret namespace : argocd type : Opaque data : ... stringData : # github webhook secret github.webhook.secret : shhhh! it's a github secret # gitlab webhook secret gitlab.webhook.secret : shhhh! it's a gitlab secret # bitbucket webhook secret bitbucket.webhook.uuid : your-bitbucket-uuid After saving, the changes should take affect automatically.","title":"2. Configure Argo CD With The WebHook Secret Optional)"},{"location":"user-guide/application_sources/","text":"Application Source Types \u00b6 Argo CD supports several different ways in which kubernetes manifests can be defined: ksonnet applications kustomize applications helm charts Directory of YAML/json/jsonnet manifests Any custom config management tool configured as a config management plugin Some additional considerations should be made when deploying apps of a particular type: Ksonnet \u00b6 Environments \u00b6 Ksonnet has a first class concept of an \"environment.\" To create an application from a ksonnet app directory, an environment must be specified. For example, the following command creates the \"guestbook-default\" app, which points to the default environment: argocd app create guestbook-default --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --env default Parameters \u00b6 Ksonnet parameters all belong to a component. For example, the following are the parameters available in the guestbook app, all of which belong to the guestbook-ui component: $ ks param list COMPONENT PARAM VALUE ========= ===== ===== guestbook-ui containerPort 80 guestbook-ui image \"gcr.io/heptio-images/ks-guestbook-demo:0.1\" guestbook-ui name \"guestbook-ui\" guestbook-ui replicas 1 guestbook-ui servicePort 80 guestbook-ui type \"LoadBalancer\" When overriding ksonnet parameters in Argo CD, the component name should also be specified in the argocd app set command, in the form of -p COMPONENT=PARAM=VALUE . For example: argocd app set guestbook-default -p guestbook-ui = image = gcr.io/heptio-images/ks-guestbook-demo:0.1 Helm \u00b6 Values Files \u00b6 Helm has the ability to use a different, or even multiple \"values.yaml\" files to derive its parameters from. Alternate or multiple values file(s), can be specified using the --values flag. The flag can be repeated to support multiple values files: argocd app set helm-guestbook --values values-production.yaml Helm Parameters \u00b6 Helm has the ability to set parameter values, which override any values in a values.yaml . For example, service.type is a common parameter which is exposed in a Helm chart: helm template . --set service.type = LoadBalancer Similarly Argo CD can override values in the values.yaml parameters using argo app set command, in the form of -p PARAM=VALUE . For example: argocd app set helm-guestbook -p service.type = LoadBalancer Helm Hooks \u00b6 Helm hooks are equivalent in concept to Argo CD resource hooks . In helm, a hook is any normal kubernetes resource annotated with the helm.sh/hook annotation. When Argo CD deploys helm application which contains helm hooks, all helm hook resources are currently ignored during the kubectl apply of the manifests. There is an open issue to map Helm hooks to Argo CD's concept of Pre/Post/Sync hooks. Random Data \u00b6 Helm templating has the ability to generate random data during chart rendering via the randAlphaNum function. Many helm charts from the charts repository make use of this feature. For example, the following is the secret for the redis helm chart : data : {{ - if .Values.password }} redis-password : {{ .Values.password | b64enc | quote }} {{ - else }} redis-password : {{ randAlphaNum 10 | b64enc | quote }} {{ - end }} The Argo CD application controller periodically compares Git state against the live state, running the helm template <CHART> command to generate the helm manifests. Because the random value is regenerated every time the comparison is made, any application which makes use of the randAlphaNum function will always be in an OutOfSync state. This can be mitigated by explicitly setting a value, in the values.yaml such that the value is stable between each comparison. For example: argocd app set redis -p password = abc123 Config Management Plugins \u00b6 Argo CD allows integrating more config management tools using config management plugins. Following changes are required to configure new plugin: Make sure required binaries are available in argocd-repo-server pod. The binaries can be added via volume mounts or using custom image (see custom_tools ). Register a new plugin in argocd-cm ConfigMap: data : configManagementPlugins : | - name: pluginName init: # Optional command to initialize application source directory command: [\"sample command\"] args: [\"sample args\"] generate: # Command to generate manifests YAML command: [\"sample command\"] args: [\"sample args\"] The generate command must print a valid YAML stream to stdout. Both init and generate commands are executed inside the application source directory. Commands have access to system environment variables and following additional variables: ARGOCD_APP_NAME - name of application; ARGOCD_APP_NAMESPACE - destination application namespace Create an application and specify required config management plugin name. argocd app create <appName> --config-management-plugin <pluginName> More config management plugin examples are available in argocd-example-apps .","title":"Application Source Types"},{"location":"user-guide/application_sources/#application-source-types","text":"Argo CD supports several different ways in which kubernetes manifests can be defined: ksonnet applications kustomize applications helm charts Directory of YAML/json/jsonnet manifests Any custom config management tool configured as a config management plugin Some additional considerations should be made when deploying apps of a particular type:","title":"Application Source Types"},{"location":"user-guide/application_sources/#ksonnet","text":"","title":"Ksonnet"},{"location":"user-guide/application_sources/#environments","text":"Ksonnet has a first class concept of an \"environment.\" To create an application from a ksonnet app directory, an environment must be specified. For example, the following command creates the \"guestbook-default\" app, which points to the default environment: argocd app create guestbook-default --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --env default","title":"Environments"},{"location":"user-guide/application_sources/#parameters","text":"Ksonnet parameters all belong to a component. For example, the following are the parameters available in the guestbook app, all of which belong to the guestbook-ui component: $ ks param list COMPONENT PARAM VALUE ========= ===== ===== guestbook-ui containerPort 80 guestbook-ui image \"gcr.io/heptio-images/ks-guestbook-demo:0.1\" guestbook-ui name \"guestbook-ui\" guestbook-ui replicas 1 guestbook-ui servicePort 80 guestbook-ui type \"LoadBalancer\" When overriding ksonnet parameters in Argo CD, the component name should also be specified in the argocd app set command, in the form of -p COMPONENT=PARAM=VALUE . For example: argocd app set guestbook-default -p guestbook-ui = image = gcr.io/heptio-images/ks-guestbook-demo:0.1","title":"Parameters"},{"location":"user-guide/application_sources/#helm","text":"","title":"Helm"},{"location":"user-guide/application_sources/#values-files","text":"Helm has the ability to use a different, or even multiple \"values.yaml\" files to derive its parameters from. Alternate or multiple values file(s), can be specified using the --values flag. The flag can be repeated to support multiple values files: argocd app set helm-guestbook --values values-production.yaml","title":"Values Files"},{"location":"user-guide/application_sources/#helm-parameters","text":"Helm has the ability to set parameter values, which override any values in a values.yaml . For example, service.type is a common parameter which is exposed in a Helm chart: helm template . --set service.type = LoadBalancer Similarly Argo CD can override values in the values.yaml parameters using argo app set command, in the form of -p PARAM=VALUE . For example: argocd app set helm-guestbook -p service.type = LoadBalancer","title":"Helm Parameters"},{"location":"user-guide/application_sources/#helm-hooks","text":"Helm hooks are equivalent in concept to Argo CD resource hooks . In helm, a hook is any normal kubernetes resource annotated with the helm.sh/hook annotation. When Argo CD deploys helm application which contains helm hooks, all helm hook resources are currently ignored during the kubectl apply of the manifests. There is an open issue to map Helm hooks to Argo CD's concept of Pre/Post/Sync hooks.","title":"Helm Hooks"},{"location":"user-guide/application_sources/#random-data","text":"Helm templating has the ability to generate random data during chart rendering via the randAlphaNum function. Many helm charts from the charts repository make use of this feature. For example, the following is the secret for the redis helm chart : data : {{ - if .Values.password }} redis-password : {{ .Values.password | b64enc | quote }} {{ - else }} redis-password : {{ randAlphaNum 10 | b64enc | quote }} {{ - end }} The Argo CD application controller periodically compares Git state against the live state, running the helm template <CHART> command to generate the helm manifests. Because the random value is regenerated every time the comparison is made, any application which makes use of the randAlphaNum function will always be in an OutOfSync state. This can be mitigated by explicitly setting a value, in the values.yaml such that the value is stable between each comparison. For example: argocd app set redis -p password = abc123","title":"Random Data"},{"location":"user-guide/application_sources/#config-management-plugins","text":"Argo CD allows integrating more config management tools using config management plugins. Following changes are required to configure new plugin: Make sure required binaries are available in argocd-repo-server pod. The binaries can be added via volume mounts or using custom image (see custom_tools ). Register a new plugin in argocd-cm ConfigMap: data : configManagementPlugins : | - name: pluginName init: # Optional command to initialize application source directory command: [\"sample command\"] args: [\"sample args\"] generate: # Command to generate manifests YAML command: [\"sample command\"] args: [\"sample args\"] The generate command must print a valid YAML stream to stdout. Both init and generate commands are executed inside the application source directory. Commands have access to system environment variables and following additional variables: ARGOCD_APP_NAME - name of application; ARGOCD_APP_NAMESPACE - destination application namespace Create an application and specify required config management plugin name. argocd app create <appName> --config-management-plugin <pluginName> More config management plugin examples are available in argocd-example-apps .","title":"Config Management Plugins"},{"location":"user-guide/auto_sync/","text":"Automated Sync Policy \u00b6 Argo CD has the ability to automatically sync an application when it detects differences between the desired manifests in Git, and the live state in the cluster. A benefit of automatic sync is that CI/CD pipelines no longer need direct access to the Argo CD API server to perform the deployment. Instead, the pipeline makes a commit and push to the Git repository with the changes to the manifests in the tracking Git repo. To configure automated sync run: argocd app set <APPNAME> --sync-policy automated Alternatively, if creating the application an application manifest, specify a syncPolicy with an automated policy. spec : syncPolicy : automated : {} Automatic Pruning \u00b6 By default (and as a safety mechanism), automated sync will not delete resources when Argo CD detects the resource is no longer defined in Git. To prune the resources, a manual sync can always be performed (with pruning checked). Pruning can also be enabled to happen automatically as part of the automated sync by running: argocd app set <APPNAME> --auto-prune Or by setting the prune option to true in the automated sync policy: spec : syncPolicy : automated : prune : true Automated Sync Semantics \u00b6 An automated sync will only be performed if the application is OutOfSync. Applications in a Synced or error state will not attempt automated sync. Automated sync will only attempt one synchronization per unique combination of commit SHA1 and application parameters. If the most recent successful sync in the history was already performed against the same commit-SHA and parameters, a second sync will not be attempted. Automatic sync will not reattempt a sync if the previous sync attempt against the same commit-SHA and parameters had failed. Rollback cannot be performed against an application with automated sync enabled.","title":"Automated Sync Policy"},{"location":"user-guide/auto_sync/#automated-sync-policy","text":"Argo CD has the ability to automatically sync an application when it detects differences between the desired manifests in Git, and the live state in the cluster. A benefit of automatic sync is that CI/CD pipelines no longer need direct access to the Argo CD API server to perform the deployment. Instead, the pipeline makes a commit and push to the Git repository with the changes to the manifests in the tracking Git repo. To configure automated sync run: argocd app set <APPNAME> --sync-policy automated Alternatively, if creating the application an application manifest, specify a syncPolicy with an automated policy. spec : syncPolicy : automated : {}","title":"Automated Sync Policy"},{"location":"user-guide/auto_sync/#automatic-pruning","text":"By default (and as a safety mechanism), automated sync will not delete resources when Argo CD detects the resource is no longer defined in Git. To prune the resources, a manual sync can always be performed (with pruning checked). Pruning can also be enabled to happen automatically as part of the automated sync by running: argocd app set <APPNAME> --auto-prune Or by setting the prune option to true in the automated sync policy: spec : syncPolicy : automated : prune : true","title":"Automatic Pruning"},{"location":"user-guide/auto_sync/#automated-sync-semantics","text":"An automated sync will only be performed if the application is OutOfSync. Applications in a Synced or error state will not attempt automated sync. Automated sync will only attempt one synchronization per unique combination of commit SHA1 and application parameters. If the most recent successful sync in the history was already performed against the same commit-SHA and parameters, a second sync will not be attempted. Automatic sync will not reattempt a sync if the previous sync attempt against the same commit-SHA and parameters had failed. Rollback cannot be performed against an application with automated sync enabled.","title":"Automated Sync Semantics"},{"location":"user-guide/best_practices/","text":"Best Practices \u00b6 Separating Config Vs. Source Code Repositories \u00b6 Using a separate Git repository to hold your kubernetes manifests, keeping the config separate from your application source code, is highly recommended for the following reasons: It provides a clean separation of application code vs. application config. There will be times when you wish to modify just the manifests without triggering an entire CI build. For example, you likely do not want to trigger a build if you simply wish to bump the number of replicas in a Deployment spec. Cleaner audit log. For auditing purposes, a repo which only holds configuration will have a much cleaner Git history of what changes were made, without the noise coming from check-ins due to normal development activity. Your application may be comprised of services built from multiple Git repositories, but is deployed as a single unit. Oftentimes, microservices applications are comprised of services with different versioning schemes, and release cycles (e.g. ELK, Kafka + Zookeeper). It may not make sense to store the manifests in one of the source code repositories of a single component. Separation of access. The developers who are developing the application, may not necessarily be the same people who can/should push to production environments, either intentionally or unintentionally. By having separate repos, commit access can be given to the source code repo, and not the application config repo. If you are automating your CI pipeline, pushing manifest changes to the same Cit repository can trigger an infinite loop of build jobs and Git commit triggers. Having a separate repo to push config changes to, prevents this from happening. Leaving Rroom For Imperativeness \u00b6 It may be desired to leave room for some imperativeness/automation, and not have everything defined in your Git manifests. For example, if you want the number of your deployment's replicas to be managed by Horizontal Pod Autoscaler , then you would not want to track replicas in Git. apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment spec : # do not include replicas in the manifests if you want replicas to be controlled by HPA # replicas: 1 template : spec : containers : - image : nginx:1.7.9 name : nginx ports : - containerPort : 80 ... Ensuring Manifests At Git Revisions Are Truly Immutable \u00b6 When using templating tools like helm or kustomize , it is possible for manifests to change their meaning from one day to the next. This is typically caused by changes made to an upstream helm repository or kustomize base. For example, consider the following kustomization.yaml bases : - github.com/argoproj/argo-cd//manifests/cluster-install The above kustomization has a remote base to he HEAD revision of the argo-cd repo. Since this is not stable target, the manifests for this kustomize application can suddenly change meaning, even without any changes to your own Git repository. A better version would be to use a Git tag or commit SHA. For example: bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.11.1","title":"Best Practices"},{"location":"user-guide/best_practices/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/best_practices/#separating-config-vs-source-code-repositories","text":"Using a separate Git repository to hold your kubernetes manifests, keeping the config separate from your application source code, is highly recommended for the following reasons: It provides a clean separation of application code vs. application config. There will be times when you wish to modify just the manifests without triggering an entire CI build. For example, you likely do not want to trigger a build if you simply wish to bump the number of replicas in a Deployment spec. Cleaner audit log. For auditing purposes, a repo which only holds configuration will have a much cleaner Git history of what changes were made, without the noise coming from check-ins due to normal development activity. Your application may be comprised of services built from multiple Git repositories, but is deployed as a single unit. Oftentimes, microservices applications are comprised of services with different versioning schemes, and release cycles (e.g. ELK, Kafka + Zookeeper). It may not make sense to store the manifests in one of the source code repositories of a single component. Separation of access. The developers who are developing the application, may not necessarily be the same people who can/should push to production environments, either intentionally or unintentionally. By having separate repos, commit access can be given to the source code repo, and not the application config repo. If you are automating your CI pipeline, pushing manifest changes to the same Cit repository can trigger an infinite loop of build jobs and Git commit triggers. Having a separate repo to push config changes to, prevents this from happening.","title":"Separating Config Vs. Source Code Repositories"},{"location":"user-guide/best_practices/#leaving-rroom-for-imperativeness","text":"It may be desired to leave room for some imperativeness/automation, and not have everything defined in your Git manifests. For example, if you want the number of your deployment's replicas to be managed by Horizontal Pod Autoscaler , then you would not want to track replicas in Git. apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment spec : # do not include replicas in the manifests if you want replicas to be controlled by HPA # replicas: 1 template : spec : containers : - image : nginx:1.7.9 name : nginx ports : - containerPort : 80 ...","title":"Leaving Rroom For Imperativeness"},{"location":"user-guide/best_practices/#ensuring-manifests-at-git-revisions-are-truly-immutable","text":"When using templating tools like helm or kustomize , it is possible for manifests to change their meaning from one day to the next. This is typically caused by changes made to an upstream helm repository or kustomize base. For example, consider the following kustomization.yaml bases : - github.com/argoproj/argo-cd//manifests/cluster-install The above kustomization has a remote base to he HEAD revision of the argo-cd repo. Since this is not stable target, the manifests for this kustomize application can suddenly change meaning, even without any changes to your own Git repository. A better version would be to use a Git tag or commit SHA. For example: bases : - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.11.1","title":"Ensuring Manifests At Git Revisions Are Truly Immutable"},{"location":"user-guide/ci_automation/","text":"Automation from CI Pipelines \u00b6 Argo CD follows the GitOps model of deployment, where desired configuration changes are first pushed to Git, and the cluster state then syncs to the desired state in git. This is a departure from imperative pipelines which do not traditionally use Git repositories to hold application config. To push new container images into to a cluster managed by Argo CD, the following workflow (or variations), might be used: Build And Publish A New Container Image \u00b6 docker build -t mycompany/guestbook:v2.0 . docker push mycompany/guestbook:v2.0 Update The Local Manifests Using Your Preferred Templating Tool, And Push The Changes To Git \u00b6 Note The use of a different Git repository to hold your kubernetes manifests (separate from your application source code), is highly recommended. See best practices for further rationale. git clone https://github.com/mycompany/guestbook-config.git cd guestbook-config # kustomize kustomize edit set imagetag mycompany/guestbook:v2.0 # ksonnet ks param set guestbook image mycompany/guestbook:v2.0 # plain yaml kubectl patch --local -f config-deployment.yaml -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"guestbook\",\"image\":\"mycompany/guestbook:v2.0\"}]}}}}' -o yaml git add . -m \"Update guestbook to v2.0\" git push Synchronize The App (Optional) \u00b6 For convenience, the argocd CLI can be downloaded directly from the API server. This is useful so that the CLI used in the CI pipeline is always kept in-sync and uses argocd binary that is always compatible with the Argo CD API server. export ARGOCD_SERVER = argocd.mycompany.com export ARGOCD_AUTH_TOKEN = <JWT token generated from project> curl -sSL -o /usr/local/bin/argocd https:// ${ ARGOCD_SERVER } /download/argocd-linux-amd64 argocd app sync guestbook argocd app wait guestbook If automated synchronization is configured for the application, this step is unnecessary. The controller will automatically detect the new config (fast tracked using a webhook , or polled every 3 minutes), and automatically sync the new manifests.","title":"Automation from CI Pipelines"},{"location":"user-guide/ci_automation/#automation-from-ci-pipelines","text":"Argo CD follows the GitOps model of deployment, where desired configuration changes are first pushed to Git, and the cluster state then syncs to the desired state in git. This is a departure from imperative pipelines which do not traditionally use Git repositories to hold application config. To push new container images into to a cluster managed by Argo CD, the following workflow (or variations), might be used:","title":"Automation from CI Pipelines"},{"location":"user-guide/ci_automation/#build-and-publish-a-new-container-image","text":"docker build -t mycompany/guestbook:v2.0 . docker push mycompany/guestbook:v2.0","title":"Build And Publish A New Container Image"},{"location":"user-guide/ci_automation/#update-the-local-manifests-using-your-preferred-templating-tool-and-push-the-changes-to-git","text":"Note The use of a different Git repository to hold your kubernetes manifests (separate from your application source code), is highly recommended. See best practices for further rationale. git clone https://github.com/mycompany/guestbook-config.git cd guestbook-config # kustomize kustomize edit set imagetag mycompany/guestbook:v2.0 # ksonnet ks param set guestbook image mycompany/guestbook:v2.0 # plain yaml kubectl patch --local -f config-deployment.yaml -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"guestbook\",\"image\":\"mycompany/guestbook:v2.0\"}]}}}}' -o yaml git add . -m \"Update guestbook to v2.0\" git push","title":"Update The Local Manifests Using Your Preferred Templating Tool, And Push The Changes To Git"},{"location":"user-guide/ci_automation/#synchronize-the-app-optional","text":"For convenience, the argocd CLI can be downloaded directly from the API server. This is useful so that the CLI used in the CI pipeline is always kept in-sync and uses argocd binary that is always compatible with the Argo CD API server. export ARGOCD_SERVER = argocd.mycompany.com export ARGOCD_AUTH_TOKEN = <JWT token generated from project> curl -sSL -o /usr/local/bin/argocd https:// ${ ARGOCD_SERVER } /download/argocd-linux-amd64 argocd app sync guestbook argocd app wait guestbook If automated synchronization is configured for the application, this step is unnecessary. The controller will automatically detect the new config (fast tracked using a webhook , or polled every 3 minutes), and automatically sync the new manifests.","title":"Synchronize The App (Optional)"},{"location":"user-guide/diffing/","text":"Diffing Customization \u00b6 It is possible for an application to be OutOfSync even immediately after a successful Sync operation. Some reasons for this might be: There is a bug in the manifest, where it contains extra/unknown fields from the actual K8s spec. These extra fields would get dropped when querying Kubernetes for the live state, resulting in an OutOfSync status indicating a missing field was detected. The sync was performed (with pruning disabled), and there are resources which need to be deleted. A controller or mutating webhook is altering the object after it was submitted to Kubernetes in a manner which contradicts Git. A Helm chart is using a template function such as randAlphaNum , which generates different data every time helm template is invoked. For Horizontal Pod Autoscaling (HPA) objects, the HPA controller is known to reorder spec.metrics in a specific order. See kubernetes issue #74099 . To work around this, you can order spec.replicas in Git in the same order that the controller prefers. In case it is impossible to fix the upstream issue, Argo CD allows you to optionally ignore differences of problematic resources. The diffing customization can be configured for single or multiple application resources or at a system level. Application Level Configuration \u00b6 Argo CD allows ignoring differences at a specific JSON path. The following sample application is configured to ignore differences in spec.replicas for all deployments: spec : ignoreDifferences : - group : apps kind : Deployment jsonPointers : - /spec/replicas The above customization could be narrowed to a resource with the specified name and optional namespace: spec : ignoreDifferences : - group : apps kind : Deployment name : guestbook namespace : default jsonPointers : - /spec/replicas System-Level Configuration \u00b6 The comparison of resources with well-known issues can be customized at a system level. Ignored differences can be configured for a specified group and kind in resource.customizations key of argocd-cm ConfigMap. Following is an example of a customization which ignores the caBundle field of a MutatingWebhookConfiguration webhooks: data : resource.customizations : | admissionregistration.k8s.io/MutatingWebhookConfiguration: ignoreDifferences: | jsonPointers: - /webhooks/0/clientConfig/caBundle","title":"Diffing Customization"},{"location":"user-guide/diffing/#diffing-customization","text":"It is possible for an application to be OutOfSync even immediately after a successful Sync operation. Some reasons for this might be: There is a bug in the manifest, where it contains extra/unknown fields from the actual K8s spec. These extra fields would get dropped when querying Kubernetes for the live state, resulting in an OutOfSync status indicating a missing field was detected. The sync was performed (with pruning disabled), and there are resources which need to be deleted. A controller or mutating webhook is altering the object after it was submitted to Kubernetes in a manner which contradicts Git. A Helm chart is using a template function such as randAlphaNum , which generates different data every time helm template is invoked. For Horizontal Pod Autoscaling (HPA) objects, the HPA controller is known to reorder spec.metrics in a specific order. See kubernetes issue #74099 . To work around this, you can order spec.replicas in Git in the same order that the controller prefers. In case it is impossible to fix the upstream issue, Argo CD allows you to optionally ignore differences of problematic resources. The diffing customization can be configured for single or multiple application resources or at a system level.","title":"Diffing Customization"},{"location":"user-guide/diffing/#application-level-configuration","text":"Argo CD allows ignoring differences at a specific JSON path. The following sample application is configured to ignore differences in spec.replicas for all deployments: spec : ignoreDifferences : - group : apps kind : Deployment jsonPointers : - /spec/replicas The above customization could be narrowed to a resource with the specified name and optional namespace: spec : ignoreDifferences : - group : apps kind : Deployment name : guestbook namespace : default jsonPointers : - /spec/replicas","title":"Application Level Configuration"},{"location":"user-guide/diffing/#system-level-configuration","text":"The comparison of resources with well-known issues can be customized at a system level. Ignored differences can be configured for a specified group and kind in resource.customizations key of argocd-cm ConfigMap. Following is an example of a customization which ignores the caBundle field of a MutatingWebhookConfiguration webhooks: data : resource.customizations : | admissionregistration.k8s.io/MutatingWebhookConfiguration: ignoreDifferences: | jsonPointers: - /webhooks/0/clientConfig/caBundle","title":"System-Level Configuration"},{"location":"user-guide/health/","text":"Resource Health \u00b6 Overview \u00b6 Argo CD provides built-in health assessment for several standard Kubernetes types, which is then surfaced to the overall Application health status as a whole. The following checks are made for specific types of kuberentes resources: Deployment, ReplicaSet, StatefulSet DaemonSet \u00b6 Observed generation is equal to desired generation. Number of updated replicas equals the number of desired replicas. Service \u00b6 If service type is of type LoadBalancer , the status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . Ingress \u00b6 The status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP . PersistentVolumeClaim \u00b6 The status.phase is Bound Custom Health Checks \u00b6 Argo CD supports custom health checks written in Lua . This is useful if you: Are affected by known issues where your Ingress or StatefulSet resources are stuck in Progressing state because of bug in your resource controller. Have a custom resource for which Argo CD does not have a built-in health check. There are two ways to configure a custom health check. The next two sections describe those ways. Way 1. Define a Custom Health Check in argocd-cm ConfigMap \u00b6 Custom health checks can be defined in resource.customizations field of argocd-cm . Following example demonstrates a health check for certmanager.k8s.io/Certificate . data : resource.customizations : | certmanager.k8s.io/Certificate: health.lua: | hs = {} if obj.status ~= nil then if obj.status.conditions ~= nil then for i, condition in ipairs(obj.status.conditions) do if condition.type == \"Ready\" and condition.status == \"False\" then hs.status = \"Degraded\" hs.message = condition.message return hs end if condition.type == \"Ready\" and condition.status == \"True\" then hs.status = \"Healthy\" hs.message = condition.message return hs end end end end hs.status = \"Progressing\" hs.message = \"Waiting for certificate\" return hs The obj is a global variable which contains the resource. The script must return an object with status and optional message field. NOTE: as a security measure you don't have access to most of the standard Lua libraries. Way 2. Contribute a Custom Health Check \u00b6 A health check can be bundled into Argo CD. Custom health check scripts are located in the resource_customizations directory of https://github.com/argoproj/argo-cd . This must have the following directory structure: argo-cd |-- resource_customizations | |-- your.crd.group.io # CRD group | | |-- MyKind # Resource kind | | | |-- health.lua # Health check | | | |-- health_test.yaml # Test inputs and expected results | | | +-- testdata # Directory with test resource YAML definitions Each health check must have tests defined in health_test.yaml file. The health_test.yaml is a YAML file with the following structure: tests : - healthStatus : status : ExpectedStatus message : Expected message inputPath : testdata/test-resource-definition.yaml The PR#1139 is an example of Cert Manager CRDs custom health check.","title":"Resource Health"},{"location":"user-guide/health/#resource-health","text":"","title":"Resource Health"},{"location":"user-guide/health/#overview","text":"Argo CD provides built-in health assessment for several standard Kubernetes types, which is then surfaced to the overall Application health status as a whole. The following checks are made for specific types of kuberentes resources:","title":"Overview"},{"location":"user-guide/health/#deployment-replicaset-statefulset-daemonset","text":"Observed generation is equal to desired generation. Number of updated replicas equals the number of desired replicas.","title":"Deployment, ReplicaSet, StatefulSet DaemonSet"},{"location":"user-guide/health/#service","text":"If service type is of type LoadBalancer , the status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP .","title":"Service"},{"location":"user-guide/health/#ingress","text":"The status.loadBalancer.ingress list is non-empty, with at least one value for hostname or IP .","title":"Ingress"},{"location":"user-guide/health/#persistentvolumeclaim","text":"The status.phase is Bound","title":"PersistentVolumeClaim"},{"location":"user-guide/health/#custom-health-checks","text":"Argo CD supports custom health checks written in Lua . This is useful if you: Are affected by known issues where your Ingress or StatefulSet resources are stuck in Progressing state because of bug in your resource controller. Have a custom resource for which Argo CD does not have a built-in health check. There are two ways to configure a custom health check. The next two sections describe those ways.","title":"Custom Health Checks"},{"location":"user-guide/health/#way-1-define-a-custom-health-check-in-argocd-cm-configmap","text":"Custom health checks can be defined in resource.customizations field of argocd-cm . Following example demonstrates a health check for certmanager.k8s.io/Certificate . data : resource.customizations : | certmanager.k8s.io/Certificate: health.lua: | hs = {} if obj.status ~= nil then if obj.status.conditions ~= nil then for i, condition in ipairs(obj.status.conditions) do if condition.type == \"Ready\" and condition.status == \"False\" then hs.status = \"Degraded\" hs.message = condition.message return hs end if condition.type == \"Ready\" and condition.status == \"True\" then hs.status = \"Healthy\" hs.message = condition.message return hs end end end end hs.status = \"Progressing\" hs.message = \"Waiting for certificate\" return hs The obj is a global variable which contains the resource. The script must return an object with status and optional message field. NOTE: as a security measure you don't have access to most of the standard Lua libraries.","title":"Way 1. Define a Custom Health Check in argocd-cm ConfigMap"},{"location":"user-guide/health/#way-2-contribute-a-custom-health-check","text":"A health check can be bundled into Argo CD. Custom health check scripts are located in the resource_customizations directory of https://github.com/argoproj/argo-cd . This must have the following directory structure: argo-cd |-- resource_customizations | |-- your.crd.group.io # CRD group | | |-- MyKind # Resource kind | | | |-- health.lua # Health check | | | |-- health_test.yaml # Test inputs and expected results | | | +-- testdata # Directory with test resource YAML definitions Each health check must have tests defined in health_test.yaml file. The health_test.yaml is a YAML file with the following structure: tests : - healthStatus : status : ExpectedStatus message : Expected message inputPath : testdata/test-resource-definition.yaml The PR#1139 is an example of Cert Manager CRDs custom health check.","title":"Way 2. Contribute a Custom Health Check"},{"location":"user-guide/parameters/","text":"Parameter Overrides \u00b6 Argo CD provides a mechanism to override the parameters of a ksonnet/helm app. This provides flexibility in having most of the application manifests defined in Git, while leaving room for some parts of the k8s manifests determined dynamically, or outside of Git. It also serves as an alternative way of redeploying an application by changing application parameters via Argo CD, instead of making the changes to the manifests in Git. Note Many consider this mode of operation as an anti-pattern to GitOps, since the source of truth becomes a union of the Git repository, and the application overrides. The Argo CD parameter overrides feature is provided mainly as a convenience to developers and is intended to be used in dev/test environments, vs. production environments. To use parameter overrides, run the argocd app set -p (COMPONENT=)PARAM=VALUE command: argocd app set guestbook -p guestbook = image = example/guestbook:abcd123 argocd app sync guestbook The PARAM is expected to be a normal YAML path argocd app set guestbook -p guestbook = ingress.enabled = true argocd app set guestbook -p guestbook = ingress.hosts [ 0 ]= guestbook.myclusterurl The following are situations where parameter overrides would be useful: A team maintains a \"dev\" environment, which needs to be continually updated with the latest version of their guestbook application after every build in the tip of master. To address this use case, the application would expose a parameter named image , whose value used in the dev environment contains a placeholder value (e.g. example/guestbook:replaceme ). The placeholder value would be determined externally (outside of Git) such as a build system. Then, as part of the build pipeline, the parameter value of the image would be continually updated to the freshly built image (e.g. argocd app set guestbook -p guestbook=image=example/guestbook:abcd123 ). A sync operation would result in the application being redeployed with the new image. A repository of Helm manifests is already publicly available (e.g. https://github.com/helm/charts). Since commit access to the repository is unavailable, it is useful to be able to install charts from the public repository and customize the deployment with different parameters, without resorting to forking the repository to make the changes. For example, to install Redis from the Helm chart repository and customize the the database password, you would run: argocd app create redis --repo https://github.com/helm/charts.git --path stable/redis --dest-server https://kubernetes.default.svc --dest-namespace default -p password = abc123","title":"Parameter Overrides"},{"location":"user-guide/parameters/#parameter-overrides","text":"Argo CD provides a mechanism to override the parameters of a ksonnet/helm app. This provides flexibility in having most of the application manifests defined in Git, while leaving room for some parts of the k8s manifests determined dynamically, or outside of Git. It also serves as an alternative way of redeploying an application by changing application parameters via Argo CD, instead of making the changes to the manifests in Git. Note Many consider this mode of operation as an anti-pattern to GitOps, since the source of truth becomes a union of the Git repository, and the application overrides. The Argo CD parameter overrides feature is provided mainly as a convenience to developers and is intended to be used in dev/test environments, vs. production environments. To use parameter overrides, run the argocd app set -p (COMPONENT=)PARAM=VALUE command: argocd app set guestbook -p guestbook = image = example/guestbook:abcd123 argocd app sync guestbook The PARAM is expected to be a normal YAML path argocd app set guestbook -p guestbook = ingress.enabled = true argocd app set guestbook -p guestbook = ingress.hosts [ 0 ]= guestbook.myclusterurl The following are situations where parameter overrides would be useful: A team maintains a \"dev\" environment, which needs to be continually updated with the latest version of their guestbook application after every build in the tip of master. To address this use case, the application would expose a parameter named image , whose value used in the dev environment contains a placeholder value (e.g. example/guestbook:replaceme ). The placeholder value would be determined externally (outside of Git) such as a build system. Then, as part of the build pipeline, the parameter value of the image would be continually updated to the freshly built image (e.g. argocd app set guestbook -p guestbook=image=example/guestbook:abcd123 ). A sync operation would result in the application being redeployed with the new image. A repository of Helm manifests is already publicly available (e.g. https://github.com/helm/charts). Since commit access to the repository is unavailable, it is useful to be able to install charts from the public repository and customize the deployment with different parameters, without resorting to forking the repository to make the changes. For example, to install Redis from the Helm chart repository and customize the the database password, you would run: argocd app create redis --repo https://github.com/helm/charts.git --path stable/redis --dest-server https://kubernetes.default.svc --dest-namespace default -p password = abc123","title":"Parameter Overrides"},{"location":"user-guide/projects/","text":"Projects \u00b6 Projects provide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. Projects provide the following features: restrict what may be deployed (trusted Git source repositories) restrict where apps may be deployed to (destination clusters and namespaces) restrict what kinds of objects may or may not be deployed (e.g. RBAC, CRDs, DaemonSets, NetworkPolicy etc...) defining project roles to provide application RBAC (bound to OIDC groups and/or JWT tokens) The Default Project \u00b6 Every application belongs to a single project. If unspecified, an application belongs to the default project, which is created automatically and by default, permits deployments from any source repo, to any cluster, and all resource Kinds. The default project can be modified, but not deleted. When initially created, it's specification is configured to be the most permissive: spec : sourceRepos : - '*' destinations : - namespace : '*' server : '*' clusterResourceWhitelist : - group : '*' kind : '*' Creating Projects \u00b6 Additional projects can be created to give separate teams different levels of access to namespaces. The following command creates a new project myproject which can deploy applications to namespace mynamespace of cluster https://kubernetes.default.svc . The permitted Git source repository is set to https://github.com/argoproj/argocd-example-apps.git repository. argocd proj create myproject -d https://kubernetes.default.svc,mynamespace -s https://github.com/argoproj/argocd-example-apps.git Managing Projects \u00b6 Permitted source Git repositories are managed using commands: argocd proj add-source <PROJECT> <REPO> argocd proj remove-source <PROJECT> <REPO> Permitted destination clusters and namespaces are managed with the commands: argocd proj add-destination <PROJECT> <CLUSTER>,<NAMESPACE> argocd proj remove-destination <PROJECT> <CLUSTER>,<NAMESPACE> Permitted destination K8s resource kinds are managed with the commands. Note that namespaced-scoped resources are restricted via a blacklist, whereas cluster-scoped resources are restricted via whitelist. argocd proj allow-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj allow-namespace-resource <PROJECT> <GROUP> <KIND> argocd proj deny-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj deny-namespace-resource <PROJECT> <GROUP> <KIND> Assign Application To A Project \u00b6 The application project can be changed using app set command. In order to change the project of an app, the user must have permissions to access the new project. argocd app set guestbook-default --project myproject Configuring RBAC With Projects \u00b6 Once projects have been defined, RBAC rules can be written to restrict access to the applications in the project. The following example configures RBAC for two GitHub teams: team1 and team2 , both in the GitHub org, some-github-org . There are two projects, project-a and project-b . team1 can only manage applications in project-a , while team2 can only manage applications in project-b . Both team1 and team2 have the ability to manage repositories. ConfigMap argocd-rbac-cm example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm data : policy.default : \"\" policy.csv : | p, some-github-org:team1, applications, *, project-a/*, allow p, some-github-org:team2, applications, *, project-a/*, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, some-github-org:team1, org-admin g, some-github-org:team2, org-admin Project Roles \u00b6 Projects include a feature called roles that enable automated access to a project's applications. These can be used to give a CI pipeline a restricted set of permissions. For example, a CI system may only be able to sync a single app (but not change its source or destination). Projects can have multiple roles, and those roles can have different access granted to them. These permissions are called policies, and they are stored within the role as a list of policy strings. A role's policy can only grant access to that role and are limited to applications within the role's project. However, the policies have an option for granting wildcard access to any application within a project. In order to create roles in a project and add policies to a role, a user will need permission to update a project. The following commands can be used to manage a role. argocd proj role list argocd proj role get argocd proj role create argocd proj role delete argocd proj role add-policy argocd proj role remove-policy Project roles in itself are not useful without generating a token to associate to that role. Argo CD supports JWT tokens as the means to authenticate to a role. Since the JWT token is associated with a role's policies, any changes to the role's policies will immediately take effect for that JWT token. The following commands are used to manage the JWT tokens. argocd proj role create-token PROJECT ROLE-NAME argocd proj role delete-token PROJECT ROLE-NAME ISSUED-AT Since the JWT tokens aren't stored in Argo CD, they can only be retrieved when they are created. A user can leverage them in the cli by either passing them in using the --auth-token flag or setting the ARGOCD_AUTH_TOKEN environment variable. The JWT tokens can be used until they expire or are revoked. The JWT tokens can created with or without an expiration, but the default on the cli is creates them without an expirations date. Even if a token has not expired, it cannot be used if the token has been revoked. Below is an example of leveraging a JWT token to access a guestbook application. It makes the assumption that the user already has a project named myproject and an application called guestbook-default. PROJ = myproject APP = guestbook-default ROLE = get-role argocd proj role create $PROJ $ROLE argocd proj role create-token $PROJ $ROLE -e 10m JWT = <value from command above> argocd proj role list $PROJ argocd proj role get $PROJ $ROLE # This command will fail because the JWT Token associated with the project role does not have a policy to allow access to the application argocd app get $APP --auth-token $JWT # Adding a policy to grant access to the application for the new role argocd proj role add-policy $PROJ $ROLE --action get --permission allow --object $APP argocd app get $PROJ - $ROLE --auth-token $JWT # Removing the policy we added and adding one with a wildcard. argocd proj role remove-policy $PROJ $TOKEN -a get -o $PROJ - $TOKEN argocd proj role remove-policy $PROJ $TOKEN -a get -o '*' # The wildcard allows us to access the application due to the wildcard. argocd app get $PROJ - $TOKEN --auth-token $JWT argocd proj role get $PROJ argocd proj role get $PROJ $ROLE # Revoking the JWT token argocd proj role delete-token $PROJ $ROLE <id field from the last command> # This will fail since the JWT Token was deleted for the project role. argocd app get $APP --auth-token $JWT","title":"Projects"},{"location":"user-guide/projects/#projects","text":"Projects provide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. Projects provide the following features: restrict what may be deployed (trusted Git source repositories) restrict where apps may be deployed to (destination clusters and namespaces) restrict what kinds of objects may or may not be deployed (e.g. RBAC, CRDs, DaemonSets, NetworkPolicy etc...) defining project roles to provide application RBAC (bound to OIDC groups and/or JWT tokens)","title":"Projects"},{"location":"user-guide/projects/#the-default-project","text":"Every application belongs to a single project. If unspecified, an application belongs to the default project, which is created automatically and by default, permits deployments from any source repo, to any cluster, and all resource Kinds. The default project can be modified, but not deleted. When initially created, it's specification is configured to be the most permissive: spec : sourceRepos : - '*' destinations : - namespace : '*' server : '*' clusterResourceWhitelist : - group : '*' kind : '*'","title":"The Default Project"},{"location":"user-guide/projects/#creating-projects","text":"Additional projects can be created to give separate teams different levels of access to namespaces. The following command creates a new project myproject which can deploy applications to namespace mynamespace of cluster https://kubernetes.default.svc . The permitted Git source repository is set to https://github.com/argoproj/argocd-example-apps.git repository. argocd proj create myproject -d https://kubernetes.default.svc,mynamespace -s https://github.com/argoproj/argocd-example-apps.git","title":"Creating Projects"},{"location":"user-guide/projects/#managing-projects","text":"Permitted source Git repositories are managed using commands: argocd proj add-source <PROJECT> <REPO> argocd proj remove-source <PROJECT> <REPO> Permitted destination clusters and namespaces are managed with the commands: argocd proj add-destination <PROJECT> <CLUSTER>,<NAMESPACE> argocd proj remove-destination <PROJECT> <CLUSTER>,<NAMESPACE> Permitted destination K8s resource kinds are managed with the commands. Note that namespaced-scoped resources are restricted via a blacklist, whereas cluster-scoped resources are restricted via whitelist. argocd proj allow-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj allow-namespace-resource <PROJECT> <GROUP> <KIND> argocd proj deny-cluster-resource <PROJECT> <GROUP> <KIND> argocd proj deny-namespace-resource <PROJECT> <GROUP> <KIND>","title":"Managing Projects"},{"location":"user-guide/projects/#assign-application-to-a-project","text":"The application project can be changed using app set command. In order to change the project of an app, the user must have permissions to access the new project. argocd app set guestbook-default --project myproject","title":"Assign Application To A Project"},{"location":"user-guide/projects/#configuring-rbac-with-projects","text":"Once projects have been defined, RBAC rules can be written to restrict access to the applications in the project. The following example configures RBAC for two GitHub teams: team1 and team2 , both in the GitHub org, some-github-org . There are two projects, project-a and project-b . team1 can only manage applications in project-a , while team2 can only manage applications in project-b . Both team1 and team2 have the ability to manage repositories. ConfigMap argocd-rbac-cm example: apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm data : policy.default : \"\" policy.csv : | p, some-github-org:team1, applications, *, project-a/*, allow p, some-github-org:team2, applications, *, project-a/*, allow p, role:org-admin, repositories, get, *, allow p, role:org-admin, repositories, create, *, allow p, role:org-admin, repositories, update, *, allow p, role:org-admin, repositories, delete, *, allow g, some-github-org:team1, org-admin g, some-github-org:team2, org-admin","title":"Configuring RBAC With Projects"},{"location":"user-guide/projects/#project-roles","text":"Projects include a feature called roles that enable automated access to a project's applications. These can be used to give a CI pipeline a restricted set of permissions. For example, a CI system may only be able to sync a single app (but not change its source or destination). Projects can have multiple roles, and those roles can have different access granted to them. These permissions are called policies, and they are stored within the role as a list of policy strings. A role's policy can only grant access to that role and are limited to applications within the role's project. However, the policies have an option for granting wildcard access to any application within a project. In order to create roles in a project and add policies to a role, a user will need permission to update a project. The following commands can be used to manage a role. argocd proj role list argocd proj role get argocd proj role create argocd proj role delete argocd proj role add-policy argocd proj role remove-policy Project roles in itself are not useful without generating a token to associate to that role. Argo CD supports JWT tokens as the means to authenticate to a role. Since the JWT token is associated with a role's policies, any changes to the role's policies will immediately take effect for that JWT token. The following commands are used to manage the JWT tokens. argocd proj role create-token PROJECT ROLE-NAME argocd proj role delete-token PROJECT ROLE-NAME ISSUED-AT Since the JWT tokens aren't stored in Argo CD, they can only be retrieved when they are created. A user can leverage them in the cli by either passing them in using the --auth-token flag or setting the ARGOCD_AUTH_TOKEN environment variable. The JWT tokens can be used until they expire or are revoked. The JWT tokens can created with or without an expiration, but the default on the cli is creates them without an expirations date. Even if a token has not expired, it cannot be used if the token has been revoked. Below is an example of leveraging a JWT token to access a guestbook application. It makes the assumption that the user already has a project named myproject and an application called guestbook-default. PROJ = myproject APP = guestbook-default ROLE = get-role argocd proj role create $PROJ $ROLE argocd proj role create-token $PROJ $ROLE -e 10m JWT = <value from command above> argocd proj role list $PROJ argocd proj role get $PROJ $ROLE # This command will fail because the JWT Token associated with the project role does not have a policy to allow access to the application argocd app get $APP --auth-token $JWT # Adding a policy to grant access to the application for the new role argocd proj role add-policy $PROJ $ROLE --action get --permission allow --object $APP argocd app get $PROJ - $ROLE --auth-token $JWT # Removing the policy we added and adding one with a wildcard. argocd proj role remove-policy $PROJ $TOKEN -a get -o $PROJ - $TOKEN argocd proj role remove-policy $PROJ $TOKEN -a get -o '*' # The wildcard allows us to access the application due to the wildcard. argocd app get $PROJ - $TOKEN --auth-token $JWT argocd proj role get $PROJ argocd proj role get $PROJ $ROLE # Revoking the JWT token argocd proj role delete-token $PROJ $ROLE <id field from the last command> # This will fail since the JWT Token was deleted for the project role. argocd app get $APP --auth-token $JWT","title":"Project Roles"},{"location":"user-guide/resource_hooks/","text":"Resource Hooks \u00b6 Overview \u00b6 Synchronization can be configured using resource hooks. Hooks are ways to interject custom logic before, during, and after a Sync operation. Some use cases for hooks are: Using a PreSync hook to perform a database schema migration before deploying a new version of the app. Using a Sync hook to orchestrate a complex deployment requiring more sophistication than the kubernetes rolling update strategy (e.g. a blue/green deployment). Using a PostSync hook to run integration and health checks after a deployment. Usage \u00b6 Hooks are simply kubernetes manifests annotated with the argocd.argoproj.io/hook annotation. To make use of hooks, simply add the annotation to any resource: apiVersion : batch/v1 kind : Job metadata : generateName : schema-migrate- annotations : argocd.argoproj.io/hook : PreSync During a Sync operation, Argo CD will create the resource during the appropriate stage of the deployment. Hooks can be any type of Kuberentes resource kind, but tend to be most useful as a Pod, Job or Argo Workflows . Multiple hooks can be specified as a comma separated list. Available Hooks \u00b6 The following hooks are defined: Hook Description PreSync Executes prior to the apply of the manifests. Sync Executes after all PreSync hooks completed and were successful. Occurs in conjuction with the apply of the manifests. Skip Indicates to Argo CD to skip the apply of the manifest. This is typically used in conjunction with a Sync hook which is presumably handling the deployment in an alternate way (e.g. blue-green deployment) PostSync Executes after all Sync hooks completed and were successful, a succcessful apply, and all resources in a Healthy state. Hook Deletion Policies \u00b6 Hooks can be deleted in an automatic fashion using the annotation: argocd.argoproj.io/hook-delete-policy . apiVersion : batch/v1 kind : Job metadata : generateName : integration-test- annotations : argocd.argoproj.io/hook : PostSync argocd.argoproj.io/hook-delete-policy : HookSucceeded The following policies define when the hook will be deleted. Policy Description HookSucceeded The hook resource is deleted after the hook succeeded (e.g. Job/Workflow completed successfully). HookFailed The hook resource is deleted after the hook failed. As an alternative to hook deletion policies, both Jobs and Argo Workflows support the ttlSecondsAfterFinished field in the spec, which let their respective controllers delete the Job/Workflow after it completes. spec : ttlSecondsAfterFinished : 600","title":"Resource Hooks"},{"location":"user-guide/resource_hooks/#resource-hooks","text":"","title":"Resource Hooks"},{"location":"user-guide/resource_hooks/#overview","text":"Synchronization can be configured using resource hooks. Hooks are ways to interject custom logic before, during, and after a Sync operation. Some use cases for hooks are: Using a PreSync hook to perform a database schema migration before deploying a new version of the app. Using a Sync hook to orchestrate a complex deployment requiring more sophistication than the kubernetes rolling update strategy (e.g. a blue/green deployment). Using a PostSync hook to run integration and health checks after a deployment.","title":"Overview"},{"location":"user-guide/resource_hooks/#usage","text":"Hooks are simply kubernetes manifests annotated with the argocd.argoproj.io/hook annotation. To make use of hooks, simply add the annotation to any resource: apiVersion : batch/v1 kind : Job metadata : generateName : schema-migrate- annotations : argocd.argoproj.io/hook : PreSync During a Sync operation, Argo CD will create the resource during the appropriate stage of the deployment. Hooks can be any type of Kuberentes resource kind, but tend to be most useful as a Pod, Job or Argo Workflows . Multiple hooks can be specified as a comma separated list.","title":"Usage"},{"location":"user-guide/resource_hooks/#available-hooks","text":"The following hooks are defined: Hook Description PreSync Executes prior to the apply of the manifests. Sync Executes after all PreSync hooks completed and were successful. Occurs in conjuction with the apply of the manifests. Skip Indicates to Argo CD to skip the apply of the manifest. This is typically used in conjunction with a Sync hook which is presumably handling the deployment in an alternate way (e.g. blue-green deployment) PostSync Executes after all Sync hooks completed and were successful, a succcessful apply, and all resources in a Healthy state.","title":"Available Hooks"},{"location":"user-guide/resource_hooks/#hook-deletion-policies","text":"Hooks can be deleted in an automatic fashion using the annotation: argocd.argoproj.io/hook-delete-policy . apiVersion : batch/v1 kind : Job metadata : generateName : integration-test- annotations : argocd.argoproj.io/hook : PostSync argocd.argoproj.io/hook-delete-policy : HookSucceeded The following policies define when the hook will be deleted. Policy Description HookSucceeded The hook resource is deleted after the hook succeeded (e.g. Job/Workflow completed successfully). HookFailed The hook resource is deleted after the hook failed. As an alternative to hook deletion policies, both Jobs and Argo Workflows support the ttlSecondsAfterFinished field in the spec, which let their respective controllers delete the Job/Workflow after it completes. spec : ttlSecondsAfterFinished : 600","title":"Hook Deletion Policies"},{"location":"user-guide/tracking_strategies/","text":"Tracking and Deployment Strategies \u00b6 An Argo CD application spec provides several different ways of track kubernetes resource manifests in Git. This document describes the different techniques and the means of deploying those manifests to the target environment. HEAD / Branch Tracking \u00b6 If a branch name, or a symbolic reference (like HEAD) is specified, Argo CD will continually compare live state against the resource manifests defined at the tip of the specified branch or the dereferenced commit of the symbolic reference. To redeploy an application, a user makes changes to the manifests, and commit/pushes those the changes to the tracked branch/symbolic reference, which will then be detected by Argo CD controller. Tag Tracking \u00b6 If a tag is specified, the manifests at the specified Git tag will be used to perform the sync comparison. This provides some advantages over branch tracking in that a tag is generally considered more stable, and less frequently updated, with some manual judgement of what constitutes a tag. To redeploy an application, the user uses Git to change the meaning of a tag by retagging it to a different commit SHA. Argo CD will detect the new meaning of the tag when performing the comparison/sync. Commit Pinning \u00b6 If a Git commit SHA is specified, the application is effectively pinned to the manifests defined at the specified commit. This is the most restrictive of the techniques and is typically used to control production environments. Since commit SHAs cannot change meaning, the only way to change the live state of an application which is pinned to a commit, is by updating the tracking revision in the application to a different commit containing the new manifests. Note that parameter overrides can still be set on an application which is pinned to a revision. Automated Sync \u00b6 In all tracking strategies, the application has the option to sync automatically. If auto-sync is configured, the new resources manifests will be applied automatically -- as soon as a difference is detected between the target state (Git) and live state. If auto-sync is disabled, a manual sync will be needed using the Argo UI, CLI, or API. Parameter Overrides \u00b6 Note that in all tracking strategies, any parameter overrides set in the application instance take precedence over the Git state.","title":"Tracking and Deployment Strategies"},{"location":"user-guide/tracking_strategies/#tracking-and-deployment-strategies","text":"An Argo CD application spec provides several different ways of track kubernetes resource manifests in Git. This document describes the different techniques and the means of deploying those manifests to the target environment.","title":"Tracking and Deployment Strategies"},{"location":"user-guide/tracking_strategies/#head-branch-tracking","text":"If a branch name, or a symbolic reference (like HEAD) is specified, Argo CD will continually compare live state against the resource manifests defined at the tip of the specified branch or the dereferenced commit of the symbolic reference. To redeploy an application, a user makes changes to the manifests, and commit/pushes those the changes to the tracked branch/symbolic reference, which will then be detected by Argo CD controller.","title":"HEAD / Branch Tracking"},{"location":"user-guide/tracking_strategies/#tag-tracking","text":"If a tag is specified, the manifests at the specified Git tag will be used to perform the sync comparison. This provides some advantages over branch tracking in that a tag is generally considered more stable, and less frequently updated, with some manual judgement of what constitutes a tag. To redeploy an application, the user uses Git to change the meaning of a tag by retagging it to a different commit SHA. Argo CD will detect the new meaning of the tag when performing the comparison/sync.","title":"Tag Tracking"},{"location":"user-guide/tracking_strategies/#commit-pinning","text":"If a Git commit SHA is specified, the application is effectively pinned to the manifests defined at the specified commit. This is the most restrictive of the techniques and is typically used to control production environments. Since commit SHAs cannot change meaning, the only way to change the live state of an application which is pinned to a commit, is by updating the tracking revision in the application to a different commit containing the new manifests. Note that parameter overrides can still be set on an application which is pinned to a revision.","title":"Commit Pinning"},{"location":"user-guide/tracking_strategies/#automated-sync","text":"In all tracking strategies, the application has the option to sync automatically. If auto-sync is configured, the new resources manifests will be applied automatically -- as soon as a difference is detected between the target state (Git) and live state. If auto-sync is disabled, a manual sync will be needed using the Argo UI, CLI, or API.","title":"Automated Sync"},{"location":"user-guide/tracking_strategies/#parameter-overrides","text":"Note that in all tracking strategies, any parameter overrides set in the application instance take precedence over the Git state.","title":"Parameter Overrides"}]}